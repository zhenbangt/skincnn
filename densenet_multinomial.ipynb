{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T08:01:00.445240Z",
     "start_time": "2020-02-02T08:00:58.351808Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "IMG_WIDTH, IMG_HEIGHT = 224, 224\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.applications.densenet import (\n",
    "    DenseNet121,\n",
    "    preprocess_input,\n",
    ")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import IPython.display as display\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense,\n",
    "    GlobalAveragePooling2D,\n",
    "    Conv2D,\n",
    "    Flatten,\n",
    "    GlobalMaxPooling2D,\n",
    "    Dropout,\n",
    "    Activation,\n",
    ")\n",
    "\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam, Nadam\n",
    "from datetime import datetime\n",
    "from packaging import version\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import (\n",
    "    TensorBoard,\n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint,\n",
    "    ReduceLROnPlateau,\n",
    ")\n",
    "import efficientnet.tfkeras as enet\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def append_extension(fn):\n",
    "    return (fn + \".jpg\").zfill(7)\n",
    "\n",
    "\n",
    "def ordered_logit(class_number):\n",
    "    # zero portability\n",
    "    target = np.zeros(4, dtype=int)\n",
    "    target[: class_number - 2] = 1\n",
    "    return target\n",
    "\n",
    "\n",
    "DATADIR = r\"./adult\"\n",
    "CSV_PATH = r\"./adult/CastControls_ALP.xlsx\"\n",
    "response = pd.read_excel(CSV_PATH, sheet_name=0,)[[\"GreenID\", \"Grade\"]].dropna(\n",
    "    axis=0, subset=[\"Grade\"]\n",
    ")\n",
    "response.Grade = response.Grade.astype(\"int\")\n",
    "response.GreenID = response.GreenID.astype(\"str\").apply(append_extension)\n",
    "response = response[response.Grade != 99]\n",
    "response = pd.concat(\n",
    "    [response, pd.DataFrame.from_dict(dict(response.Grade.apply(ordered_logit))).T,],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "\n",
    "# shuffle dataset\n",
    "response = response.sample(frac=1)\n",
    "seed = np.random.randint(30027)\n",
    "\n",
    "\n",
    "def soft_acc(y_true, y_pred):\n",
    "    return K.mean(K.equal(K.round(y_true), K.round(y_pred)))\n",
    "\n",
    "\n",
    "def soft_acc_multi_output(y_true, y_pred):\n",
    "    return K.mean(\n",
    "        K.all(\n",
    "            K.equal(\n",
    "                K.cast(K.round(y_true), \"int32\"), K.cast(K.round(y_pred), \"int32\"),\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "# policy = tf.keras.mixed_precision.experimental.Policy(\"mixed_float16\")\n",
    "# mixed_precision.experimental.set_policy(policy)\n",
    "\n",
    "# gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "# if gpus:\n",
    "#     try:\n",
    "#         # Currently, memory growth needs to be the same across GPUs\n",
    "#         for gpu in gpus:\n",
    "#             tf.config.experimental.set_memory_growth(gpu, True)\n",
    "#         logical_gpus = tf.config.experimental.list_logical_devices(\"GPU\")\n",
    "#         print(\n",
    "#             len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\",\n",
    "#         )\n",
    "#     except RuntimeError as e:\n",
    "#         # Memory growth must be set before GPUs have been initialized\n",
    "#         print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 507 validated image filenames.\n",
      "Found 56 validated image filenames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\feroc\\anaconda3\\envs\\tf\\lib\\site-packages\\keras_preprocessing\\image\\dataframe_iterator.py:273: UserWarning: Found 6 invalid image filename(s) in x_col=\"GreenID\". These filename(s) will be ignored.\n",
      "  .format(n_invalid, x_col)\n"
     ]
    }
   ],
   "source": [
    "data_gen = ImageDataGenerator(\n",
    "    rotation_range=5,\n",
    "    fill_mode=\"reflect\",\n",
    "    horizontal_flip=True,\n",
    "    #     vertical_flip=True,\n",
    "    validation_split=0.1,\n",
    "    rescale=1.0 / 255.0,\n",
    "    #     preprocessing_function = preprocess_input\n",
    "    zoom_range=0.1,\n",
    ")\n",
    "\n",
    "train_set = data_gen.flow_from_dataframe(\n",
    "    dataframe=response,\n",
    "    directory=DATADIR,\n",
    "    x_col=\"GreenID\",\n",
    "    target_size=(224, 224),\n",
    "    color_mode=\"rgb\",\n",
    "    subset=\"training\",\n",
    "    shuffle=True,\n",
    "    #     class_mode = \"sparse\"\n",
    "    #     y_col=\"Grade\",\n",
    "    y_col=[0, 1, 2, 3,],\n",
    "    class_mode=\"raw\",\n",
    ")\n",
    "\n",
    "validation_set = data_gen.flow_from_dataframe(\n",
    "    dataframe=response,\n",
    "    directory=DATADIR,\n",
    "    x_col=\"GreenID\",\n",
    "    target_size=(224, 224),\n",
    "    color_mode=\"rgb\",\n",
    "    subset=\"validation\",\n",
    "    shuffle=True,\n",
    "    batch_size=28,\n",
    "    #     class_mode = \"sparse\"\n",
    "    #     y_col=\"Grade\",\n",
    "    y_col=[0, 1, 2, 3,],\n",
    "    class_mode=\"raw\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T08:01:12.865686Z",
     "start_time": "2020-02-02T08:01:08.228017Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    }
   ],
   "source": [
    "base_model = DenseNet121(\n",
    "    include_top=False, weights=\"imagenet\", input_shape=(224, 224, 3)\n",
    ")\n",
    "base_model.trainable = False\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.8)(x)\n",
    "preds = Dense(4, activation=\"sigmoid\")(x)\n",
    "full_model = Model(inputs=base_model.input, outputs=preds, name=\"multilogit_v1_\")\n",
    "full_model.compile(\n",
    "    optimizer=keras.optimizers.Nadam(),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[soft_acc_multi_output],\n",
    ")\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=21, restore_best_weights=True\n",
    ")\n",
    "reduce_lr_plateau = ReduceLROnPlateau(monitor=\"val_loss\", patience=7, factor=0.8)\n",
    "\n",
    "# history = full_model.fit_generator(\n",
    "#     generator=train_set,\n",
    "#     epochs=50,\n",
    "#     verbose=0,\n",
    "#     validation_data=validation_set,\n",
    "#     callbacks=[early_stopping, reduce_lr_plateau],\n",
    "# )\n",
    "# full_model.save(filepath =\"./saved_models/my_densenet_multinomial_logit/my_densenet_multinomial_logit_untuned.h5\", save_format='h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-686195eaf4cd>:34: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 16 steps, validate for 2 steps\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "16/16 [==============================] - 32s 2s/step - loss: 0.7077 - soft_acc_multi_output: 0.2679 - val_loss: 4.0527 - val_soft_acc_multi_output: 0.1964\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 5s 341ms/step - loss: 0.4731 - soft_acc_multi_output: 0.4379 - val_loss: 11.9961 - val_soft_acc_multi_output: 0.1964\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 6s 344ms/step - loss: 0.4093 - soft_acc_multi_output: 0.4637 - val_loss: 16.7500 - val_soft_acc_multi_output: 0.1964\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 6s 354ms/step - loss: 0.3865 - soft_acc_multi_output: 0.5047 - val_loss: 19.3750 - val_soft_acc_multi_output: 0.1964\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 5s 341ms/step - loss: 0.3589 - soft_acc_multi_output: 0.5386 - val_loss: 12.9688 - val_soft_acc_multi_output: 0.1964\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 6s 351ms/step - loss: 0.2992 - soft_acc_multi_output: 0.5979 - val_loss: 3.9014 - val_soft_acc_multi_output: 0.2321\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 6s 347ms/step - loss: 0.2447 - soft_acc_multi_output: 0.6354 - val_loss: 2.4092 - val_soft_acc_multi_output: 0.3214\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 5s 341ms/step - loss: 0.2592 - soft_acc_multi_output: 0.6363 - val_loss: 3.0576 - val_soft_acc_multi_output: 0.3214\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 6s 344ms/step - loss: 0.2470 - soft_acc_multi_output: 0.6395 - val_loss: 2.4287 - val_soft_acc_multi_output: 0.3214\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 6s 353ms/step - loss: 0.2191 - soft_acc_multi_output: 0.6775 - val_loss: 3.5195 - val_soft_acc_multi_output: 0.2500\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 5s 343ms/step - loss: 0.1871 - soft_acc_multi_output: 0.7459 - val_loss: 11.8203 - val_soft_acc_multi_output: 0.2321\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 6s 352ms/step - loss: 0.1645 - soft_acc_multi_output: 0.7514 - val_loss: 1.8257 - val_soft_acc_multi_output: 0.3393\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 6s 344ms/step - loss: 0.1690 - soft_acc_multi_output: 0.7537 - val_loss: 4.6641 - val_soft_acc_multi_output: 0.3214\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 6s 358ms/step - loss: 0.1688 - soft_acc_multi_output: 0.7658 - val_loss: 0.9907 - val_soft_acc_multi_output: 0.4107\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 6s 350ms/step - loss: 0.1186 - soft_acc_multi_output: 0.8197 - val_loss: 1.0703 - val_soft_acc_multi_output: 0.5536\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 5s 343ms/step - loss: 0.1079 - soft_acc_multi_output: 0.8404 - val_loss: 1.4253 - val_soft_acc_multi_output: 0.3929\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 6s 347ms/step - loss: 0.1162 - soft_acc_multi_output: 0.8471 - val_loss: 1.0686 - val_soft_acc_multi_output: 0.4464\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 6s 349ms/step - loss: 0.1005 - soft_acc_multi_output: 0.8529 - val_loss: 1.3818 - val_soft_acc_multi_output: 0.2321\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 6s 359ms/step - loss: 0.1054 - soft_acc_multi_output: 0.8805 - val_loss: 3.1855 - val_soft_acc_multi_output: 0.2321\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 6s 352ms/step - loss: 0.1091 - soft_acc_multi_output: 0.8544 - val_loss: 1.7798 - val_soft_acc_multi_output: 0.3393\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 6s 349ms/step - loss: 0.0882 - soft_acc_multi_output: 0.8645 - val_loss: 1.1577 - val_soft_acc_multi_output: 0.4107\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 6s 349ms/step - loss: 0.0704 - soft_acc_multi_output: 0.8915 - val_loss: 1.9043 - val_soft_acc_multi_output: 0.3750\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 6s 345ms/step - loss: 0.0497 - soft_acc_multi_output: 0.9368 - val_loss: 1.6206 - val_soft_acc_multi_output: 0.4464\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 6s 349ms/step - loss: 0.0393 - soft_acc_multi_output: 0.9638 - val_loss: 0.8215 - val_soft_acc_multi_output: 0.4286\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 6s 349ms/step - loss: 0.0421 - soft_acc_multi_output: 0.9567 - val_loss: 0.6736 - val_soft_acc_multi_output: 0.4821\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 6s 349ms/step - loss: 0.0302 - soft_acc_multi_output: 0.9606 - val_loss: 0.7507 - val_soft_acc_multi_output: 0.3929\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 6s 347ms/step - loss: 0.0163 - soft_acc_multi_output: 0.9899 - val_loss: 0.6455 - val_soft_acc_multi_output: 0.4107\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 6s 346ms/step - loss: 0.0223 - soft_acc_multi_output: 0.9797 - val_loss: 0.8948 - val_soft_acc_multi_output: 0.4286\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 6s 345ms/step - loss: 0.0366 - soft_acc_multi_output: 0.9648 - val_loss: 1.2710 - val_soft_acc_multi_output: 0.4286\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 6s 345ms/step - loss: 0.0368 - soft_acc_multi_output: 0.9567 - val_loss: 1.4829 - val_soft_acc_multi_output: 0.3036\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 6s 347ms/step - loss: 0.0442 - soft_acc_multi_output: 0.9469 - val_loss: 1.5015 - val_soft_acc_multi_output: 0.3393\n",
      "Epoch 32/100\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0478 - soft_acc_multi_output: 0.9301WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,soft_acc_multi_output\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: loss,soft_acc_multi_output,lr\n",
      "16/16 [==============================] - 6s 345ms/step - loss: 0.0464 - soft_acc_multi_output: 0.9306\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-686195eaf4cd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_set\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m     \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce_lr_plateau\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m )\n\u001b[0;32m     36\u001b[0m \u001b[1;31m# full_model.save(filepath =\"./saved_models/my_densenet_multinomial_logit/tuned.h5\", save_format='h5')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    322\u001b[0m               \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'after %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m               instructions)\n\u001b[1;32m--> 324\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[0;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'deprecated'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1304\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1305\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1306\u001b[1;33m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m   @deprecation.deprecated(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    393\u001b[0m                       \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTEST\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m                       \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meval_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 395\u001b[1;33m                       total_epochs=1)\n\u001b[0m\u001b[0;32m    396\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[0;32m    397\u001b[0m                                  prefix='val_')\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    604\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 606\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    607\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    608\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2361\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2365\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1613\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#################################\n",
    "# fine tunes\n",
    "#################################\n",
    "full_model.trainable = True\n",
    "for layer in full_model.layers[:70]:\n",
    "    layer.trainable = False\n",
    "for layer in full_model.layers[70:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "validation_set.reset()\n",
    "train_set.reset()\n",
    "\n",
    "full_model.compile(\n",
    "    optimizer=keras.optimizers.Nadam(),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[soft_acc_multi_output],\n",
    ")\n",
    "logdir_name = (\n",
    "    r\".\\tfb\\logs\\densenet_multinomial_logit\\\\\"\n",
    "    + full_model.name\n",
    "    + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    ")\n",
    "tensorboard_callback = TensorBoard(log_dir=logdir_name)\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=21, restore_best_weights=True\n",
    ")\n",
    "reduce_lr_plateau = ReduceLROnPlateau(monitor=\"val_loss\", patience=7, factor=0.8)\n",
    "\n",
    "\n",
    "history_fine = full_model.fit_generator(\n",
    "    generator=train_set,\n",
    "    epochs=100,\n",
    "    validation_data=validation_set,\n",
    "    callbacks=[tensorboard_callback, early_stopping, reduce_lr_plateau],\n",
    ")\n",
    "# full_model.save(filepath =\"./saved_models/my_densenet_multinomial_logit/tuned.h5\", save_format='h5')\n",
    "# full_model.save_weights('./saved_models/my_densenet_multinomial_logit/weights_tuned.h5', save_format='h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_val_test(train_index, val_index, test_index):\n",
    "    train_dataset = response.iloc[train_index]\n",
    "    val_dataset = response.iloc[val_index]\n",
    "    test_dataset = response.iloc[test_index]\n",
    "    train_gen = ImageDataGenerator(\n",
    "        rotation_range=5,\n",
    "        fill_mode=\"reflect\",\n",
    "        horizontal_flip=True,\n",
    "        rescale=1.0 / 255.0,\n",
    "        zoom_range=0.1,\n",
    "    )\n",
    "    valid_test_gen = ImageDataGenerator(rescale=1.0 / 255.0,)\n",
    "\n",
    "    train_set = train_gen.flow_from_dataframe(\n",
    "        dataframe=train_dataset,\n",
    "        directory=DATADIR,\n",
    "        x_col=\"GreenID\",\n",
    "        target_size=(224, 224),\n",
    "        color_mode=\"rgb\",\n",
    "        subset=\"training\",\n",
    "        shuffle=True,\n",
    "        y_col=[0, 1, 2, 3,],\n",
    "        class_mode=\"raw\",\n",
    "    )\n",
    "\n",
    "    validation_set = valid_test_gen.flow_from_dataframe(\n",
    "        dataframe=val_dataset,\n",
    "        directory=DATADIR,\n",
    "        x_col=\"GreenID\",\n",
    "        target_size=(224, 224),\n",
    "        color_mode=\"rgb\",\n",
    "        subset=\"training\",\n",
    "        shuffle=False,\n",
    "        batch_size=64,\n",
    "        y_col=[0, 1, 2, 3,],\n",
    "        class_mode=\"raw\",\n",
    "    )\n",
    "\n",
    "    test_set = valid_test_gen.flow_from_dataframe(\n",
    "        dataframe=test_dataset,\n",
    "        directory=DATADIR,\n",
    "        x_col=\"GreenID\",\n",
    "        target_size=(224, 224),\n",
    "        color_mode=\"rgb\",\n",
    "        subset=\"training\",\n",
    "        shuffle=False,\n",
    "        batch_size=64,\n",
    "        y_col=[0, 1, 2, 3,],\n",
    "        class_mode=\"raw\",\n",
    "    )\n",
    "    return train_set, validation_set, test_set\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "innerkf = StratifiedKFold(n_splits=2, shuffle=True, random_state=seed)\n",
    "response = response.sample(frac=1.0)\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=21, restore_best_weights=True,\n",
    ")\n",
    "reduce_lr_plateau = ReduceLROnPlateau(monitor=\"val_loss\", patience=7, factor=0.8)\n",
    "\n",
    "\n",
    "\n",
    "trainable_sequence = np.array([429, 424, 423, 422, 421, 420, 419, 418, 417, 416, 415, 414, 413, 412,\n",
    "       411, 410, 409, 408, 407, 406, 405, 404, 403, 402, 401, 400, 399,\n",
    "       398, 397, 396, 395, 394, 393, 392, 391, 390, 389, 388, 387, 386,\n",
    "       385, 384, 383, 382, 381, 380, 379, 378, 377, 376, 375, 374, 373,\n",
    "       372, 371, 370, 369, 368, 367, 366, 365, 364, 363, 362, 361, 360,\n",
    "       359, 358, 357, 356, 355, 354, 353, 352, 351, 350, 349, 348, 347,\n",
    "       346, 345, 344, 343, 342, 341, 340, 339, 338, 337, 336, 335, 334,\n",
    "       333, 332, 331, 330, 329, 328, 327, 326, 325, 324, 323, 322, 321,\n",
    "       320, 319, 318, 317, 316, 315, 314, 313, 311, 308, 307, 306, 305,\n",
    "       304, 303, 302, 301, 300, 299, 298, 297, 296, 295, 294, 293, 292,\n",
    "       291, 290, 289, 288, 287, 286, 285, 284, 283, 282, 281, 280, 279,\n",
    "       278, 277, 276, 275, 274, 273, 272, 271, 270, 269, 268, 267, 266,\n",
    "       265, 264, 263, 262, 261, 260, 259, 258, 257, 256, 255, 254, 253,\n",
    "       252, 251, 250, 249, 248, 247, 246, 245, 244, 243, 242, 241, 240,\n",
    "       239, 238, 237, 236, 235, 234, 233, 232, 231, 230, 229, 228, 227,\n",
    "       226, 225, 224, 223, 222, 221, 220, 219, 218, 217, 216, 215, 214,\n",
    "       213, 212, 211, 210, 209, 208, 207, 206, 205, 204, 203, 202, 201,\n",
    "       200, 199, 198, 197, 196, 195, 194, 193, 192, 191, 190, 189, 188,\n",
    "       187, 186, 185, 184, 183, 182, 181, 180, 179, 178, 177, 176, 175,\n",
    "       174, 173, 172, 171, 170, 169, 168, 167, 166, 165, 164, 163, 162,\n",
    "       161, 160, 159, 158, 157, 156, 155, 154, 153, 152, 151, 150, 149,\n",
    "       148, 147, 146, 145, 144, 143, 142, 141, 139, 136, 135, 134, 133,\n",
    "       132, 131, 130, 129, 128, 127, 126, 125, 124, 123, 122, 121, 120,\n",
    "       119, 118, 117, 116, 115, 114, 113, 112, 111, 110, 109, 108, 107,\n",
    "       106, 105, 104, 103, 102, 101, 100,  99,  98,  97,  96,  95,  94,\n",
    "        93,  92,  91,  90,  89,  88,  87,  86,  85,  84,  83,  82,  81,\n",
    "        80,  79,  78,  77,  76,  75,  74,  73,  72,  71,  70,  69,  68,\n",
    "        67,  66,  65,  64,  63,  62,  61,  60,  59,  58,  57,  56,  55,\n",
    "        54,  53,  51,  48,  47,  46,  45,  44,  43,  42,  41,  40,  39,\n",
    "        38,  37,  36,  35,  34,  33,  32,  31,  30,  29,  28,  27,  26,\n",
    "        25,  24,  23,  22,  21,  20,  19,  18,  17,  16,  15,  14,  13,\n",
    "        12,  11,  10,   9,   8,   7,   4,   3,   2])\n",
    "\n",
    "def fine_tune_model(fine_tune=None):\n",
    "    if fine_tune is None:\n",
    "        try:\n",
    "            fine_tune = 424\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    base_model = DenseNet121(\n",
    "        include_top=False, weights=\"imagenet\", input_shape=(224, 224, 3)\n",
    "    )\n",
    "    base_model.trainable = False\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(0.8)(x)\n",
    "    x = Dense(4)(x)\n",
    "    preds = Activation(\"sigmoid\", dtype=\"float32\")(x)\n",
    "    model = Model(inputs=base_model.input, outputs=preds)\n",
    "\n",
    "    model.trainable = True\n",
    "    for layer in model.layers[:fine_tune]:\n",
    "        layer.trainable = False\n",
    "    for layer in model.layers[fine_tune:]:\n",
    "        layer.trainable = True\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Nadam(),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[soft_acc_multi_output],\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "def stratified_cv(fine_tune_layer=None):\n",
    "    acc_coef_scores = []\n",
    "    raw_outputs = []\n",
    "    for train_index, val_test_index in kf.split(\n",
    "        np.zeros(len(response)), response[\"Grade\"]\n",
    "    ):\n",
    "        val_index, test_index = next(\n",
    "            innerkf.split(\n",
    "                np.zeros(len(val_test_index)), response[\"Grade\"].iloc[val_test_index]\n",
    "            )\n",
    "        )\n",
    "        val_index, test_index = val_test_index[val_index], val_test_index[test_index]\n",
    "        train_set, validation_set, test_set = generate_train_val_test(\n",
    "            train_index, val_index, test_index\n",
    "        )\n",
    "\n",
    "        model = fine_tune_model(fine_tune=fine_tune_layer)\n",
    "\n",
    "        _ = model.fit(\n",
    "            x=train_set,\n",
    "            epochs=100,\n",
    "            validation_data=validation_set,\n",
    "            callbacks=[early_stopping, reduce_lr_plateau],\n",
    "                    verbose=0,\n",
    "        )\n",
    "\n",
    "        batch = next(test_set)\n",
    "        true_labels = batch[1]\n",
    "        predictions = model.predict(batch[0])\n",
    "        acc = soft_acc_multi_output(predictions, true_labels).numpy()\n",
    "        corr = np.corrcoef(np.sum(predictions, axis=1), np.sum(true_labels, axis=1))[0][\n",
    "            1\n",
    "        ]\n",
    "        acc_coef_scores.append([acc, corr])\n",
    "        raw_outputs.append(\n",
    "            [np.array(response.iloc[test_index].index), true_labels, predictions]\n",
    "        )\n",
    "        del train_set, validation_set, test_set, _, model, batch, true_labels, predictions, acc, corr\n",
    "        tf.keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "    return acc_coef_scores, raw_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune_scores_acc_coef = []\n",
    "fine_tune_raw_outputs = []\n",
    "\n",
    "# already includes untuned 429\n",
    "\n",
    "for fine_tune in trainable_sequence[360:0:-12]:\n",
    "    acc_coef_scores, raw_outputs = stratified_cv(fine_tune)\n",
    "    fine_tune_scores_acc_coef.append(acc_coef_scores)\n",
    "    fine_tune_raw_outputs.append(raw_outputs)\n",
    "    # better save before crash\n",
    "    np.save(r\"./stratified_cross_validation_results/densenets/multinomial_acc_coef\", np.array(fine_tune_scores_acc_coef))\n",
    "    np.save(r\"./stratified_cross_validation_results/densenets/multinomial_raw_outputs\", np.array(fine_tune_raw_outputs))\n",
    "    del acc_coef_scores, raw_outputs\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "[[35  9  0  0  0]\n",
      " [ 7 40  4  4  0]\n",
      " [13 40 16  7  2]\n",
      " [ 0 20 11 15  3]\n",
      " [ 1 12 13 17 15]]\n",
      "+++++++++++++++++++++++++++++++++\n",
      "+++++++++++++++++++++++++++++++++\n",
      "[[0.79545455 0.20454545 0.         0.         0.        ]\n",
      " [0.12727273 0.72727273 0.07272727 0.07272727 0.        ]\n",
      " [0.16666667 0.51282051 0.20512821 0.08974359 0.02564103]\n",
      " [0.         0.40816327 0.2244898  0.30612245 0.06122449]\n",
      " [0.01724138 0.20689655 0.22413793 0.29310345 0.25862069]]\n",
      "+++++++++++++++++++++++++++++++++\n",
      "+++++++++++++++++++++++++++++++++\n",
      "42.6056338028169\n",
      "+++++++++++++++++++++++++++++++++\n",
      "+++++++++++++++++++++++++++++++++\n",
      "max accuracy with tuning from 95 layers, or tune 335 layers\n",
      "3 [79.54545454545455, 72.72727272727273, 20.51282051282051, 30.612244897959183, 25.862068965517242]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAD4CAYAAAANbUbJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3dd3xU153//9dnRn3UNRICdVGMAVOEQC4QG8cFp+Ce2BDH+cYbJ7/YSTbZbzbZb7Ip3vVu4vR84xTsOJs4xsQ9xD8cQ9yCHFNEMUWAJQSooV5Qb3O+f2iEB1RmRhoxmtHn+XjwsHTn3jvnMuatq3PO/RwxxqCUUiq4WfzdAKWUUpNPw14ppaYBDXullJoGNOyVUmoa0LBXSqlpIMTfDbiQ3W432dnZ/m6GUkoFlL179zYYY5JHe33KhX12djZFRUX+boZSSgUUETk91uvajaOUUtOAhr1SSk0DGvZKKTUNaNgrpdQ0oGGvlFLTgIa9UkpNAxr2Sik1DWjY+8jxmja2F9f6uxlKKTUiDXsfqG7pYsPjO/nC0/vo7Xf4uzlKKTWMhv0EdfcN8Nkn99LQ3kt3n4NDVa3+bpJSSg2jYT8Bxhj+7YVDHKpq5Xu3XQbAnlNNfm6VUkoNp2E/Ab8tPMmL+6v48nXzuGtlJrl2G0Ua9kqpKUjDfpwKSxr4r61HuXHhDL5w7RwAVmQnsudUMw6HruurlJpaNOzHobyxkwef3seclGh+9LGlWCwCQH52Aq1dfZTUtfu5hUopdT4Ney919PRz/5NFOByGjffkEx3+fpXolTmJAOzWrhyl1BSjYe8FYwxffe5d3qtt4xfr88i22857PTMxipSYcO23V0pNORr2XvjlmyfYeqiGr980nw/MG74gjIiwIieRPSc17JVSU0tQhX1Nazdt3X2Tcu7Xj9Xyw23HWbdkFp9ZnTvqfiuzE6lu7aayuXNS2qGUUuMRNGF/urGDK7/3Gi8dqPb5uU/Ut/Olpw+wYGYs3799MSIy6r752QmAzrdXSk0tQRP2mYlRzJsRwwv7Kn163rPdfXzmD0WEhlj4zT3LiQyzjrn//NRYYsJD2HOq2aftUEqpiQiasBcRbstLY395C2X1vpn6aIzhK386QHljJ7/ckEd6QpTbY6wWYXl2gvbbK6WmlKAJe4Cbl6ZhEXhxf5VPzvdOWSN/O1rH19bO5/LcJI+PW5GdSEldO80dvT5ph1JKTVRQhf2M2AhWzU3mhX1VPnmKddOucuIiQ7nniiyvjluRPTjfXvvtlVJTRVCFPcDteWlUtXRN+MGmhvYeXj1Sw+156USEjt1Pf6HF6XGEWS0UndZ+e6XU1OBR2IvIWhE5LiKlIvL1Mfa7Q0SMiOS7bPs353HHReRGXzR6LDcsSMUWZp3wQO2zRZX0DRjWF2R4fWxEqJUlGXHs1n57pdQU4TbsRcQKPArcBCwA7haRBSPsFwN8Edjlsm0BcBewEFgL/NJ5vkkTGWblQ5fNZOuhGrr7BsZ1DofD8PTucgpyEpmTEjOuc6zITuRwVSudvf3jOl4ppXzJkzv7lUCpMabMGNMLbAZuHmG//wAeAbpdtt0MbDbG9BhjTgKlzvNNqtvy0mnv6WfbOJcJfPtEA+VNnawvyBx3G1ZkJ9LvMBwobxn3OZRSylc8Cfs0oMLl+0rntnNEZBmQYYx52dtjncffLyJFIlJUX1/vUcPHUpCTSFp85Li7cp7aWU6iLYy1i1LH3Ya8rARE0Pn2SqkpwZOwH+lx0XNTXUTEAvwE+Bdvjz23wZiNxph8Y0x+cvLwmjPesliEW5el8ff36qlr63Z/gIu6s91sP1rLHcvTCQ8Zf49TXGQo81NjdUaOUmpK8CTsKwHXUcp0wLUmQQywCHhTRE4BlwNbnIO07o6dNLfmpeEwsMXL8gnPFFUw4DDcvXL8XThDVmYnsK+8mf4BXYRcKeVfnoT9HmCuiOSISBiDA65bhl40xrQaY+zGmGxjTDawE1hnjCly7neXiISLSA4wF9jt86sYwezkaJZkxPP8Ps8fsBpwGJ7eXcFVc5LIuaB88XjkZyfS2TvAkeqzEz6XUkpNhNuwN8b0Aw8CrwJHgWeMMUdE5CERWefm2CPAM0Ax8FfgAWPM+KbIjMPteWkcPXOWYg/D9u/v1VPV0sX6ld49RDWaocVMtCtHKeVvHs2zN8ZsNcbMM8bMNsY87Nz2LWPMlhH2vcZ5Vz/0/cPO4y4xxrziu6a795HFswi1Ci/u92yg9qld5dijw7l+wQyfvP+M2AgyE6M07JVSfhd0T9C6SrSFseaSFF46UO2237y6pYvXj9Xysfx0wkJ899eyIjuRolPNGON9+YbSujbePF7ns7YopaavoA57GJxzX9/WQ2Fpw5j7/WlPBQZ8MjDrakV2Ao0dvZyo7/DquN5+B/c/uZcvPr1/XD8olFLKVdCH/Zr5ycRHhY5ZCbN/wMGf9lTwgbnJZCS6L2PsjRXj7Lf/wzunKKvv4Gx3Py2dk7P6llJq+gj6sA8PsfLRxbN49UjNqEsWvn6sjpqz3RN6YnY0uXYb9ugwr8K+sb2Hn71WQqItDIDTTbrEoVJqYoI+7AFuy0uju8/BK4drRnx90+5yZsSG88H5KT5/bxEhPyvRq7D/0fb36Ood4L9uXQQMLrmolFITMS3CfmlGPLl224jlEyqaOnnrvXo+np9BiHVy/jrysxOoaOqiptX907zF1WfZvLuce67I4ppLUhCBUw16Z6+UmphpEfZDSxbuLGuisvn84Ny8pxwBPu7jgVlXQ/Pt3dXYN8bw0MtHiIsM5Z8/OI+IUCupsRGcbtI7e6XUxEyLsAe4Zdlg/bWXXAZq+wYcPFNUyZpLUkiLj5y0914wMxZbmJUiN2H/18M17Cxr4is3XEJcVCgAWUlRnG7UO3ul1MRMm7BPT4iiICeRF/ZVnZvK+LfiWurbeiZlYNZViNVCXlbCmIuZdPcN8PDWo8xPjeHuFe+XE8pOsmmfvVJqwqZN2APcnpdOWUMHByoGa8w/taucWXERXHOJ7wdmL5Sflcjx2jZau0aeEfTbwpNUNnfxrY8sOG/sIDMpiob2Xtp7dBEUpdT4Tauwv+myVMJDLLywr4pTDR0UljZw18pMrJaRKjH71oqcBIyBvaeH393XtHbz6Bul3LhwBlfOsZ/3WnbSYEE2vbtXSk3EtAr7mIhQblyYyl8OVvOHd05jtQgfX+H9GrPjsSwjgVCrjLiYySN/PUb/gOEbHxq22iNZSYMPeZVrv71SagKmVdjD4Jz7ls4+fvePk1x3aQozYiMuyvtGhllZlBbHngv67feXN/PC/iruW51DZtLwp3eznHf2pzTslVITMO3CftUcO8kx4RgD6wt8U8rYUyuzEzlY2XpuIXSHw/DdvxSTHBPOA2vmjHhMdHgI9ugw7cZRSk3ItAv7EKuFT12ZzZKMeFZf0D8+2fKzE+kdcPCuc4D4z+9WcaCihX+98RKiw0NGPS4zUadfKqUmZtqFPcADa+bw5weuwnIRBmZd5WclAFB0upmOnn6+98oxFqfHcXte+pjH6fRLpdREjX47qXwuwRbGvBnR7D7ZRFfvALVne/jlhjy3P3Sykmy8eKCK7r4BIkLHvwi6Umr6mpZ39v60InuwKNrGHWXcvHQWy7MS3R6TlRSFMQwr9aCUUp7SsL/IVjgXIbcIfG3tfI+OGZp+qQXRlFLjpWF/kV2em0SIRXjgmjnM8rAez7kHq7SuvVJqnLTP/iJLjYtgx9fWkOrF/P74qFBiIkJ0kFYpNW4a9n4wM867CpsiQnaSTR+sUkqNm3bjBIjMpCjK9c5eKTVOGvYBIjspisrmLvoHHP5uilIqAGnYB4isJBv9DkN1i/ulDZVS6kIa9gEiK9E5/VK7cpRS46BhHyCy7VrXXik1fhr2ASIlJpyIUIsWRFNKjYtHYS8ia0XkuIiUisjXR3j9cyJySEQOiEihiCxwbs8WkS7n9gMi8mtfX8B0ISJkJer0S6XU+LidZy8iVuBR4HqgEtgjIluMMcUuu20yxvzauf864MfAWudrJ4wxS33b7OkpKymKkw3ajaOU8p4nd/YrgVJjTJkxphfYDNzsuoMx5qzLtzbA+K6JakhWUhTlTZ04HPrXq5TyjidhnwZUuHxf6dx2HhF5QEROAI8AX3R5KUdE9ovIWyKyeqQ3EJH7RaRIRIrq6+u9aP70kpVko6ffQW2bTr9USnnHk7Afqdj6sFtLY8yjxpjZwNeAbzo3nwEyjTHLgK8Am0QkdoRjNxpj8o0x+cnJyZ63fpo5VxBN++2VUl7yJOwrgQyX79OB6jH23wzcAmCM6THGNDq/3gucAOaNr6lqqNSxTr9USnnLk7DfA8wVkRwRCQPuAra47iAic12+/TBQ4tye7BzgRURygblAmS8aPh3NjIsg1Co6I0cp5TW3s3GMMf0i8iDwKmAFnjDGHBGRh4AiY8wW4EERuQ7oA5qBe52HfwB4SET6gQHgc8aYpsm4kOkgxGohIyGKcg17pZSXPCpxbIzZCmy9YNu3XL7+0ijHPQ88P5EGqvNlJkVpyQSllNf0CdoAk51ko7yxE2N0+qVSynMa9gEmMzGKtp5+mjp6/d0UpVQA0bAPMNn2oeqX2m+vlPKchn2AyXLOtS9v0n57pZTnNOwDTHpCJCJwqkHv7JVSntOwDzDhIVZmxUXqg1VKKa9o2AegbHsUp5v0zl4p5TkN+wCUmWjT+jhKKa9o2Aeg7KQomjp6Odvd5++mKKUChIZ9ABoqiKZlE5RSntKwD0BD0y+1bIJSylMa9gHo/VLHemevlPKMhn0AigoLITkmXKdfKqU8pmEfoLKTorRkglLKYxr2ASrLWf1SKaU8oWEfoLISo6g5201334C/m6KUCgAa9gEqyz5UEE3v7pVS7mnYB6isRGep4wYdpFVKuadhH6CynXPtdfqlUsoTGvYBKi4qlPioUE5rXXullAc07ANYVmKU3tkrpTyiYR/AspK0+qVSyjMa9gEsOymKyuZOevsd/m6KUmqK07APYJlJNhwGqlq6/N0UpdQUp2EfwLLPFUTTQVql1Ng07ANYpla/VEp5SMM+gCVHhxMVZtW69koptzTsA5iIaEE0pZRHPAp7EVkrIsdFpFREvj7C658TkUMickBECkVkgctr/+Y87riI3OjLxqvBufZ6Z6+Ucsdt2IuIFXgUuAlYANztGuZOm4wxlxljlgKPAD92HrsAuAtYCKwFfuk8n/KRLHsUFU1dDDiMv5uilJrCPLmzXwmUGmPKjDG9wGbgZtcdjDFnXb61AUPJczOw2RjTY4w5CZQ6z6d8JDvJRu+Ag5qz3f5uilJqCvMk7NOACpfvK53bziMiD4jICQbv7L/o5bH3i0iRiBTV19d72nbF+9UvT2v1S6XUGDwJexlh27A+A2PMo8aY2cDXgG96eexGY0y+MSY/OTnZgyapIUN17U9rXXul1Bg8CftKIMPl+3Sgeoz9NwO3jPNY5aXU2AjCrBaPB2mbOnqpbNYfDEpNN56E/R5grojkiEgYgwOuW1x3EJG5Lt9+GChxfr0FuEtEwkUkB5gL7J54s9UQq0XISIzkdMPYAV7X1s1/vlzMld97jZt+ukNLLCg1zYS428EY0y8iDwKvAlbgCWPMERF5CCgyxmwBHhSR64A+oBm413nsERF5BigG+oEHjDG6aKqPZSfZRu3GqTvbza/fKuOpXafpdxg+sngm24tr+frzB/nDp1ciMlJPm1Iq2LgNewBjzFZg6wXbvuXy9ZfGOPZh4OHxNlC5l5kUxTtljRhjzoV3TWs3v37rBJt2lzPgMNy6LI0H18wh227jyZ2n+feXDvP07grWF2T6ufVKqYvBo7BXU1t2ko3O3gEa2nvpdzj41Zsn2LynAofDcHteOp9fM5ss5zKGABtWZvLXw2d4+P8vZvVcOxnOGT1KqeClYR8EspwF0f71uXd5u7QRhzHcmZ/O56+ZM2KQWyzC929fzI0/+Ttfe/4gf7yvAItFu3OUCmZaGycI5NqjASgsbeCO/HTe/Oo1/Pdti8e8Y09PiOKbH1nAP0408tSu0xerqUopP9E7+yCQmRTF7z61gnmpMaTFR3p83F0rMth66Az/tfUYV89LOVcyWSkVfPTOPkismZ/iVdDDYNXM79++mBCL8L+fexeH1tdRKmhp2E9zs+Ij+fePLmD3ySb+5x+n/N0cpdQk0bBX3Lk8nWvnp/DIq8coq2/3d3OUUpNAw14hIvz3bZcRZrXw1ecOarlkpYKQhr0CYEZsBN+9eSF7TzfzROFJfzdHKeVjGvbqnFuWpnH9ghn8YNtxSuu0O0epYKJhr84RER6+dRFRYVb+5dl36R9w+LtJSikf0bBX50mJieChmxfxbkULG3eU+bs5Sikf0bBXw3x08UxuWpTKT7eXUFrX5u/mKKV8QMNeDSMi/MctixCB3/9DSykoFQw07NWI7NHh3LAwlb8crKa3f2r33RujU0WVckfDXo3qtrw0Wjr7eON4nb+bMqqKpk4Wf3cbb5c2+LspSk1pGvZqVKvn2LFHh/PCvkp/N2VUm3aX09bdT9GpZn83RakpTcNejSrEauGWpbN4/VgdzR29/m7OML39Dp4tqgDgZIM+F6DUWDTs1Zhuy0unb8Dw8sFqfzdlmG3FNTS09xITEUJZQ4e/m6PUlKZhr8a0YFYs81NjeH5flb+bMsymXeWkxUeybsksTtZ36ECtUmPQsFdu3Z6XzoGKFk5MoYqYZfXt/ONEI+sLMpmTEk1bTz8N7VOvq0mpqULDXrl189JZWARenEJ390/vLifEItyZn06OfXAx9ZPalaPUqDTslVspsRGsmpvMi/urpsRqVt19Azy3t5LrF8wgJSaC2cmDa/BqLX6lRqdhrzxye14aVS1d7DrZ5O+m8OqRGpo7+9hQkAUMrrYVZrXonb1SY9CwVx65YUEqtjDrlJhz/9TOcrKSorhydhIAVouQlRSlM3KUGoOGvfJIZJiVD102k1cO19DVO+C3dpTUtrH7VBN3r8zEYpFz23PsNr2zV2oMGvbKY7flpdPe08+24hq/tWHT7nJCrcIdy9PP256bHM3pxg6twa/UKDTslccKchJJi4/kBT/NyunuG+D5vZWsXTQTe3T4ea/l2m30DRiqWrr80jalpjqPwl5E1orIcREpFZGvj/D6V0SkWEQOishrIpLl8tqAiBxw/tniy8ari8tiEW5dlsaOknrqznZf9Pd/+eAZznb3s35l5rDXcpIHp19qv71SI3Mb9iJiBR4FbgIWAHeLyIILdtsP5BtjFgPPAY+4vNZljFnq/LPOR+1WfnJrXhoOA38+cPHLJ2zadZrcZBuX5yYOey13aK59vYa9UiPx5M5+JVBqjCkzxvQCm4GbXXcwxrxhjOl0frsTSEcFpdnJ0SzNiOf5izwr5+iZs+wrb2H9ykxEZNjribYwYiNCKNOCaEqNyJOwTwMqXL6vdG4bzX3AKy7fR4hIkYjsFJFbRjpARO537lNUX1/vQZOUP92el8axmjaKq89etPfctKucsBDLsIHZISJCTnK0zshRahSehP3w2ygY8TFKEfkEkA/8wGVzpjEmH1gP/FREZg87mTEbjTH5xpj85ORkD5qk/Okji2cRapWLNue+o6efF/dX8eHLZhIfFTbqfrl2m3bjKDUKT8K+Eshw+T4dGNZhKyLXAd8A1hljeoa2G2Oqnf8tA94Elk2gvWoKSLCFce38FF46UH1Rpjr+5d1q2nv62VAwfGDWVa7dRnVrt1+fA1BqqvIk7PcAc0UkR0TCgLuA82bViMgy4DcMBn2dy/YEEQl3fm0HrgKKfdV45T+3Lkunob2HHRdhOcBNu8uZNyOa5VkJY+43NCNHu3KUGs5t2Btj+oEHgVeBo8AzxpgjIvKQiAzNrvkBEA08e8EUy0uBIhF5F3gD+J4xRsM+CKyZn0x8VOikV8I8XNXKwcrWUQdmXWn1S6VGF+LJTsaYrcDWC7Z9y+Xr60Y57h/AZRNpoJqawkOsfHTxLJ4pqqCtu4+YiNBJeZ+ndpUTEWrh1jz3E7zeD3udkaPUhfQJWjVut+Wl0dPv4JVDk1M+oa27jz8fqOKji2cRF+n+h0lUWAgz4yL0wSqlRqBhr8ZtaUY8uXbbpM25//OBajp7B1jvZmDWVY7dRpnOyFFqGA17NW4iwm15aew62URFU6f7A7xgjOGpXeVcOjOWpRnxHh83GPbtuh6tUhfQsFcTcsuywefrXtrv24HaAxUtHD1zlg0F7gdmXeXYbZzt7qe5s8+n7VEq0Hk0QKvUaNITorg8N5GNO8oIsVr45BVZ2MLH/7/VgMPw8sFqfvq3EqLCrNy8dJZXxw8tUXiyoZ1E2/AaOkpNV3pnrybsv269jGWZCXz/r8dY9f3XefSNUtp7+r06x4DD8NL+Km74yVt8afMBQizCrz6x3OtZPkMzck5ov71S59E7ezVhucnR/OHTK9lX3szPXyvhB68e57EdZfzTqhzuvTJ7zMDuH3Cw5d1qfvF6KWUNHVwyI4ZH1+dx06LU81ai8lR6QiQhFtG59kpdQMNe+UxeZgL/879WcqCihZ+/VsIPt73Hxr+Xcd+qXD51VfZ50yf7Bxy8dKCaX7xewqnGTuanxvCrDXncuHB8IT8kxGohMylKa+QodQENe+VzSzPieeJTKzhU2crPXivhJ397j8cLy/j0VTl88oosXjtaxy/eKKW8qZMFM2P59SeWc8OCGRMKeVe5dq1+qdSFNOzVpLksPY7H783ncFUrP3+thJ85/wAsSovlsU/mc92lKV7NtvFEbrKNv5fUM+AwWH30A2Qqau3s4/Ob9vLddYuYkxLt7+aoKU7DXk26RWlxbPxkPkeqW3lxXxVXzE7i2vm+D/khOXYbvf0Oqlu6yEiMmpT3mApeP17L26WNbD10hi9+cK6/m6OmOA17ddEsnBXHwllxk/4+uS4F0YI57HeUDFYc3Vfe7OeWTD8N7T0YA8kx4e53niJ06qUKOtOh1LExhsKhsD/djMOhTwxfDGdau/jWnw9z5fdeZ90vCmntCpyH9zTsVdBJjg4nOjyEsvrgrX5ZWtdOXVsPy7MSONvdr2vvTrKqli6++dIhrn7kTTbtKmftwlTq2nr4z5cDp2K7duOooCMigzVygvjOfqgL54sfnMu9T+xm3+kW5qTE+LlVwaeiqZNfvnmC5/YOLsN9Z34Gn79mNukJUWQkRvLoGye46bJUrp0/Y8Lv9djfy+jqG+DBNXN8NjPNlYa9Ckq5yTb2ng7evuzC0gZy7DZWz7ETFxnK3tPNfGxFhvsDlUfKGzv55ZulPLe3EosId63I5HPXzCYtPvLcPl/84FxeO1rH158/xLYvJ4y5PrI7b71Xz3+/cpS1i1KZpHkL2o2jglOO3UZVSxfdfcG3Hm1vv4OdZY2smmPHYhGWZcbrIK2PnG7s4KvPvsuaH73JC/ur2FCQyVv/eg3/ccui84IeBhfw+eGdS2jq6OW7fxl/d86phg6+sGkf82bE8IM7lkzaLDW9s1dBKcduwxg43djJJanB1b2xv7yZzt4BVs21A7A8M4E3j9fT2tXn0SIvamSldW186GeFIHDP5Vl87urZpMZFjHnMorQ4Hlgzh5+9VsLaRancuDDVq/ds7+nn/ieLsFiEjffkT6iIoDt6Z6+CUq79/eqXwaawtAGLwBWzkwDIcy7Evl/v7ifkT3sqMBhe+8rVfGfdQrdBP+SBNXNYMDOWb7x4iKaOXo/fz+Ew/MszByita+cXd+eRmTS504Q17FVQGpp+GYyDtDtKGliSEU+ss8Dckox4LAL7ylv83LLANVSrac0lKV4/mxEWYuGHdy6htauPb2854vFxv3ijlFeP1PJ/PnTpud/SJpOGvQpK0eEhpMSEB11BtNbOPg5WtrB6zvvhEB0ewiWpsewL4gHpyVZY2kB9Ww+3ebCw/UgWzIrli9fO5S/vVrP10Bm3+28vruXH29/j1mVp3LcqZ1zv6S0NexW0gnH65TtljTgMrJqbfN72vMx4DlS0MKAPV43LC/uqiI8KZc38ZPc7j+Jz18zmsrQ4vvnSYRrae0bdr7SujS//6QCXpcXx37ddNmkDshfSsFdBKzfZNmWeom3t7GPD4zs5eubshM5TWFqPLczKsszz1+VdnpVAe08/JXVtEzp/oGhs7+HujTsp9cH1tnX38eqRGj66eBbhIdZxnyfUauFHH1tCe3c///7S4RHXQW7t6uMzf9hLRKiF39yznIjQ8b+ftzTsVdDKtUfT1NFLS6fng2aT5dUjNbxd2siv3zoxofMUljRweW4Sodbz/+nmZQ4O0gbzswWu/na0lnfKGvm/r5dO+FyvHKqhp9/BbXlpEz7XvBkxfPn6ebxyuIa/HDy/O2fAYfjS5v2DD2ptWM6sC6ZyTjYNexW0cuxTp0bOtuJaYDBYvJmx4aqiqZNTjZ0jDuZlJUWRZAtj3+npMUhbWNoIwMsHz1Dd0jWhcz2/r5Jcu42lGfHud/bAZ1bnsDQjnm/9+TB1bd3ntv9o23HePF7Pt9ctZGXOxV8fWcNeBa1zM3L8PEjb2dvPjpJ6rpydRO+Ag+f3Vo7rPIWlgyUSVo8Q9iLCssyEafFwlcNheLu0gctzBwPz9/84Ne5zVTZ3sutkE7cuS/NZ33mIdXB2TmfvAN94cbA75+WD1fzyzRPcvTKDTxRk+uR9vKVhr4JWZmIU1imwHu2OkgZ6+h08uGYOy7MS2LS7fMT+XHcKSxpIjY1gdvLIC5XkZcVzsqFj3L85BIriM2dp6ujlzuUZ3LQolU27y71e4H7IS/urALhl2cS7cFzNSYnmqzdcwvbiWn7w6nG++uxBlmcl8J11Cy/agOyFNOxV0Aq1WshMjPJ72G87UktsRAgrchJZvzKTkw0dvHOi0atzDDgMb59o4Ko59lHDYnnm9Hi4aug3nFVz7XxmdS5t3f38aU+F1+cxxvDCvioKchInZd2DT6/KYXlWAr988wSxkSH8akPehAaAJ8qjsBeRtSJyXERKReTrI7z+FREpFpGDIvKaiGS5vHaviJQ4/9zry8Yr5Y6/p1/2Dzh4/Vgt185PIdRq4cOLZxIXGcpTux43/M0AABLHSURBVMu9Ok9x9VlaOvtG7MIZsjg9nhCLBP0gbWFJA/NmRDMjNoIlGfGszE7kicKT9A84vDrPgYoWyho6uH2cc+vdsVqEH925hNVz7Wy8J5+UWM+eyJ0sbsNeRKzAo8BNwALgbhFZcMFu+4F8Y8xi4DngEeexicC3gQJgJfBtEUnwXfOVGluO3cbJhna/Le5RdLqZ5s4+bnDWTIkItXJ7XjrbjtRQ3zb6XOwL7SitB+CqOaOHfWSYlQWzYoO63767b4Ddp5pYNef9+fD/tDqHqpYu/nqkxqtzvbCvivAQCzdd5l09G29k2208eV8BS3w0+DsRntzZrwRKjTFlxpheYDNws+sOxpg3jDGdzm93AkM/Km8EthtjmowxzcB2YK1vmq6Ue7nJNrr7HNSc7Xa/8yTYXlxLmNXCB+a9H07rCzLoGzA8u9fzrofCkgbmp8a4XQYvLzOBdytavb7LDRR7TjXR2+847zec6y6dQXZSFI/tOOnxWEhP/wB/OVjNjQtTiYmYHsXjPAn7NMD1/8pK57bR3Ae84s2xInK/iBSJSFF9fb0HTVLKM/6cfmmMYVtxDVfNSSLapZrhnJQYCnIS2by7wqPfOLp6Byg61TxmF86QZZnxdPUNcKwmOB+uKixtINQqFOS+P3XRYhHuW5XDuxUtHndhvXGsnpbOPp/MrQ8UnoT9SKNBI/4fKiKfAPKBH3hzrDFmozEm3xiTn5w8/seVlbrQUPVLf/TbH69to6Kpi+sXDO8mWF+QSXlT57nBxrHsPtVE74BjWImEkSx3VsAM1q6cwpIG8jITiAo7vxTwHcsziI8K5bEdZR6d54V9lSTHhLNqjG6xYONJ2FcCrkvgpAPVF+4kItcB3wDWGWN6vDlWqckyIzacqDCrX9aj3XakFhG4bkHKsNfWLkol0RbGpl3uB2oLS+oJs1pYme3+QZy0+EhSYsKDcpC2sb2HI9VnR/wNJzLMyicKsthWXMspNz/Ymzp6eeN4HbcsnUWIdfpMSPTkSvcAc0UkR0TCgLuALa47iMgy4DcMBn2dy0uvAjeISIJzYPYG5zalLoqh9Wj90Y2zvbiWpRnxpMQMn4URHmLlzuXpbD9aS62b8YQdJQ0sz0ogMsz9tD0RYXlWcD5c9bZzuupog9SfvDKLUIuF3719cszzvHywmr4BM+4Kl4HKbdgbY/qBBxkM6aPAM8aYIyLykIisc+72AyAaeFZEDojIFuexTcB/MPgDYw/wkHObUheNP8K+uqWLQ1Wt3DBCF86Qu1dmMuAwPDPGHPH6th6O1bR5Ve88LzOBiqau8x7VDwaFJfXERoSwOH3kmS0pMRGsWzqLZ4oqx6yH9Py+Ki6dGculM2Mnq6lTkke/wxhjthpj5hljZhtjHnZu+5YxZijUrzPGzDDGLHX+Wedy7BPGmDnOP7+bnMtQanS5dhsVTZ309l+8GSp/OzpYC+f6BTNG3SfbbmPVHDub91SMWpr4HydGL5EwmryswTAMpjo5xhgKSxq4crYdq2X0J1D/aXUOXX0DPDVK91hpXTvvVrRw+zQamB0yfTqs1LSVk2zDYaC86eLd3W8vriU32caclJFLGwxZX5BJVUsXb71XN+LrO0oaiI8KZeGsOI/fe+GsOMKslqB6krasoYPq1m63v+HMT41l9Vw7v//HqRF/uL+4vxKLwLqlsyarqVOWhr0Keudm5FykgmitXX28c6JxzLv6IdcvmIE9OnzEgdqhu9mr3NzNXigi1MrCtNigGqQtLPH8N5zPrM6lrq2Hv7x7/lwQh8Pw0v5qVs9NHnEcJdhp2Kugl32R59q/ebyOfocZs79+SKjVwsdXpPP6sbphpXpP1LdTc9b93exIlmcmcLCq9aJ2XU2mwtIGMhIjyUqyud139Vw7l8yI4bEdZec9ZLXrZBNVLV3Tam69Kw17FfTiIkOxR4ddtLDfVlyLPTqcZR4+In/XikwMDCvmtcN5NzueueB5WQn09jsonuDKWFNB/4CDnScazyuRMBYR4b7VORyraePt0vcLzr2wr5Lo8BCPfggHIw17NS3k2G0XpRunp3+At47Xc/2CFCwedr1kJEbxgbnJbN5Tfl6Zg8KSBrKSosZVkTGYVq56t7KFtp5+rwapb146i+SY8HMPWXX1DrD10Bk+dFmqR1NYg5GGvZoWcu3RF+Up2ndONNLe0+9Rf72rDQWZ1J7t4fVjgwO1fQMOdpY1jvsJz9S4CNLiI4Nivv2OkgZE4IrcJI+PCQ+xcu8VWbz1Xj3v1baxrbiGjt6BaTe33pWGvZoWcpJtNLT3cLa7b1LfZ3txLVFhVq6c7V1IXzs/hdTYCDY5Sx/vL2+ho3fAq7vZCy3LjGd/ENzZF5Y0cFlaHAm2MK+O21CQRUSohd/uOMnz+6pIi4/06CnkYKVhr6aFoYJo7h6lnwiHw7C9uJar5yUTEepdV0GI1cLHVmTw1nv1VDhr5lgErvDyh4ar5VkJVLd2c6Z1Ymu0+lNbdx/7K1rG9RtOgi2MO5an8+L+KgpL6rktL83jrrVgpGGvpoXZHq5HO+AwHK5q5c8Hqrz+LeBgVSt1bT1ed+EMuWtFBgJs3lNOYUk9i9PjiYscf/ndoX77QH64amdZEwMOM64ZSQD3rcqlz+HAYeBWHy89GGhC3O+iVODLSIzCIsOrX/YPDM5Y2VXWxK6Tjew62URb9+B6povT43jy0wXERXkWuNuO1GC1CNfOH174zBOz4iO5dn4Km3dX0NLVx+evmT2u8wy5dGYs4SEW9p5u5sOLZ07oXP5SWFJPZKj1XDVPb+XYbaxbMovG9l5yR1m7d7rQsFfTQniIlfSEKErr2jhQ0cKuskZ2ljVSdKqZNudi1Tl2Gx++bCaX5yYhAl999iAbfruTP95XQHyU+/7i7cW1rMxO9Gjf0awvyORvRwcHaSdafjcsxMKS9PiAHqQtLG1gZU7ihNZu/enHl/qwRYFLw15NGzl2G1sP1bD10ODydbnJNj66dBYFOYlcnpvEjAvWCI2NCOWzf9zLhsd38cf7CsYcIDzZ0EFJXTvrCzIn1Mar56WQFh9Jc2cvyzInvoLnsqx4nig8SXffgNfjCL5yrOYs//nyUf7lhnleXdOZ1i5O1Hdw98qJ/Z2OtkD7dKNhr6aNz6zOJcduY3lWAgW5iW4fmV8zP4WN9yzn/if3sv7xXTz1TwUkjhL424sHf4CMt79+iNUi/Octi6hr6yYsZOJDasszE/jNQBmHq1rJ98NMlOLqs2x4fCfNnX0cr23j5S+sGvZDdTTnHiqbwIwk9T4doFXTxqq5dr6zbiEfXTLL49oo11ySwuOfzKesvp31j+2kqWPk0rnbi2tZMDOW9ATvH4C60Jr5KXx8xcTuZofk+XHlqiPVrWx4fCcRoVY23rOcjp5+PvvkXrr7Bjw6vrCkAXt0OJfMiJnklk4PGvZKufGBecn89t4VnGzoYP1jO2ls7znv9Yb2HopON0/4rn4y2KPDyUyMuuhP0h6uamXD47uIDLWy+f7LuWFhKj/+2FIOVLTw7y8ddrswuMNheLu0gVVzkrQbxkc07JXywKq5dp741ApONXaw/rFdNLgE/utH6zAGblg49cIecK5c1eI2YH3lUOVg0NvCQth8/xXnipetXZTKFz84l2f3VvKHd06PeY6jNWdp7Oj1aN1d5RkNe6U8dNWcwcA/3dTB3Rt3Ut82GPjbimtIi49kwRRd+SgvM576th4qmyf/4aqDlS1seHwn0eEhbL7/cjKTzu/W+ucPzuW6S2fw0MvFvHOicZSzvF/SeDotCD7ZNOyV8sKVs+387lMrqWzu4u7HdnK6sYMdJQ1cv2DGlO1uuFj99gcqWtjw+C5iI0PZfP/lIxZws1iEn3x8CTl2Gw9s2kdlc+eI5yosbWBuSjSpcdOv7vxk0bBXyktXzE7if/7XCqpbuvjI/y2kp9/BDVOwv37IJTNiiAqzsudUE919A27/9PR7NoDqan95M/c8vov4qFD+9NkrxqzUGRMRysZ7ltM34OCzT+6lq/f89+vuG2D3ySadheNjOvVSqXEoyE3i959eyaee2E1cZCgrcqZuga0Qq4WlGfH8cWc5f9w58tqsF5o3I5qCnCQKchMpyEkiOSZ81H33nm7m3id2k2gLY/P9lzMrPtLt+XOTo/n5Xcv49O/38K/PH+Tndy0995vR3tPN9PQ7JlQETg2nYa/UOK3ITuSlB66iraefUOvU/iX5Wx9dwBvH6j3at7tvgAMVLbywr5Indw4OpM5OtlGQm8TluUlcnpNIinOu/N7TTdz7xB6SogeDfmac+6AfsmZ+Cl+98RIe+etxFs2K5bNXD5aH2FHSQIhFKMjxvKSxck/DXqkJmBsgc8Dnp8YyP9W7AeT+AQeHq8+ys6yRXWWNbDlQfW6t3By7jfysBLYeOkNKbARPf+bycfWv/39Xz+ZI9Vm+/9djzJ8Zy9XzkiksrScvMwFbuMaTL+nfplJqREPdP0sz4vnc1bPPKxq3s6yRvx6pISMxit9/eqXHT8VeSET4wR2LOVHXzhc27eN/Pr2SI9Vn+fJ183x8NUou1txbT+Xn55uioiJ/N0Mp5YbDYRDxTe2ZiqZO1v1icLC7s3eAFz5/5bkSzcozIrLXGJM/2utTu6NRKTVlWSzis+mmGYlRPLo+j55+BzERISxOi/PJedX7tBtHKTUlXDnHzk8+vpSu3n5CpviAdyDSsFdKTRnrlszydxOClv74VEqpacCjsBeRtSJyXERKReTrI7z+ARHZJyL9InLHBa8NiMgB558tvmq4Ukopz7ntxhERK/AocD1QCewRkS3GmGKX3cqBTwH/e4RTdBljdF0wpZTyI0/67FcCpcaYMgAR2QzcDJwLe2PMKedrjkloo1JKqQnypBsnDahw+b7Suc1TESJSJCI7ReSWkXYQkfud+xTV13v2SLdSSinPeRL2I02k9eZJrEznRP/1wE9FZPawkxmz0RiTb4zJT07WxQqUUsrXPAn7SiDD5ft0oNrTNzDGVDv/Wwa8CSzzon1KKaV8wJOw3wPMFZEcEQkD7gI8mlUjIgkiEu782g5chUtfv1JKqYvDo9o4IvIh4KeAFXjCGPOwiDwEFBljtojICuBFIAHoBmqMMQtF5ErgN4CDwR8sPzXG/NbNe9UDYy9QOTY70DCB46eaYLseCL5rCrbrgeC7pmC7Hhh+TVnGmFH7wadcIbSJEpGisYoBBZpgux4IvmsKtuuB4LumYLse8P6a9AlapZSaBjTslVJqGgjGsN/o7wb4WLBdDwTfNQXb9UDwXVOwXQ94eU1B12evlFJquGC8s1dKKXUBDXullJoGgibs3ZVhDkQickpEDjnLQwfcwrwi8oSI1InIYZdtiSKyXURKnP8NqIVGR7mm74hIlUsp7w/5s43eEJEMEXlDRI6KyBER+ZJze0B+TmNcTyB/RhEisltE3nVe03ed23NEZJfzM/qT86HX0c8TDH32zjLM7+FShhm4+4IyzAFHRE4B+caYgHwYREQ+ALQDfzDGLHJuewRoMsZ8z/lDOcEY8zV/ttMbo1zTd4B2Y8wP/dm28RCRmcBMY8w+EYkB9gK3MFiyPOA+pzGu52ME7mckgM0Y0y4ioUAh8CXgK8ALxpjNIvJr4F1jzK9GO0+w3NmfK8NsjOkFhsowKz8yxvwdaLpg883A751f/57Bf4gBY5RrCljGmDPGmH3Or9uAowxWtQ3Iz2mM6wlYZlC789tQ5x8DXAs859zu9jMKlrCfaBnmqcoA20Rkr4jc7+/G+MgMY8wZGPyHCaT4uT2+8qCIHHR28wREl8eFRCSbwUKFuwiCz+mC64EA/oxExCoiB4A6YDtwAmgxxvQ7d3GbecES9hMtwzxVXWWMyQNuAh5wdiGoqedXwGxgKXAG+JF/m+M9EYkGngf+2Rhz1t/tmagRriegPyNjzIBzxb90BnsyLh1pt7HOESxhP6EyzFOVS3noOgYLza30b4t8otbZrzrUv1rn5/ZMmDGm1vmP0QE8RoB9Ts5+4OeBp4wxLzg3B+znNNL1BPpnNMQY08JgqfjLgXgRGVpt0G3mBUvYj7sM81QlIjbnABMiYgNuAA6PfVRA2ALc6/z6XuDPfmyLTwyFotOtBNDn5Bz8+y1w1BjzY5eXAvJzGu16AvwzShaReOfXkcB1DI5FvAHc4dzN7WcUFLNxYOQyzH5u0oSISC6Dd/MwuFbwpkC7JhF5GriGwVKstcC3gZeAZ4BMBheqv9MYEzADnqNc0zUMdg8Y4BTw2aH+7qlORFYBO4BDDJYiB/g/DPZzB9znNMb13E3gfkaLGRyAtTJ4g/6MMeYhZ0ZsBhKB/cAnjDE9o54nWMJeKaXU6IKlG0cppdQYNOyVUmoa0LBXSqlpQMNeKaWmAQ17pZSaBjTslVJqGtCwV0qpaeD/AaYKQ1Dg1s/SAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "acc_coef = np.load(r\"./stratified_cross_validation_results/densenets/multinomial_acc_coef_360_-12_30.npy\", allow_pickle=True)\n",
    "raw_outputs = np.load(r\"./stratified_cross_validation_results/densenets/multinomial_raw_outputs_360_-12_30.npy\", allow_pickle=True)\n",
    "print(len(acc_coef))\n",
    "\n",
    "\n",
    "trainable_sequence = np.array([429, 424, 423, 422, 421, 420, 419, 418, 417, 416, 415, 414, 413, 412,\n",
    "       411, 410, 409, 408, 407, 406, 405, 404, 403, 402, 401, 400, 399,\n",
    "       398, 397, 396, 395, 394, 393, 392, 391, 390, 389, 388, 387, 386,\n",
    "       385, 384, 383, 382, 381, 380, 379, 378, 377, 376, 375, 374, 373,\n",
    "       372, 371, 370, 369, 368, 367, 366, 365, 364, 363, 362, 361, 360,\n",
    "       359, 358, 357, 356, 355, 354, 353, 352, 351, 350, 349, 348, 347,\n",
    "       346, 345, 344, 343, 342, 341, 340, 339, 338, 337, 336, 335, 334,\n",
    "       333, 332, 331, 330, 329, 328, 327, 326, 325, 324, 323, 322, 321,\n",
    "       320, 319, 318, 317, 316, 315, 314, 313, 311, 308, 307, 306, 305,\n",
    "       304, 303, 302, 301, 300, 299, 298, 297, 296, 295, 294, 293, 292,\n",
    "       291, 290, 289, 288, 287, 286, 285, 284, 283, 282, 281, 280, 279,\n",
    "       278, 277, 276, 275, 274, 273, 272, 271, 270, 269, 268, 267, 266,\n",
    "       265, 264, 263, 262, 261, 260, 259, 258, 257, 256, 255, 254, 253,\n",
    "       252, 251, 250, 249, 248, 247, 246, 245, 244, 243, 242, 241, 240,\n",
    "       239, 238, 237, 236, 235, 234, 233, 232, 231, 230, 229, 228, 227,\n",
    "       226, 225, 224, 223, 222, 221, 220, 219, 218, 217, 216, 215, 214,\n",
    "       213, 212, 211, 210, 209, 208, 207, 206, 205, 204, 203, 202, 201,\n",
    "       200, 199, 198, 197, 196, 195, 194, 193, 192, 191, 190, 189, 188,\n",
    "       187, 186, 185, 184, 183, 182, 181, 180, 179, 178, 177, 176, 175,\n",
    "       174, 173, 172, 171, 170, 169, 168, 167, 166, 165, 164, 163, 162,\n",
    "       161, 160, 159, 158, 157, 156, 155, 154, 153, 152, 151, 150, 149,\n",
    "       148, 147, 146, 145, 144, 143, 142, 141, 139, 136, 135, 134, 133,\n",
    "       132, 131, 130, 129, 128, 127, 126, 125, 124, 123, 122, 121, 120,\n",
    "       119, 118, 117, 116, 115, 114, 113, 112, 111, 110, 109, 108, 107,\n",
    "       106, 105, 104, 103, 102, 101, 100,  99,  98,  97,  96,  95,  94,\n",
    "        93,  92,  91,  90,  89,  88,  87,  86,  85,  84,  83,  82,  81,\n",
    "        80,  79,  78,  77,  76,  75,  74,  73,  72,  71,  70,  69,  68,\n",
    "        67,  66,  65,  64,  63,  62,  61,  60,  59,  58,  57,  56,  55,\n",
    "        54,  53,  51,  48,  47,  46,  45,  44,  43,  42,  41,  40,  39,\n",
    "        38,  37,  36,  35,  34,  33,  32,  31,  30,  29,  28,  27,  26,\n",
    "        25,  24,  23,  22,  21,  20,  19,  18,  17,  16,  15,  14,  13,\n",
    "        12,  11,  10,   9,   8,   7,   4,   3,   2])\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def show_confusion_matrix(raw_outputs, fine_tune_layers):\n",
    "\n",
    "    y_true = np.sum(np.vstack(raw_outputs[fine_tune_layers, :, 1]), axis=1)\n",
    "    y_pred = np.sum(\n",
    "        np.rint(np.vstack(raw_outputs[fine_tune_layers, :, 2])), axis=1\n",
    "    ).astype(int)\n",
    "    return confusion_matrix(y_true, y_pred)\n",
    "\n",
    "def show_matrix_percentage(confusion_matrix):\n",
    "    return np.transpose(np.transpose(my_confusion_matrix) / np.sum(my_confusion_matrix, axis=1))\n",
    "\n",
    "# total accuracy\n",
    "def calculate_accuracy(my_confusion_matrix):\n",
    "    return np.trace(my_confusion_matrix)/np.sum(my_confusion_matrix)\n",
    "\n",
    "max_acc_layer = np.argmax([calculate_accuracy(show_confusion_matrix(raw_outputs, i))  for i in range(len(acc_coef))])\n",
    "\n",
    "my_confusion_matrix = show_confusion_matrix(raw_outputs,max_acc_layer)\n",
    "print(my_confusion_matrix)\n",
    "print(\"+++++++++++++++++++++++++++++++++\")\n",
    "print(\"+++++++++++++++++++++++++++++++++\")\n",
    "print(show_matrix_percentage(my_confusion_matrix))\n",
    "print(\"+++++++++++++++++++++++++++++++++\")\n",
    "print(\"+++++++++++++++++++++++++++++++++\")\n",
    "print(calculate_accuracy(my_confusion_matrix)*100)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot([i for i in range(len(acc_coef))],[calculate_accuracy(show_confusion_matrix(raw_outputs, i))  for i in range(len(acc_coef))])\n",
    "print(\"+++++++++++++++++++++++++++++++++\")\n",
    "print(\"+++++++++++++++++++++++++++++++++\")\n",
    "print(f\"max accuracy with tuning from {trainable_sequence[360:0:-12][max_acc_layer]} layers, or tune {430 - trainable_sequence[360:0:-12][max_acc_layer]} layers\")\n",
    "print(max_acc_layer, [show_matrix_percentage(my_confusion_matrix)[i,i]*100 for i in range(5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170, 182, 194, 206, 218, 230, 242, 254, 266, 278, 290, 302, 317, 329, 341, 353, 365, 377, 389, 401, 413, "
     ]
    }
   ],
   "source": [
    "for i in trainable_sequence[360:0:-12][9:]:\n",
    "    print(i, end=\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# are we overffitng on the validation set?\n",
    "# is validation set too small to measure performance accurately\n",
    "# augment validation set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 56 validated image filenames.\n",
      "1/1 [==============================] - 1s 579ms/step - loss: 0.3455 - soft_acc_multi_output: 0.5357\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.345540314912796, 0.53571427]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set = data_gen.flow_from_dataframe(\n",
    "    dataframe=response,\n",
    "    directory=DATADIR,\n",
    "    x_col=\"GreenID\",\n",
    "    target_size=(224, 224),\n",
    "    color_mode=\"rgb\",\n",
    "    subset=\"validation\",\n",
    "    shuffle=True,\n",
    "    batch_size=56,\n",
    "    y_col=[0, 1, 2, 3,],\n",
    "    class_mode=\"raw\",\n",
    ")\n",
    "\n",
    "full_model.evaluate(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(test_set)\n",
    "true_labels = batch[1]\n",
    "predictions = full_model.predict(batch[0])\n",
    "# np.corrcoef(predictions, true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 4, 4, 3, 2, 0, 3, 4, 2, 4, 3, 1, 1, 4, 1, 3, 3, 0, 3, 4, 2,\n",
       "       1, 1, 1, 1, 2, 3, 1, 2, 4, 0, 1, 0, 0, 2, 4, 2, 0, 1, 1, 4, 4, 0,\n",
       "       1, 3, 3, 2, 0, 0, 1, 3, 1, 4, 0, 1])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(true_labels, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.81680143],\n",
       "       [0.81680143, 1.        ]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(np.sum(predictions, axis=1), np.sum(true_labels, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/124\n",
      "16/16 [==============================] - 8s 506ms/step - loss: 0.5716 - soft_acc_multi_output: 0.2405 - val_loss: 0.7388 - val_soft_acc_multi_output: 0.1429\n",
      "Epoch 26/124\n",
      "16/16 [==============================] - 7s 451ms/step - loss: 0.4433 - soft_acc_multi_output: 0.3481 - val_loss: 1.3936 - val_soft_acc_multi_output: 0.1250\n",
      "Epoch 27/124\n",
      "16/16 [==============================] - 7s 458ms/step - loss: 0.4343 - soft_acc_multi_output: 0.3678 - val_loss: 1.7708 - val_soft_acc_multi_output: 0.1429\n",
      "Epoch 28/124\n",
      "16/16 [==============================] - 8s 470ms/step - loss: 0.3757 - soft_acc_multi_output: 0.4168 - val_loss: 1.7867 - val_soft_acc_multi_output: 0.1429\n",
      "Epoch 29/124\n",
      "16/16 [==============================] - 7s 455ms/step - loss: 0.3500 - soft_acc_multi_output: 0.4820 - val_loss: 2.6157 - val_soft_acc_multi_output: 0.1429\n",
      "Epoch 30/124\n",
      "16/16 [==============================] - 7s 460ms/step - loss: 0.3462 - soft_acc_multi_output: 0.4759 - val_loss: 3.0317 - val_soft_acc_multi_output: 0.1429\n",
      "Epoch 31/124\n",
      "16/16 [==============================] - 7s 456ms/step - loss: 0.2993 - soft_acc_multi_output: 0.5550 - val_loss: 3.2755 - val_soft_acc_multi_output: 0.1429\n",
      "Epoch 32/124\n",
      "16/16 [==============================] - 7s 457ms/step - loss: 0.3034 - soft_acc_multi_output: 0.5374 - val_loss: 3.0485 - val_soft_acc_multi_output: 0.1429\n",
      "Epoch 33/124\n",
      "16/16 [==============================] - 7s 464ms/step - loss: 0.2530 - soft_acc_multi_output: 0.6152 - val_loss: 3.2520 - val_soft_acc_multi_output: 0.1429\n",
      "Epoch 34/124\n",
      "16/16 [==============================] - 8s 475ms/step - loss: 0.2851 - soft_acc_multi_output: 0.5782 - val_loss: 3.3950 - val_soft_acc_multi_output: 0.1429\n",
      "Epoch 35/124\n",
      "16/16 [==============================] - 7s 460ms/step - loss: 0.2559 - soft_acc_multi_output: 0.5855 - val_loss: 3.3589 - val_soft_acc_multi_output: 0.1429\n",
      "Epoch 36/124\n",
      "16/16 [==============================] - 8s 477ms/step - loss: 0.2531 - soft_acc_multi_output: 0.5761 - val_loss: 3.2883 - val_soft_acc_multi_output: 0.1429\n",
      "Epoch 37/124\n",
      "16/16 [==============================] - 7s 458ms/step - loss: 0.2344 - soft_acc_multi_output: 0.6155 - val_loss: 3.3408 - val_soft_acc_multi_output: 0.1429\n",
      "Epoch 38/124\n",
      "16/16 [==============================] - 7s 468ms/step - loss: 0.2331 - soft_acc_multi_output: 0.6452 - val_loss: 3.4517 - val_soft_acc_multi_output: 0.1429\n",
      "Epoch 39/124\n",
      "16/16 [==============================] - 8s 489ms/step - loss: 0.2262 - soft_acc_multi_output: 0.6377 - val_loss: 3.5718 - val_soft_acc_multi_output: 0.1429\n",
      "Epoch 40/124\n",
      "16/16 [==============================] - 7s 457ms/step - loss: 0.2212 - soft_acc_multi_output: 0.6459 - val_loss: 3.2559 - val_soft_acc_multi_output: 0.1429\n",
      "Epoch 41/124\n",
      "16/16 [==============================] - 7s 465ms/step - loss: 0.2254 - soft_acc_multi_output: 0.6406 - val_loss: 3.3388 - val_soft_acc_multi_output: 0.1429\n",
      "Epoch 42/124\n",
      "16/16 [==============================] - 7s 457ms/step - loss: 0.2045 - soft_acc_multi_output: 0.6695 - val_loss: 3.3549 - val_soft_acc_multi_output: 0.1429\n",
      "Epoch 43/124\n",
      "16/16 [==============================] - 7s 460ms/step - loss: 0.2106 - soft_acc_multi_output: 0.6569 - val_loss: 3.3338 - val_soft_acc_multi_output: 0.1429\n",
      "Epoch 44/124\n",
      "16/16 [==============================] - 8s 479ms/step - loss: 0.1852 - soft_acc_multi_output: 0.7326 - val_loss: 3.3161 - val_soft_acc_multi_output: 0.1429\n",
      "Epoch 45/124\n",
      "16/16 [==============================] - 7s 468ms/step - loss: 0.2078 - soft_acc_multi_output: 0.7045 - val_loss: 3.2205 - val_soft_acc_multi_output: 0.1429\n",
      "Epoch 46/124\n",
      "16/16 [==============================] - 8s 481ms/step - loss: 0.1851 - soft_acc_multi_output: 0.6890 - val_loss: 3.4848 - val_soft_acc_multi_output: 0.1429\n"
     ]
    }
   ],
   "source": [
    "### city of overfit\n",
    "########################################\n",
    "# full_model.trainable = True\n",
    "# for layer in full_model.layers[:-12]:\n",
    "#     layer.trainable = False\n",
    "# for layer in full_model.layers[-12:]:\n",
    "#     layer.trainable = True\n",
    "\n",
    "# validation_set.reset()\n",
    "# train_set.reset()\n",
    "\n",
    "\n",
    "# full_model.compile(optimizer=keras.optimizers.Nadam(), loss='binary_crossentropy', metrics=[soft_acc_multi_output])\n",
    "# logdir_name = r\".\\tfb\\logs\\densenet_multinomial_logit\\\\\" + full_model.name + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# tensorboard_callback = TensorBoard(log_dir=logdir_name)\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=21,restore_best_weights=True)\n",
    "# reduce_lr_plateau = ReduceLROnPlateau(monitor='val_loss', patience = 7, factor = 0.5)\n",
    "\n",
    "# history_fine = full_model.fit_generator(generator=train_set,\n",
    "#                     epochs=100+history_full_model.epoch[-1],\n",
    "# #                     verbose=2,\n",
    "#                     validation_data=validation_set,\n",
    "#                     initial_epoch = history_full_model.epoch[-1],\n",
    "#                     callbacks = [tensorboard_callback,early_stopping, reduce_lr_plateau]\n",
    "#                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(filepath=\"./saved_models/my_multinomial/1/\", save_format=\"h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\n",
    "    filepath=\"./saved_models/my_effnet/1/\",\n",
    "    custom_objects={\"soft_acc_multi_output\": soft_acc_multi_output},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=80)\n",
    "batch = next(validation_set)\n",
    "\n",
    "y_true = batch[1]\n",
    "y_pred = full_model.predict(batch[0])\n",
    "print(soft_acc_multi_output(y_true, y_pred))\n",
    "\n",
    "# print examples from the validation set\n",
    "for i in range(len(batch[1])):\n",
    "    img = batch[0][i]\n",
    "    label = batch[1][i]\n",
    "\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    print(\n",
    "        f\"true label: {label}; rounded pred: {y_pred[i]}; Correct: {K.all(K.equal(K.cast(K.round(label),'int32'), K.cast(K.round(y_pred[i]),'int32')))}\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
