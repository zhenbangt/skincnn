{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "IMG_WIDTH, IMG_HEIGHT = 224, 224\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.applications.densenet import (\n",
    "    DenseNet121,\n",
    "    preprocess_input,\n",
    ")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import IPython.display as display\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense,\n",
    "    GlobalAveragePooling2D,\n",
    "    Conv2D,\n",
    "    Flatten,\n",
    "    GlobalMaxPooling2D,\n",
    "    Dropout,\n",
    ")\n",
    "\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam, Nadam\n",
    "from datetime import datetime\n",
    "from packaging import version\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import (\n",
    "    TensorBoard,\n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint,\n",
    "    ReduceLROnPlateau,\n",
    ")\n",
    "import efficientnet.tfkeras as enet\n",
    "\n",
    "\n",
    "def append_extension(fn):\n",
    "    return (fn + \".jpg\").zfill(7)\n",
    "\n",
    "\n",
    "def ordered_logit(class_number):\n",
    "    # zero portability\n",
    "    target = np.zeros(4, dtype=int)\n",
    "    target[: class_number - 2] = 1\n",
    "    return target\n",
    "\n",
    "\n",
    "DATADIR = r\"./adult\"\n",
    "CSV_PATH = r\"./adult/CastControls_ALP.xlsx\"\n",
    "response = pd.read_excel(CSV_PATH, sheet_name=0,)[[\"GreenID\", \"Grade\"]].dropna(\n",
    "    axis=0, subset=[\"Grade\"]\n",
    ")\n",
    "response.Grade = response.Grade.astype(\"int\")\n",
    "response.GreenID = response.GreenID.astype(\"str\").apply(append_extension)\n",
    "response = response[response.Grade != 99]\n",
    "response = pd.concat(\n",
    "    [response, pd.DataFrame.from_dict(dict(response.Grade.apply(ordered_logit))).T,],\n",
    "    axis=1,\n",
    ")\n",
    "response.Grade = response.Grade.astype(\"str\")\n",
    "response = response.sample(frac=1)\n",
    "seed = np.random.randint(30027)\n",
    "\n",
    "\n",
    "def soft_acc(y_true, y_pred):\n",
    "    return K.mean(K.equal(K.round(y_true), K.round(y_pred)))\n",
    "\n",
    "\n",
    "def soft_acc_multi_output(y_true, y_pred):\n",
    "    return K.mean(\n",
    "        K.all(\n",
    "            K.equal(\n",
    "                K.cast(K.round(y_true), \"int32\"), K.cast(K.round(y_pred), \"int32\"),\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "    )\n",
    "\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "policy = tf.keras.mixed_precision.experimental.Policy(\"mixed_float16\")\n",
    "mixed_precision.experimental.set_policy(policy)\n",
    "\n",
    "# gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "# if gpus:\n",
    "#     try:\n",
    "#         # Currently, memory growth needs to be the same across GPUs\n",
    "#         for gpu in gpus:\n",
    "#             tf.config.experimental.set_memory_growth(gpu, True)\n",
    "#         logical_gpus = tf.config.experimental.list_logical_devices(\"GPU\")\n",
    "#         print(\n",
    "#             len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\",\n",
    "#         )\n",
    "#     except RuntimeError as e:\n",
    "#         # Memory growth must be set before GPUs have been initialized\n",
    "#         print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "train_gen = ImageDataGenerator(\n",
    "    rotation_range=5,\n",
    "    fill_mode=\"reflect\",\n",
    "    horizontal_flip=True,\n",
    "    #     vertical_flip=True,\n",
    "    validation_split=0.1,\n",
    "    # dude i wasnt cheating...\n",
    "    rescale=1.0 / 255.0,\n",
    "    #     preprocessing_function = preprocess_input\n",
    "    zoom_range=0.1,\n",
    ")\n",
    "\n",
    "valid_gen = ImageDataGenerator(validation_split=0.1, rescale=1.0 / 255.0,)\n",
    "\n",
    "train_set = train_gen.flow_from_dataframe(\n",
    "    dataframe=response,\n",
    "    directory=DATADIR,\n",
    "    x_col=\"GreenID\",\n",
    "    target_size=(224, 224),\n",
    "    color_mode=\"rgb\",\n",
    "    subset=\"training\",\n",
    "    shuffle=True,\n",
    "    #     class_mode = \"sparse\"\n",
    "    #     y_col=\"Grade\",\n",
    "    y_col=[0, 1, 2, 3,],\n",
    "    class_mode=\"raw\",\n",
    ")\n",
    "\n",
    "validation_set = valid_gen.flow_from_dataframe(\n",
    "    dataframe=response,\n",
    "    directory=DATADIR,\n",
    "    x_col=\"GreenID\",\n",
    "    target_size=(224, 224),\n",
    "    color_mode=\"rgb\",\n",
    "    subset=\"validation\",\n",
    "    shuffle=True,\n",
    "    batch_size=28,\n",
    "    #     class_mode = \"sparse\"\n",
    "    #     y_col=\"Grade\",\n",
    "    y_col=[0, 1, 2, 3,],\n",
    "    class_mode=\"raw\",\n",
    ")\n",
    "\n",
    "train_set.reset()\n",
    "validation_set.reset()\n",
    "# print(next(validation_set)[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-4e887602e298>:25: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 16 steps, validate for 2 steps\n",
      "Epoch 1/25\n",
      "16/16 [==============================] - 11s 669ms/step - loss: 0.6069 - soft_acc_multi_output: 0.2124 - val_loss: 0.4696 - val_soft_acc_multi_output: 0.3571\n",
      "Epoch 2/25\n",
      "16/16 [==============================] - 5s 305ms/step - loss: 0.4838 - soft_acc_multi_output: 0.3115 - val_loss: 0.3935 - val_soft_acc_multi_output: 0.4286\n",
      "Epoch 3/25\n",
      "16/16 [==============================] - 5s 307ms/step - loss: 0.4403 - soft_acc_multi_output: 0.3268 - val_loss: 0.3645 - val_soft_acc_multi_output: 0.4643\n",
      "Epoch 4/25\n",
      "16/16 [==============================] - 5s 305ms/step - loss: 0.4071 - soft_acc_multi_output: 0.3978 - val_loss: 0.3524 - val_soft_acc_multi_output: 0.4821\n",
      "Epoch 5/25\n",
      "16/16 [==============================] - 5s 304ms/step - loss: 0.4045 - soft_acc_multi_output: 0.3818 - val_loss: 0.3363 - val_soft_acc_multi_output: 0.4821\n",
      "Epoch 6/25\n",
      "16/16 [==============================] - 5s 315ms/step - loss: 0.3879 - soft_acc_multi_output: 0.3942 - val_loss: 0.3301 - val_soft_acc_multi_output: 0.5357\n",
      "Epoch 7/25\n",
      "16/16 [==============================] - 5s 313ms/step - loss: 0.3993 - soft_acc_multi_output: 0.4017 - val_loss: 0.3252 - val_soft_acc_multi_output: 0.5179\n",
      "Epoch 8/25\n",
      "16/16 [==============================] - 5s 309ms/step - loss: 0.3769 - soft_acc_multi_output: 0.4029 - val_loss: 0.3267 - val_soft_acc_multi_output: 0.5000\n",
      "Epoch 9/25\n",
      "16/16 [==============================] - 5s 315ms/step - loss: 0.3710 - soft_acc_multi_output: 0.4532 - val_loss: 0.3134 - val_soft_acc_multi_output: 0.5000\n",
      "Epoch 10/25\n",
      "16/16 [==============================] - 5s 308ms/step - loss: 0.3648 - soft_acc_multi_output: 0.4408 - val_loss: 0.3185 - val_soft_acc_multi_output: 0.5000\n",
      "Epoch 11/25\n",
      "16/16 [==============================] - 5s 316ms/step - loss: 0.3566 - soft_acc_multi_output: 0.4344 - val_loss: 0.3059 - val_soft_acc_multi_output: 0.5000\n",
      "Epoch 12/25\n",
      "16/16 [==============================] - 5s 308ms/step - loss: 0.3598 - soft_acc_multi_output: 0.4314 - val_loss: 0.3156 - val_soft_acc_multi_output: 0.5179\n",
      "Epoch 13/25\n",
      "16/16 [==============================] - 5s 307ms/step - loss: 0.3551 - soft_acc_multi_output: 0.4807 - val_loss: 0.2991 - val_soft_acc_multi_output: 0.4821\n",
      "Epoch 14/25\n",
      "16/16 [==============================] - 5s 308ms/step - loss: 0.3533 - soft_acc_multi_output: 0.4884 - val_loss: 0.3131 - val_soft_acc_multi_output: 0.5179\n",
      "Epoch 15/25\n",
      "16/16 [==============================] - 5s 315ms/step - loss: 0.3763 - soft_acc_multi_output: 0.4321 - val_loss: 0.2990 - val_soft_acc_multi_output: 0.5000\n",
      "Epoch 16/25\n",
      "16/16 [==============================] - 5s 311ms/step - loss: 0.3663 - soft_acc_multi_output: 0.4442 - val_loss: 0.3008 - val_soft_acc_multi_output: 0.5179\n",
      "Epoch 17/25\n",
      "16/16 [==============================] - 5s 308ms/step - loss: 0.3455 - soft_acc_multi_output: 0.4797 - val_loss: 0.2985 - val_soft_acc_multi_output: 0.5179\n",
      "Epoch 18/25\n",
      "16/16 [==============================] - 5s 309ms/step - loss: 0.3512 - soft_acc_multi_output: 0.4610 - val_loss: 0.2978 - val_soft_acc_multi_output: 0.5000\n",
      "Epoch 19/25\n",
      "16/16 [==============================] - 5s 306ms/step - loss: 0.3634 - soft_acc_multi_output: 0.4452 - val_loss: 0.2891 - val_soft_acc_multi_output: 0.5536\n",
      "Epoch 20/25\n",
      "16/16 [==============================] - 5s 305ms/step - loss: 0.3527 - soft_acc_multi_output: 0.4789 - val_loss: 0.3024 - val_soft_acc_multi_output: 0.5357\n",
      "Epoch 21/25\n",
      "16/16 [==============================] - 5s 302ms/step - loss: 0.3424 - soft_acc_multi_output: 0.4962 - val_loss: 0.2897 - val_soft_acc_multi_output: 0.5536\n",
      "Epoch 22/25\n",
      "16/16 [==============================] - 5s 305ms/step - loss: 0.3355 - soft_acc_multi_output: 0.5012 - val_loss: 0.3051 - val_soft_acc_multi_output: 0.5179\n",
      "Epoch 23/25\n",
      "16/16 [==============================] - 5s 305ms/step - loss: 0.3406 - soft_acc_multi_output: 0.5098 - val_loss: 0.2981 - val_soft_acc_multi_output: 0.5179\n",
      "Epoch 24/25\n",
      "16/16 [==============================] - 5s 306ms/step - loss: 0.3374 - soft_acc_multi_output: 0.4695 - val_loss: 0.2924 - val_soft_acc_multi_output: 0.5536\n",
      "Epoch 25/25\n",
      "16/16 [==============================] - 5s 307ms/step - loss: 0.3361 - soft_acc_multi_output: 0.4848 - val_loss: 0.2859 - val_soft_acc_multi_output: 0.5357\n"
     ]
    }
   ],
   "source": [
    "conv_base = enet.EfficientNetB0(\n",
    "    include_top=False, input_shape=(224, 224, 3), pooling=\"avg\", weights=\"imagenet\",\n",
    ")\n",
    "conv_base.trainable = False\n",
    "\n",
    "x = conv_base.output\n",
    "x = Dropout(0.5)(x)\n",
    "preds = Dense(4, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=conv_base.input, outputs=preds)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[soft_acc_multi_output],\n",
    ")\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=21, restore_best_weights=True,\n",
    ")\n",
    "reduce_lr_plateau = ReduceLROnPlateau(monitor=\"val_loss\", patience=7, factor=0.8)\n",
    "\n",
    "history_1 = model.fit_generator(\n",
    "    generator=train_set,\n",
    "    epochs=25,\n",
    "    validation_data=validation_set,\n",
    "    callbacks=[early_stopping, reduce_lr_plateau],\n",
    "    #     verbose=0,\n",
    ")\n",
    "\n",
    "# model.save(\n",
    "#     filepath=\"./saved_models/my_effnet/my_effnet_untuned_1_layer.h5\", save_format=\"h5\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from 227 of 233 layers; 5 trainables variables\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 16 steps, validate for 2 steps\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 11s 679ms/step - loss: 0.5760 - soft_acc_multi_output: 0.3372 - val_loss: 0.3130 - val_soft_acc_multi_output: 0.5536\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 5s 311ms/step - loss: 0.3600 - soft_acc_multi_output: 0.5082 - val_loss: 0.3111 - val_soft_acc_multi_output: 0.5714\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 5s 303ms/step - loss: 0.3116 - soft_acc_multi_output: 0.5399 - val_loss: 0.3271 - val_soft_acc_multi_output: 0.5536\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 5s 312ms/step - loss: 0.2968 - soft_acc_multi_output: 0.5624 - val_loss: 0.3075 - val_soft_acc_multi_output: 0.5714\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 5s 316ms/step - loss: 0.2799 - soft_acc_multi_output: 0.5781 - val_loss: 0.3050 - val_soft_acc_multi_output: 0.5536\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 5s 315ms/step - loss: 0.2575 - soft_acc_multi_output: 0.6148 - val_loss: 0.3134 - val_soft_acc_multi_output: 0.5714\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 5s 312ms/step - loss: 0.2519 - soft_acc_multi_output: 0.6398 - val_loss: 0.3286 - val_soft_acc_multi_output: 0.5536\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 5s 314ms/step - loss: 0.2520 - soft_acc_multi_output: 0.6171 - val_loss: 0.2865 - val_soft_acc_multi_output: 0.5357\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 5s 313ms/step - loss: 0.2293 - soft_acc_multi_output: 0.6464 - val_loss: 0.3166 - val_soft_acc_multi_output: 0.5357\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 5s 311ms/step - loss: 0.2342 - soft_acc_multi_output: 0.6667 - val_loss: 0.3107 - val_soft_acc_multi_output: 0.5357\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 5s 312ms/step - loss: 0.2185 - soft_acc_multi_output: 0.6832 - val_loss: 0.3253 - val_soft_acc_multi_output: 0.5000\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 5s 312ms/step - loss: 0.2164 - soft_acc_multi_output: 0.6725 - val_loss: 0.3073 - val_soft_acc_multi_output: 0.5714\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 5s 314ms/step - loss: 0.2137 - soft_acc_multi_output: 0.6510 - val_loss: 0.3030 - val_soft_acc_multi_output: 0.5714\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 5s 311ms/step - loss: 0.1913 - soft_acc_multi_output: 0.7256 - val_loss: 0.2914 - val_soft_acc_multi_output: 0.5893\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 5s 310ms/step - loss: 0.1933 - soft_acc_multi_output: 0.7194 - val_loss: 0.3099 - val_soft_acc_multi_output: 0.5179\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 5s 309ms/step - loss: 0.1885 - soft_acc_multi_output: 0.7373 - val_loss: 0.3176 - val_soft_acc_multi_output: 0.5179\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 5s 311ms/step - loss: 0.1763 - soft_acc_multi_output: 0.7526 - val_loss: 0.3385 - val_soft_acc_multi_output: 0.5536\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 5s 302ms/step - loss: 0.1832 - soft_acc_multi_output: 0.7404 - val_loss: 0.3142 - val_soft_acc_multi_output: 0.5179\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 5s 302ms/step - loss: 0.1711 - soft_acc_multi_output: 0.7548 - val_loss: 0.3038 - val_soft_acc_multi_output: 0.5714\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 5s 303ms/step - loss: 0.1769 - soft_acc_multi_output: 0.7576 - val_loss: 0.3032 - val_soft_acc_multi_output: 0.5714\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 5s 307ms/step - loss: 0.1689 - soft_acc_multi_output: 0.7666 - val_loss: 0.3283 - val_soft_acc_multi_output: 0.5000\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 5s 306ms/step - loss: 0.1587 - soft_acc_multi_output: 0.7915 - val_loss: 0.3187 - val_soft_acc_multi_output: 0.5179\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 5s 306ms/step - loss: 0.1620 - soft_acc_multi_output: 0.7771 - val_loss: 0.3148 - val_soft_acc_multi_output: 0.5536\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 5s 304ms/step - loss: 0.1691 - soft_acc_multi_output: 0.7670 - val_loss: 0.3413 - val_soft_acc_multi_output: 0.5357\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 5s 304ms/step - loss: 0.1517 - soft_acc_multi_output: 0.7951 - val_loss: 0.3551 - val_soft_acc_multi_output: 0.5357\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 5s 304ms/step - loss: 0.1471 - soft_acc_multi_output: 0.8075 - val_loss: 0.3255 - val_soft_acc_multi_output: 0.5179\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 5s 304ms/step - loss: 0.1593 - soft_acc_multi_output: 0.7873 - val_loss: 0.3340 - val_soft_acc_multi_output: 0.5536\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 5s 304ms/step - loss: 0.1514 - soft_acc_multi_output: 0.7908 - val_loss: 0.3602 - val_soft_acc_multi_output: 0.5536\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 5s 309ms/step - loss: 0.1411 - soft_acc_multi_output: 0.8166 - val_loss: 0.3396 - val_soft_acc_multi_output: 0.5893\n"
     ]
    }
   ],
   "source": [
    "#########################\n",
    "## fine tuning\n",
    "#########################\n",
    "\n",
    "\n",
    "fine_tune = [layer.name for layer in model.layers].index(r\"top_conv\")\n",
    "\n",
    "model.trainable = True\n",
    "for layer in model.layers[:fine_tune]:\n",
    "    layer.trainable = False\n",
    "for layer in model.layers[fine_tune:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "print(\n",
    "    f\"from {fine_tune} of {len(model.layers)} layers; {len(model.trainable_variables)} trainables variables\"\n",
    ")\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Nadam(),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[soft_acc_multi_output],\n",
    ")\n",
    "train_set.reset()\n",
    "validation_set.reset()\n",
    "\n",
    "\n",
    "# logdir_name = (\n",
    "#     r\".\\tfb\\logs\\effnet\\\\\"\n",
    "#     + \"effnet__1_layer\"\n",
    "#     + \"__\"\n",
    "#     + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# )\n",
    "# tensorboard_callback = TensorBoard(log_dir=logdir_name)\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=21, restore_best_weights=True,\n",
    ")\n",
    "reduce_lr_plateau = ReduceLROnPlateau(monitor=\"val_loss\", patience=7, factor=0.8)\n",
    "\n",
    "# initial epoch is useless for keras optimizers as they update internally independent of epoch number\n",
    "history_fine = model.fit_generator(\n",
    "    generator=train_set,\n",
    "    epochs=100,\n",
    "    validation_data=validation_set,\n",
    "    callbacks=[early_stopping, reduce_lr_plateau,],\n",
    ")\n",
    "\n",
    "# model.save(\n",
    "#     filepath=\"./saved_models/my_effnet/tuned_1_layer.h5\", save_format=\"h5\",\n",
    "# )\n",
    "# model.save_weights(\n",
    "#     \"./saved_models/my_effnet/tuned_1_layer_weights_only.h5\", save_format=\"h5\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "cvscores = []\n",
    "\n",
    "# model__ = tf.keras.models.load_model(\n",
    "#     \"./saved_models/my_effnet/tuned_1_layer.h5\",\n",
    "#     custom_objects={\"soft_acc_multi_output\": soft_acc_multi_output},\n",
    "# )\n",
    "\n",
    "# model__.trainable = False\n",
    "# len(model__.trainable_variables)\n",
    "# model__.compile(\n",
    "#         optimizer=keras.optimizers.Nadam(learning_rate=0.0001),\n",
    "#         loss=\"binary_crossentropy\",\n",
    "#         metrics=[soft_acc_multi_output],\n",
    "#     )\n",
    "\n",
    "model__ = generate_base_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 450 validated image filenames.\n",
      "Found 57 validated image filenames.\n",
      "Found 56 validated image filenames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\feroc\\anaconda3\\envs\\tf\\lib\\site-packages\\keras_preprocessing\\image\\dataframe_iterator.py:273: UserWarning: Found 5 invalid image filename(s) in x_col=\"GreenID\". These filename(s) will be ignored.\n",
      "  .format(n_invalid, x_col)\n",
      "C:\\Users\\feroc\\anaconda3\\envs\\tf\\lib\\site-packages\\keras_preprocessing\\image\\dataframe_iterator.py:273: UserWarning: Found 1 invalid image filename(s) in x_col=\"GreenID\". These filename(s) will be ignored.\n",
      "  .format(n_invalid, x_col)\n"
     ]
    }
   ],
   "source": [
    "traintest = list(kf.split(np.zeros(len(response)), response[\"Grade\"]))\n",
    "train_index, test_index = np.stack(traintest)[:, 0][0], np.stack(traintest)[:, 1][0]\n",
    "\n",
    "\n",
    "train_gen = ImageDataGenerator(\n",
    "    rotation_range=5,\n",
    "    fill_mode=\"reflect\",\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0,\n",
    "    rescale=1.0 / 255.0,\n",
    "    zoom_range=0.1,\n",
    ")\n",
    "valid_gen = ImageDataGenerator(validation_split=0.5, rescale=1.0 / 255.0,)\n",
    "\n",
    "train_set = train_gen.flow_from_dataframe(\n",
    "    dataframe=response.iloc[train_index],\n",
    "    directory=DATADIR,\n",
    "    x_col=\"GreenID\",\n",
    "    target_size=(224, 224),\n",
    "    color_mode=\"rgb\",\n",
    "    subset=\"training\",\n",
    "    shuffle=False,\n",
    "    y_col=[0, 1, 2, 3,],\n",
    "    class_mode=\"raw\",\n",
    ")\n",
    "\n",
    "validation_set = valid_gen.flow_from_dataframe(\n",
    "    dataframe=response.iloc[test_index],\n",
    "    directory=DATADIR,\n",
    "    x_col=\"GreenID\",\n",
    "    target_size=(224, 224),\n",
    "    color_mode=\"rgb\",\n",
    "    subset=\"training\",\n",
    "    shuffle=True,\n",
    "    batch_size=64,\n",
    "    y_col=[0, 1, 2, 3,],\n",
    "    class_mode=\"raw\",\n",
    ")\n",
    "\n",
    "test_set = valid_gen.flow_from_dataframe(\n",
    "    dataframe=response.iloc[test_index],\n",
    "    directory=DATADIR,\n",
    "    x_col=\"GreenID\",\n",
    "    target_size=(224, 224),\n",
    "    color_mode=\"rgb\",\n",
    "    subset=\"validation\",\n",
    "    shuffle=True,\n",
    "    batch_size=64,\n",
    "    y_col=[0, 1, 2, 3,],\n",
    "    class_mode=\"raw\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model__ = fine_tune_model(model__,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 15 steps, validate for 1 steps\n",
      "Epoch 1/10\n",
      "15/15 [==============================] - 10s 647ms/step - loss: 0.8732 - soft_acc_multi_output: 0.1500 - val_loss: 0.4139 - val_soft_acc_multi_output: 0.3684\n",
      "Epoch 2/10\n",
      "15/15 [==============================] - 4s 288ms/step - loss: 0.7882 - soft_acc_multi_output: 0.1792 - val_loss: 0.4187 - val_soft_acc_multi_output: 0.3509\n",
      "Epoch 3/10\n",
      "15/15 [==============================] - 4s 287ms/step - loss: 0.6969 - soft_acc_multi_output: 0.1937 - val_loss: 0.4258 - val_soft_acc_multi_output: 0.4211\n",
      "Epoch 4/10\n",
      "15/15 [==============================] - 4s 291ms/step - loss: 0.6181 - soft_acc_multi_output: 0.2625 - val_loss: 0.4342 - val_soft_acc_multi_output: 0.4035\n",
      "Epoch 5/10\n",
      "15/15 [==============================] - 4s 288ms/step - loss: 0.5712 - soft_acc_multi_output: 0.2750 - val_loss: 0.4419 - val_soft_acc_multi_output: 0.4737\n",
      "Epoch 6/10\n",
      "15/15 [==============================] - 4s 288ms/step - loss: 0.5283 - soft_acc_multi_output: 0.3583 - val_loss: 0.4488 - val_soft_acc_multi_output: 0.4912\n",
      "Epoch 7/10\n",
      "15/15 [==============================] - 4s 287ms/step - loss: 0.5110 - soft_acc_multi_output: 0.3562 - val_loss: 0.4532 - val_soft_acc_multi_output: 0.4912\n",
      "Epoch 8/10\n",
      "15/15 [==============================] - 4s 287ms/step - loss: 0.4798 - soft_acc_multi_output: 0.4250 - val_loss: 0.4554 - val_soft_acc_multi_output: 0.4912\n",
      "Epoch 9/10\n",
      "15/15 [==============================] - 4s 286ms/step - loss: 0.4474 - soft_acc_multi_output: 0.4250 - val_loss: 0.4571 - val_soft_acc_multi_output: 0.4912\n",
      "Epoch 10/10\n",
      "15/15 [==============================] - 4s 288ms/step - loss: 0.4232 - soft_acc_multi_output: 0.4750 - val_loss: 0.4561 - val_soft_acc_multi_output: 0.4912\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x282fb52fa48>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch = next(test_set)\n",
    "# true_labels = batch[1]\n",
    "# predictions = model__.predict(batch[0])\n",
    "\n",
    "# print(model__.metrics_names)\n",
    "# print(\n",
    "#     model__.evaluate(train_set, verbose=0)\n",
    "# )  # working well with original unstratified model, including both trainning and testing sets?\n",
    "\n",
    "model__.fit(\n",
    "    x=train_set,\n",
    "    epochs=10,\n",
    "    validation_data=validation_set,\n",
    "    callbacks=[early_stopping, reduce_lr_plateau],\n",
    "    #         verbose=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_base_model():\n",
    "    conv_base = enet.EfficientNetB0(\n",
    "        include_top=False, input_shape=(224, 224, 3), pooling=\"avg\", weights=\"imagenet\",\n",
    "    )\n",
    "    conv_base.trainable = False\n",
    "\n",
    "    x = conv_base.output\n",
    "    x = Dropout(0.5)(x)\n",
    "    preds = Dense(5, activation=\"softmax\")(x)\n",
    "    model = Model(inputs=conv_base.input, outputs=preds)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Nadam(lr=0.0014),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[tf.keras.metrics.CategoricalAccuracy()],\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def fine_tune_model(model, fine_tune=None):\n",
    "    if fine_tune is None:\n",
    "        try:\n",
    "            fine_tune = [layer.name for layer in model.layers].index(r\"top_conv\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    model.trainable = True\n",
    "    for layer in model.layers[:fine_tune]:\n",
    "        layer.trainable = False\n",
    "    for layer in model.layers[fine_tune:]:\n",
    "        layer.trainable = True\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Nadam(lr=0.0003),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[tf.keras.metrics.CategoricalAccuracy()],\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def generate_train_val_test(train_index, val_index, test_index):\n",
    "    train_dataset = response.iloc[train_index]\n",
    "    val_dataset = response.iloc[val_index]\n",
    "    test_dataset = response.iloc[test_index]\n",
    "    train_gen = ImageDataGenerator(\n",
    "        rotation_range=5,\n",
    "        fill_mode=\"reflect\",\n",
    "        horizontal_flip=True,\n",
    "        rescale=1.0 / 255.0,\n",
    "        zoom_range=0.1,\n",
    "    )\n",
    "    valid_test_gen = ImageDataGenerator(rescale=1.0 / 255.0,)\n",
    "\n",
    "    train_set = train_gen.flow_from_dataframe(\n",
    "        dataframe=train_dataset,\n",
    "        directory=DATADIR,\n",
    "        x_col=\"GreenID\",\n",
    "        target_size=(224, 224),\n",
    "        color_mode=\"rgb\",\n",
    "        subset=\"training\",\n",
    "        shuffle=True,\n",
    "        y_col=\"Grade\",\n",
    "    )\n",
    "\n",
    "    validation_set = valid_test_gen.flow_from_dataframe(\n",
    "        dataframe=val_dataset,\n",
    "        directory=DATADIR,\n",
    "        x_col=\"GreenID\",\n",
    "        target_size=(224, 224),\n",
    "        color_mode=\"rgb\",\n",
    "        subset=\"training\",\n",
    "        shuffle=False,\n",
    "        batch_size=64,\n",
    "        y_col=\"Grade\",\n",
    "    )\n",
    "\n",
    "    test_set = valid_test_gen.flow_from_dataframe(\n",
    "        dataframe=test_dataset,\n",
    "        directory=DATADIR,\n",
    "        x_col=\"GreenID\",\n",
    "        target_size=(224, 224),\n",
    "        color_mode=\"rgb\",\n",
    "        subset=\"training\",\n",
    "        shuffle=False,\n",
    "        batch_size=64,\n",
    "        y_col=\"Grade\",\n",
    "    )\n",
    "    return train_set, validation_set, test_set\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "innerkf = StratifiedKFold(n_splits=2, shuffle=True, random_state=seed)\n",
    "response = response.sample(frac=1.0)\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=21, restore_best_weights=True,\n",
    ")\n",
    "reduce_lr_plateau = ReduceLROnPlateau(monitor=\"val_loss\", patience=7, factor=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "def stratified_cv(fine_tune_layer=None):\n",
    "    acc_coef_scores = []\n",
    "    raw_outputs = []\n",
    "    for train_index, val_test_index in kf.split(\n",
    "        np.zeros(len(response)), response[\"Grade\"]\n",
    "    ):\n",
    "        val_index, test_index = next(\n",
    "            innerkf.split(\n",
    "                np.zeros(len(val_test_index)), response[\"Grade\"].iloc[val_test_index]\n",
    "            )\n",
    "        )\n",
    "        val_index, test_index = val_test_index[val_index], val_test_index[test_index]\n",
    "        train_set, validation_set, test_set = generate_train_val_test(\n",
    "            train_index, val_index, test_index\n",
    "        )\n",
    "        model = generate_base_model()\n",
    "\n",
    "        _ = model.fit(\n",
    "            x=train_set,\n",
    "            epochs=15,\n",
    "            validation_data=validation_set,\n",
    "            callbacks=[early_stopping, reduce_lr_plateau],\n",
    "            verbose=0,\n",
    "        )\n",
    "\n",
    "        model = fine_tune_model(model, fine_tune=fine_tune_layer)\n",
    "\n",
    "        _ = model.fit(\n",
    "            x=train_set,\n",
    "            epochs=100,\n",
    "            validation_data=validation_set,\n",
    "            callbacks=[early_stopping, reduce_lr_plateau],\n",
    "            verbose=0,\n",
    "        )\n",
    "\n",
    "        batch = next(test_set)\n",
    "        true_labels = batch[1]\n",
    "        predictions = model.predict(batch[0])\n",
    "        acc = soft_acc_multi_output(predictions, true_labels).numpy()\n",
    "        corr = np.corrcoef(np.argmax(predictions, axis=1),np.argmax(true_labels, axis=1))[0][\n",
    "            1\n",
    "        ]\n",
    "        acc_coef_scores.append([acc, corr])\n",
    "        raw_outputs.append([np.array(response.iloc[test_index].index), true_labels, predictions])\n",
    "        del train_set, validation_set, test_set, _, model, batch, true_labels, predictions, acc, corr\n",
    "        tf.keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "    return acc_coef_scores, raw_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  np.where(np.array(['conv' in layer.name for layer in model.layers]) == True)[0][::-1]\n",
    "trainable_sequence = np.array([227, 225, 217, 214, 210, 202, 199, 195, 187, 184, 180, 172, 169,\n",
    "       167, 159, 156, 152, 144, 141, 137, 129, 126, 124, 116, 113, 109,\n",
    "       101,  98,  94,  86,  83,  81,  73,  70,  66,  58,  55,  53,  45,\n",
    "        42,  38,  30,  27,  25,  17,  14,  12,   4,   1])\n",
    "\n",
    "fine_tune_scores_acc_coef = []\n",
    "fine_tune_raw_outputs = []\n",
    "\n",
    "for fine_tune in trainable_sequence[20:30]:\n",
    "    acc_coef_scores, raw_outputs = stratified_cv(fine_tune)\n",
    "    fine_tune_scores_acc_coef.append(acc_coef_scores)\n",
    "    fine_tune_raw_outputs.append(raw_outputs)\n",
    "    np.save(r\"./stratified_cross_validation_results/effnets/onehot_acc_coef_20-30\", np.array(fine_tune_scores_acc_coef))\n",
    "    np.save(r\"./stratified_cross_validation_results/effnets/onehot_raw_outputs_20-30\", np.array(fine_tune_raw_outputs))\n",
    "    del acc_coef_scores, raw_outputs\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "[[34  7  2  0  0]\n",
      " [14 25  9  7  0]\n",
      " [15 16 35  7  7]\n",
      " [ 0  6  9 28  6]\n",
      " [ 0  1 10  7 37]]\n",
      "+++++++++++++++++++++++++++++++++\n",
      "+++++++++++++++++++++++++++++++++\n",
      "[[0.79069767 0.1627907  0.04651163 0.         0.        ]\n",
      " [0.25454545 0.45454545 0.16363636 0.12727273 0.        ]\n",
      " [0.1875     0.2        0.4375     0.0875     0.0875    ]\n",
      " [0.         0.12244898 0.18367347 0.57142857 0.12244898]\n",
      " [0.         0.01818182 0.18181818 0.12727273 0.67272727]]\n",
      "+++++++++++++++++++++++++++++++++\n",
      "+++++++++++++++++++++++++++++++++\n",
      "Accuracy:  56.38297872340425\n",
      "Raw Correlation:  0.7498620711173194\n",
      "Rounded Correlation:  0.6439188662889387\n",
      "+++++++++++++++++++++++++++++++++\n",
      "+++++++++++++++++++++++++++++++++\n",
      "max accuracy with tuning from 199 layers, or tune 34 layers\n",
      "6 [79.06976744186046, 45.45454545454545, 43.75, 57.14285714285714, 67.27272727272727]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "acc_coef8 = np.load(r\"./stratified_cross_validation_results/effnets/onehot_acc_coef_0-9.npy\", allow_pickle=True)[:9]\n",
    "raw_outputs8 = np.load(r\"./stratified_cross_validation_results/effnets/onehot_raw_outputs_0-9.npy\", allow_pickle=True)[:9]\n",
    "acc_coef12 = np.load(r\"./stratified_cross_validation_results/effnets/onehot_acc_coef_9-20.npy\", allow_pickle=True)\n",
    "raw_outputs12 = np.load(r\"./stratified_cross_validation_results/effnets/onehot_raw_outputs_9-20.npy\", allow_pickle=True)\n",
    "acc_coef20 = np.load(r\"./stratified_cross_validation_results/effnets/onehot_acc_coef_20-30.npy\", allow_pickle=True)\n",
    "raw_outputs20 = np.load(r\"./stratified_cross_validation_results/effnets/onehot_raw_outputs_20-30.npy\", allow_pickle=True)\n",
    "acc_coef = np.vstack(np.array([acc_coef8,acc_coef12,acc_coef20]))\n",
    "raw_outputs = np.vstack(np.array([raw_outputs8,raw_outputs12,raw_outputs20]))\n",
    "print(len(acc_coef))\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def show_confusion_matrix(raw_outputs, fine_tune_layers):\n",
    "\n",
    "    y_true = np.argmax(np.vstack(raw_outputs[fine_tune_layers, :, 1]), axis=1)\n",
    "    y_pred = np.argmax(\n",
    "        np.vstack(raw_outputs[fine_tune_layers, :, 2]), axis=1\n",
    "    ).astype(int)\n",
    "    return confusion_matrix(y_true, y_pred)\n",
    "\n",
    "def raw_correlation(raw_outputs, fine_tune_layers):\n",
    "    y_true = np.argmax(np.vstack(raw_outputs[fine_tune_layers, :, 1]), axis=1).astype(float)\n",
    "    y_pred = np.argmax(np.vstack(raw_outputs[fine_tune_layers, :, 2]), axis=1).astype(float)\n",
    "    return np.corrcoef(y_true, y_pred)\n",
    "\n",
    "def rounded_correlation(raw_outputs, fine_tune_layers):\n",
    "    y_true = np.argmax(np.vstack(raw_outputs[fine_tune_layers, :, 1]), axis=1)\n",
    "    y_pred = np.argmax(np.rint(np.vstack(raw_outputs[fine_tune_layers, :, 2])), axis=1).astype(int)\n",
    "    return np.corrcoef(y_true, y_pred)\n",
    "\n",
    "\n",
    "def show_matrix_percentage(confusion_matrix):\n",
    "    return np.transpose(np.transpose(my_confusion_matrix) / np.sum(my_confusion_matrix, axis=1))\n",
    "\n",
    "# total accuracy\n",
    "def calculate_accuracy(my_confusion_matrix):\n",
    "    return np.trace(my_confusion_matrix)/np.sum(my_confusion_matrix)\n",
    "\n",
    "max_acc_layer = np.argmax([calculate_accuracy(show_confusion_matrix(raw_outputs, i))  for i in range(len(acc_coef))])\n",
    "\n",
    "my_confusion_matrix = show_confusion_matrix(raw_outputs,max_acc_layer)\n",
    "print(my_confusion_matrix)\n",
    "print(\"+++++++++++++++++++++++++++++++++\")\n",
    "print(\"+++++++++++++++++++++++++++++++++\")\n",
    "print(show_matrix_percentage(my_confusion_matrix))\n",
    "print(\"+++++++++++++++++++++++++++++++++\")\n",
    "print(\"+++++++++++++++++++++++++++++++++\")\n",
    "print(\"Accuracy: \",calculate_accuracy(my_confusion_matrix)*100)\n",
    "print(\"Raw Correlation: \",raw_correlation(raw_outputs,max_acc_layer)[0][1])\n",
    "print(\"Rounded Correlation: \",rounded_correlation(raw_outputs,max_acc_layer)[0][1])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot([i for i in range(len(acc_coef))],[calculate_accuracy(show_confusion_matrix(raw_outputs, i))  for i in range(len(acc_coef))])\n",
    "print(\"+++++++++++++++++++++++++++++++++\")\n",
    "print(\"+++++++++++++++++++++++++++++++++\")\n",
    "trainable_sequence = np.array([227, 225, 217, 214, 210, 202, 199, 195, 187, 184, 180, 172, 169,\n",
    "       167, 159, 156, 152, 144, 141, 137, 129, 126, 124, 116, 113, 109,\n",
    "       101,  98,  94,  86,  83,  81,  73,  70,  66,  58,  55,  53,  45,\n",
    "        42,  38,  30,  27,  25,  17,  14,  12,   4,   1])\n",
    "print(f\"max accuracy with tuning from {trainable_sequence[max_acc_layer]} layers, or tune {233-trainable_sequence[max_acc_layer]} layers\")\n",
    "print(max_acc_layer, [show_matrix_percentage(my_confusion_matrix)[i,i]*100 for i in range(5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56, 4)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(cvscores)[:, 1][:, 0][4].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.54385966, 0.7954881615019466],\n",
       "       [0.5964912, 0.851205445137053],\n",
       "       [0.4107143, 0.7907964869050351],\n",
       "       [0.54545456, 0.8426871818521864],\n",
       "       [0.66071427, 0.8581865352844125]], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(cvscores)[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5,  3,  1,  0,  0],\n",
       "       [ 0,  9,  3,  0,  0],\n",
       "       [ 1,  4, 11,  0,  0],\n",
       "       [ 0,  0,  3,  5,  3],\n",
       "       [ 0,  0,  1,  3,  5]], dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(\n",
    "    y_true=K.sum(K.cast(K.round(cvscores[1][1][0]), \"int32\"), axis=1).numpy(),\n",
    "    y_pred=K.sum(K.cast(K.round(cvscores[1][1][1]), \"int32\"), axis=1).numpy(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "mycsv = pd.DataFrame(\n",
    "    np.hstack(\n",
    "        np.array(\n",
    "            [\n",
    "                np.vstack(np.array(cvscores)[:, 1][:, 0]),\n",
    "                np.vstack(np.array(cvscores)[:, 1][:, 1]),\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(mycsv[range(4, 8)].to_numpy(), np.vstack(np.array(cvscores)[:, 1][:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "mycsv.to_csv(\n",
    "    \"./stratified_cross_validation_results/effnet_multinomial.csv\", index=False\n",
    ")\n",
    "# next time include which image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "mycsv = pd.read_csv(\"./stratified_cross_validation_results/effnet_multinomial.csv\")\n",
    "y_true = np.sum((mycsv[[str(i) for i in range(0, 4)]]).to_numpy(dtype=int), axis=1)\n",
    "y_pred = np.sum(\n",
    "    np.rint((mycsv[[str(i) for i in range(4, 8)]]).to_numpy()), axis=1\n",
    ").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[33, 10,  1,  0,  0],\n",
       "       [ 6, 36, 14,  1,  0],\n",
       "       [ 7, 22, 35,  5,  4],\n",
       "       [ 0,  2, 20, 17,  9],\n",
       "       [ 0,  0,  7, 17, 35]], dtype=int64)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "my_confusion_matrix = confusion_matrix(y_true, y_pred,)\n",
    "my_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.75      , 0.22727273, 0.02272727, 0.        , 0.        ],\n",
       "       [0.10526316, 0.63157895, 0.24561404, 0.01754386, 0.        ],\n",
       "       [0.09589041, 0.30136986, 0.47945205, 0.06849315, 0.05479452],\n",
       "       [0.        , 0.04166667, 0.41666667, 0.35416667, 0.1875    ],\n",
       "       [0.        , 0.        , 0.11864407, 0.28813559, 0.59322034]])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.transpose(np.transpose(my_confusion_matrix) / np.sum(my_confusion_matrix, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.82710261],\n",
       "       [0.82710261, 1.        ]])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# coef\n",
    "np.corrcoef(y_true, np.sum((mycsv[[str(i) for i in range(4, 8)]]).to_numpy(), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5551601423487544"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# acc\n",
    "sum(np.isclose(y_true, y_pred)) / len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.utils import plot_model\n",
    "\n",
    "# plot_model(model, to_file=\"effnet.png\", show_shapes=True)\n",
    "# from IPython.display import Image\n",
    "\n",
    "# Image(filename=\"effnet.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 281 validated image filenames.\n",
      "['loss', 'soft_acc_multi_output']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "[0.21299249678850174, 0.74404764]\n"
     ]
    }
   ],
   "source": [
    "response = response.sample(frac=1.0)\n",
    "\n",
    "test_set = valid_gen.flow_from_dataframe(\n",
    "    dataframe=response,\n",
    "    directory=DATADIR,\n",
    "    x_col=\"GreenID\",\n",
    "    target_size=(224, 224),\n",
    "    color_mode=\"rgb\",\n",
    "    subset=\"validation\",\n",
    "    shuffle=False,\n",
    "    batch_size=56,\n",
    "    y_col=[0, 1, 2, 3,],\n",
    "    class_mode=\"raw\",\n",
    "    #     seed = seed\n",
    ")\n",
    "\n",
    "batch = next(test_set)\n",
    "true_labels = batch[1]\n",
    "predictions = model.predict(batch[0])\n",
    "\n",
    "print(model.metrics_names)\n",
    "print(model.evaluate(test_set, verbose=0))  # loss/accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.85824516],\n",
       "       [0.85824516, 1.        ]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(np.sum(predictions, axis=1), np.sum(true_labels, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=80)\n",
    "batch = next(test_set)\n",
    "\n",
    "y_true = batch[1]\n",
    "y_pred = model.predict(batch[0])\n",
    "print(soft_acc_multi_output(y_true, y_pred))\n",
    "\n",
    "# print examples from the validation set\n",
    "for i in range(len(batch[1])):\n",
    "    img = batch[0][i]\n",
    "    label = batch[1][i]\n",
    "    assert (label == y_true[i]).all()\n",
    "    right = K.all(\n",
    "        K.equal(K.cast(K.round(label), \"int32\"), K.cast(K.round(y_pred[i]), \"int32\"),)\n",
    "    )\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    print(f\"true label: {label}; rounded pred: {y_pred[i]}; Correct: {right}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ds = tf.data.Dataset.list_files(\n",
    "    str(\"C:/Users/feroc/OneDrive - The University of Melbourne/Dataset/adult/*\")\n",
    ")\n",
    "\n",
    "\n",
    "def decode_img(img):\n",
    "    # convert the compressed string to a 3D uint8 tensor\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    # resize the image to the desired size.\n",
    "    return tf.image.resize(img, [IMG_WIDTH, IMG_HEIGHT])\n",
    "\n",
    "\n",
    "def get_label(file_path):\n",
    "    # convert the path to a list of path components\n",
    "    image_id = tf.strings.split(file_path, os.path.sep)[-1]\n",
    "    return response.loc[] \n",
    "\n",
    "list(list_ds.take(1).as_numpy_iterator())[0]\n",
    "tf.strings.split(list(list_ds.take(1).as_numpy_iterator())[0],os.path.sep)[-1].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
