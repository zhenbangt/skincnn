{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "IMG_WIDTH, IMG_HEIGHT = 224, 224\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.applications.densenet import (\n",
    "    DenseNet121,\n",
    "    preprocess_input,\n",
    ")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import IPython.display as display\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense,\n",
    "    GlobalAveragePooling2D,\n",
    "    Conv2D,\n",
    "    Flatten,\n",
    "    GlobalMaxPooling2D,\n",
    "    Dropout,\n",
    ")\n",
    "\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam, Nadam\n",
    "from datetime import datetime\n",
    "from packaging import version\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import (\n",
    "    TensorBoard,\n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint,\n",
    "    ReduceLROnPlateau,\n",
    ")\n",
    "import efficientnet.tfkeras as enet\n",
    "\n",
    "\n",
    "def append_extension(fn):\n",
    "    return (fn + \".jpg\").zfill(7)\n",
    "\n",
    "\n",
    "def ordered_logit(class_number):\n",
    "    # zero portability\n",
    "    target = np.zeros(4, dtype=int)\n",
    "    target[: class_number - 2] = 1\n",
    "    return target\n",
    "\n",
    "\n",
    "DATADIR = r\"./adult\"\n",
    "CSV_PATH = r\"./adult/CastControls_ALP.xlsx\"\n",
    "response = pd.read_excel(CSV_PATH, sheet_name=0,)[[\"GreenID\", \"Grade\"]].dropna(\n",
    "    axis=0, subset=[\"Grade\"]\n",
    ")\n",
    "response.Grade = response.Grade.astype(\"int\")\n",
    "response.GreenID = response.GreenID.astype(\"str\").apply(append_extension)\n",
    "response = response[response.Grade != 99]\n",
    "response = pd.concat(\n",
    "    [response, pd.DataFrame.from_dict(dict(response.Grade.apply(ordered_logit))).T,],\n",
    "    axis=1,\n",
    ")\n",
    "response.Grade = response.Grade.astype(\"str\")\n",
    "response = response.sample(frac=1)\n",
    "seed = np.random.randint(30027)\n",
    "\n",
    "\n",
    "def soft_acc(y_true, y_pred):\n",
    "    return K.mean(K.equal(K.round(y_true), K.round(y_pred)))\n",
    "\n",
    "\n",
    "def soft_acc_multi_output(y_true, y_pred):\n",
    "    return K.mean(\n",
    "        K.all(\n",
    "            K.equal(\n",
    "                K.cast(K.round(y_true), \"int32\"), K.cast(K.round(y_pred), \"int32\"),\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "    )\n",
    "\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "policy = tf.keras.mixed_precision.experimental.Policy(\"mixed_float16\")\n",
    "mixed_precision.experimental.set_policy(policy)\n",
    "\n",
    "# gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "# if gpus:\n",
    "#     try:\n",
    "#         # Currently, memory growth needs to be the same across GPUs\n",
    "#         for gpu in gpus:\n",
    "#             tf.config.experimental.set_memory_growth(gpu, True)\n",
    "#         logical_gpus = tf.config.experimental.list_logical_devices(\"GPU\")\n",
    "#         print(\n",
    "#             len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\",\n",
    "#         )\n",
    "#     except RuntimeError as e:\n",
    "#         # Memory growth must be set before GPUs have been initialized\n",
    "#         print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "train_gen = ImageDataGenerator(\n",
    "    rotation_range=5,\n",
    "    fill_mode=\"reflect\",\n",
    "    horizontal_flip=True,\n",
    "    #     vertical_flip=True,\n",
    "    validation_split=0.1,\n",
    "    # dude i wasnt cheating...\n",
    "    rescale=1.0 / 255.0,\n",
    "    #     preprocessing_function = preprocess_input\n",
    "    zoom_range=0.1,\n",
    ")\n",
    "\n",
    "valid_gen = ImageDataGenerator(validation_split=0.1, rescale=1.0 / 255.0,)\n",
    "\n",
    "train_set = train_gen.flow_from_dataframe(\n",
    "    dataframe=response,\n",
    "    directory=DATADIR,\n",
    "    x_col=\"GreenID\",\n",
    "    target_size=(224, 224),\n",
    "    color_mode=\"rgb\",\n",
    "    subset=\"training\",\n",
    "    shuffle=True,\n",
    "    #     class_mode = \"sparse\"\n",
    "    #     y_col=\"Grade\",\n",
    "    y_col=[0, 1, 2, 3,],\n",
    "    class_mode=\"raw\",\n",
    ")\n",
    "\n",
    "validation_set = valid_gen.flow_from_dataframe(\n",
    "    dataframe=response,\n",
    "    directory=DATADIR,\n",
    "    x_col=\"GreenID\",\n",
    "    target_size=(224, 224),\n",
    "    color_mode=\"rgb\",\n",
    "    subset=\"validation\",\n",
    "    shuffle=True,\n",
    "    batch_size=28,\n",
    "    #     class_mode = \"sparse\"\n",
    "    #     y_col=\"Grade\",\n",
    "    y_col=[0, 1, 2, 3,],\n",
    "    class_mode=\"raw\",\n",
    ")\n",
    "\n",
    "train_set.reset()\n",
    "validation_set.reset()\n",
    "# print(next(validation_set)[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-4e887602e298>:25: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 16 steps, validate for 2 steps\n",
      "Epoch 1/25\n",
      "16/16 [==============================] - 11s 669ms/step - loss: 0.6069 - soft_acc_multi_output: 0.2124 - val_loss: 0.4696 - val_soft_acc_multi_output: 0.3571\n",
      "Epoch 2/25\n",
      "16/16 [==============================] - 5s 305ms/step - loss: 0.4838 - soft_acc_multi_output: 0.3115 - val_loss: 0.3935 - val_soft_acc_multi_output: 0.4286\n",
      "Epoch 3/25\n",
      "16/16 [==============================] - 5s 307ms/step - loss: 0.4403 - soft_acc_multi_output: 0.3268 - val_loss: 0.3645 - val_soft_acc_multi_output: 0.4643\n",
      "Epoch 4/25\n",
      "16/16 [==============================] - 5s 305ms/step - loss: 0.4071 - soft_acc_multi_output: 0.3978 - val_loss: 0.3524 - val_soft_acc_multi_output: 0.4821\n",
      "Epoch 5/25\n",
      "16/16 [==============================] - 5s 304ms/step - loss: 0.4045 - soft_acc_multi_output: 0.3818 - val_loss: 0.3363 - val_soft_acc_multi_output: 0.4821\n",
      "Epoch 6/25\n",
      "16/16 [==============================] - 5s 315ms/step - loss: 0.3879 - soft_acc_multi_output: 0.3942 - val_loss: 0.3301 - val_soft_acc_multi_output: 0.5357\n",
      "Epoch 7/25\n",
      "16/16 [==============================] - 5s 313ms/step - loss: 0.3993 - soft_acc_multi_output: 0.4017 - val_loss: 0.3252 - val_soft_acc_multi_output: 0.5179\n",
      "Epoch 8/25\n",
      "16/16 [==============================] - 5s 309ms/step - loss: 0.3769 - soft_acc_multi_output: 0.4029 - val_loss: 0.3267 - val_soft_acc_multi_output: 0.5000\n",
      "Epoch 9/25\n",
      "16/16 [==============================] - 5s 315ms/step - loss: 0.3710 - soft_acc_multi_output: 0.4532 - val_loss: 0.3134 - val_soft_acc_multi_output: 0.5000\n",
      "Epoch 10/25\n",
      "16/16 [==============================] - 5s 308ms/step - loss: 0.3648 - soft_acc_multi_output: 0.4408 - val_loss: 0.3185 - val_soft_acc_multi_output: 0.5000\n",
      "Epoch 11/25\n",
      "16/16 [==============================] - 5s 316ms/step - loss: 0.3566 - soft_acc_multi_output: 0.4344 - val_loss: 0.3059 - val_soft_acc_multi_output: 0.5000\n",
      "Epoch 12/25\n",
      "16/16 [==============================] - 5s 308ms/step - loss: 0.3598 - soft_acc_multi_output: 0.4314 - val_loss: 0.3156 - val_soft_acc_multi_output: 0.5179\n",
      "Epoch 13/25\n",
      "16/16 [==============================] - 5s 307ms/step - loss: 0.3551 - soft_acc_multi_output: 0.4807 - val_loss: 0.2991 - val_soft_acc_multi_output: 0.4821\n",
      "Epoch 14/25\n",
      "16/16 [==============================] - 5s 308ms/step - loss: 0.3533 - soft_acc_multi_output: 0.4884 - val_loss: 0.3131 - val_soft_acc_multi_output: 0.5179\n",
      "Epoch 15/25\n",
      "16/16 [==============================] - 5s 315ms/step - loss: 0.3763 - soft_acc_multi_output: 0.4321 - val_loss: 0.2990 - val_soft_acc_multi_output: 0.5000\n",
      "Epoch 16/25\n",
      "16/16 [==============================] - 5s 311ms/step - loss: 0.3663 - soft_acc_multi_output: 0.4442 - val_loss: 0.3008 - val_soft_acc_multi_output: 0.5179\n",
      "Epoch 17/25\n",
      "16/16 [==============================] - 5s 308ms/step - loss: 0.3455 - soft_acc_multi_output: 0.4797 - val_loss: 0.2985 - val_soft_acc_multi_output: 0.5179\n",
      "Epoch 18/25\n",
      "16/16 [==============================] - 5s 309ms/step - loss: 0.3512 - soft_acc_multi_output: 0.4610 - val_loss: 0.2978 - val_soft_acc_multi_output: 0.5000\n",
      "Epoch 19/25\n",
      "16/16 [==============================] - 5s 306ms/step - loss: 0.3634 - soft_acc_multi_output: 0.4452 - val_loss: 0.2891 - val_soft_acc_multi_output: 0.5536\n",
      "Epoch 20/25\n",
      "16/16 [==============================] - 5s 305ms/step - loss: 0.3527 - soft_acc_multi_output: 0.4789 - val_loss: 0.3024 - val_soft_acc_multi_output: 0.5357\n",
      "Epoch 21/25\n",
      "16/16 [==============================] - 5s 302ms/step - loss: 0.3424 - soft_acc_multi_output: 0.4962 - val_loss: 0.2897 - val_soft_acc_multi_output: 0.5536\n",
      "Epoch 22/25\n",
      "16/16 [==============================] - 5s 305ms/step - loss: 0.3355 - soft_acc_multi_output: 0.5012 - val_loss: 0.3051 - val_soft_acc_multi_output: 0.5179\n",
      "Epoch 23/25\n",
      "16/16 [==============================] - 5s 305ms/step - loss: 0.3406 - soft_acc_multi_output: 0.5098 - val_loss: 0.2981 - val_soft_acc_multi_output: 0.5179\n",
      "Epoch 24/25\n",
      "16/16 [==============================] - 5s 306ms/step - loss: 0.3374 - soft_acc_multi_output: 0.4695 - val_loss: 0.2924 - val_soft_acc_multi_output: 0.5536\n",
      "Epoch 25/25\n",
      "16/16 [==============================] - 5s 307ms/step - loss: 0.3361 - soft_acc_multi_output: 0.4848 - val_loss: 0.2859 - val_soft_acc_multi_output: 0.5357\n"
     ]
    }
   ],
   "source": [
    "conv_base = enet.EfficientNetB0(\n",
    "    include_top=False, input_shape=(224, 224, 3), pooling=\"avg\", weights=\"imagenet\",\n",
    ")\n",
    "conv_base.trainable = False\n",
    "\n",
    "x = conv_base.output\n",
    "x = Dropout(0.5)(x)\n",
    "preds = Dense(4, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=conv_base.input, outputs=preds)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[soft_acc_multi_output],\n",
    ")\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=21, restore_best_weights=True,\n",
    ")\n",
    "reduce_lr_plateau = ReduceLROnPlateau(monitor=\"val_loss\", patience=7, factor=0.8)\n",
    "\n",
    "history_1 = model.fit_generator(\n",
    "    generator=train_set,\n",
    "    epochs=25,\n",
    "    validation_data=validation_set,\n",
    "    callbacks=[early_stopping, reduce_lr_plateau],\n",
    "    #     verbose=0,\n",
    ")\n",
    "\n",
    "# model.save(\n",
    "#     filepath=\"./saved_models/my_effnet/my_effnet_untuned_1_layer.h5\", save_format=\"h5\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from 227 of 233 layers; 5 trainables variables\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 16 steps, validate for 2 steps\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 11s 679ms/step - loss: 0.5760 - soft_acc_multi_output: 0.3372 - val_loss: 0.3130 - val_soft_acc_multi_output: 0.5536\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 5s 311ms/step - loss: 0.3600 - soft_acc_multi_output: 0.5082 - val_loss: 0.3111 - val_soft_acc_multi_output: 0.5714\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 5s 303ms/step - loss: 0.3116 - soft_acc_multi_output: 0.5399 - val_loss: 0.3271 - val_soft_acc_multi_output: 0.5536\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 5s 312ms/step - loss: 0.2968 - soft_acc_multi_output: 0.5624 - val_loss: 0.3075 - val_soft_acc_multi_output: 0.5714\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 5s 316ms/step - loss: 0.2799 - soft_acc_multi_output: 0.5781 - val_loss: 0.3050 - val_soft_acc_multi_output: 0.5536\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 5s 315ms/step - loss: 0.2575 - soft_acc_multi_output: 0.6148 - val_loss: 0.3134 - val_soft_acc_multi_output: 0.5714\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 5s 312ms/step - loss: 0.2519 - soft_acc_multi_output: 0.6398 - val_loss: 0.3286 - val_soft_acc_multi_output: 0.5536\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 5s 314ms/step - loss: 0.2520 - soft_acc_multi_output: 0.6171 - val_loss: 0.2865 - val_soft_acc_multi_output: 0.5357\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 5s 313ms/step - loss: 0.2293 - soft_acc_multi_output: 0.6464 - val_loss: 0.3166 - val_soft_acc_multi_output: 0.5357\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 5s 311ms/step - loss: 0.2342 - soft_acc_multi_output: 0.6667 - val_loss: 0.3107 - val_soft_acc_multi_output: 0.5357\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 5s 312ms/step - loss: 0.2185 - soft_acc_multi_output: 0.6832 - val_loss: 0.3253 - val_soft_acc_multi_output: 0.5000\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 5s 312ms/step - loss: 0.2164 - soft_acc_multi_output: 0.6725 - val_loss: 0.3073 - val_soft_acc_multi_output: 0.5714\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 5s 314ms/step - loss: 0.2137 - soft_acc_multi_output: 0.6510 - val_loss: 0.3030 - val_soft_acc_multi_output: 0.5714\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 5s 311ms/step - loss: 0.1913 - soft_acc_multi_output: 0.7256 - val_loss: 0.2914 - val_soft_acc_multi_output: 0.5893\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 5s 310ms/step - loss: 0.1933 - soft_acc_multi_output: 0.7194 - val_loss: 0.3099 - val_soft_acc_multi_output: 0.5179\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 5s 309ms/step - loss: 0.1885 - soft_acc_multi_output: 0.7373 - val_loss: 0.3176 - val_soft_acc_multi_output: 0.5179\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 5s 311ms/step - loss: 0.1763 - soft_acc_multi_output: 0.7526 - val_loss: 0.3385 - val_soft_acc_multi_output: 0.5536\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 5s 302ms/step - loss: 0.1832 - soft_acc_multi_output: 0.7404 - val_loss: 0.3142 - val_soft_acc_multi_output: 0.5179\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 5s 302ms/step - loss: 0.1711 - soft_acc_multi_output: 0.7548 - val_loss: 0.3038 - val_soft_acc_multi_output: 0.5714\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 5s 303ms/step - loss: 0.1769 - soft_acc_multi_output: 0.7576 - val_loss: 0.3032 - val_soft_acc_multi_output: 0.5714\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 5s 307ms/step - loss: 0.1689 - soft_acc_multi_output: 0.7666 - val_loss: 0.3283 - val_soft_acc_multi_output: 0.5000\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 5s 306ms/step - loss: 0.1587 - soft_acc_multi_output: 0.7915 - val_loss: 0.3187 - val_soft_acc_multi_output: 0.5179\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 5s 306ms/step - loss: 0.1620 - soft_acc_multi_output: 0.7771 - val_loss: 0.3148 - val_soft_acc_multi_output: 0.5536\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 5s 304ms/step - loss: 0.1691 - soft_acc_multi_output: 0.7670 - val_loss: 0.3413 - val_soft_acc_multi_output: 0.5357\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 5s 304ms/step - loss: 0.1517 - soft_acc_multi_output: 0.7951 - val_loss: 0.3551 - val_soft_acc_multi_output: 0.5357\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 5s 304ms/step - loss: 0.1471 - soft_acc_multi_output: 0.8075 - val_loss: 0.3255 - val_soft_acc_multi_output: 0.5179\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 5s 304ms/step - loss: 0.1593 - soft_acc_multi_output: 0.7873 - val_loss: 0.3340 - val_soft_acc_multi_output: 0.5536\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 5s 304ms/step - loss: 0.1514 - soft_acc_multi_output: 0.7908 - val_loss: 0.3602 - val_soft_acc_multi_output: 0.5536\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 5s 309ms/step - loss: 0.1411 - soft_acc_multi_output: 0.8166 - val_loss: 0.3396 - val_soft_acc_multi_output: 0.5893\n"
     ]
    }
   ],
   "source": [
    "#########################\n",
    "## fine tuning\n",
    "#########################\n",
    "\n",
    "\n",
    "fine_tune = [layer.name for layer in model.layers].index(r\"top_conv\")\n",
    "\n",
    "model.trainable = True\n",
    "for layer in model.layers[:fine_tune]:\n",
    "    layer.trainable = False\n",
    "for layer in model.layers[fine_tune:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "print(\n",
    "    f\"from {fine_tune} of {len(model.layers)} layers; {len(model.trainable_variables)} trainables variables\"\n",
    ")\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Nadam(),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[soft_acc_multi_output],\n",
    ")\n",
    "train_set.reset()\n",
    "validation_set.reset()\n",
    "\n",
    "\n",
    "# logdir_name = (\n",
    "#     r\".\\tfb\\logs\\effnet\\\\\"\n",
    "#     + \"effnet__1_layer\"\n",
    "#     + \"__\"\n",
    "#     + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# )\n",
    "# tensorboard_callback = TensorBoard(log_dir=logdir_name)\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=21, restore_best_weights=True,\n",
    ")\n",
    "reduce_lr_plateau = ReduceLROnPlateau(monitor=\"val_loss\", patience=7, factor=0.8)\n",
    "\n",
    "# initial epoch is useless for keras optimizers as they update internally independent of epoch number\n",
    "history_fine = model.fit_generator(\n",
    "    generator=train_set,\n",
    "    epochs=100,\n",
    "    validation_data=validation_set,\n",
    "    callbacks=[early_stopping, reduce_lr_plateau,],\n",
    ")\n",
    "\n",
    "# model.save(\n",
    "#     filepath=\"./saved_models/my_effnet/tuned_1_layer.h5\", save_format=\"h5\",\n",
    "# )\n",
    "# model.save_weights(\n",
    "#     \"./saved_models/my_effnet/tuned_1_layer_weights_only.h5\", save_format=\"h5\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "cvscores = []\n",
    "\n",
    "# model__ = tf.keras.models.load_model(\n",
    "#     \"./saved_models/my_effnet/tuned_1_layer.h5\",\n",
    "#     custom_objects={\"soft_acc_multi_output\": soft_acc_multi_output},\n",
    "# )\n",
    "\n",
    "# model__.trainable = False\n",
    "# len(model__.trainable_variables)\n",
    "# model__.compile(\n",
    "#         optimizer=keras.optimizers.Nadam(learning_rate=0.0001),\n",
    "#         loss=\"binary_crossentropy\",\n",
    "#         metrics=[soft_acc_multi_output],\n",
    "#     )\n",
    "\n",
    "model__ = generate_base_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 450 validated image filenames.\n",
      "Found 57 validated image filenames.\n",
      "Found 56 validated image filenames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\feroc\\anaconda3\\envs\\tf\\lib\\site-packages\\keras_preprocessing\\image\\dataframe_iterator.py:273: UserWarning: Found 5 invalid image filename(s) in x_col=\"GreenID\". These filename(s) will be ignored.\n",
      "  .format(n_invalid, x_col)\n",
      "C:\\Users\\feroc\\anaconda3\\envs\\tf\\lib\\site-packages\\keras_preprocessing\\image\\dataframe_iterator.py:273: UserWarning: Found 1 invalid image filename(s) in x_col=\"GreenID\". These filename(s) will be ignored.\n",
      "  .format(n_invalid, x_col)\n"
     ]
    }
   ],
   "source": [
    "traintest = list(kf.split(np.zeros(len(response)), response[\"Grade\"]))\n",
    "train_index, test_index = np.stack(traintest)[:, 0][0], np.stack(traintest)[:, 1][0]\n",
    "\n",
    "\n",
    "train_gen = ImageDataGenerator(\n",
    "    rotation_range=5,\n",
    "    fill_mode=\"reflect\",\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0,\n",
    "    rescale=1.0 / 255.0,\n",
    "    zoom_range=0.1,\n",
    ")\n",
    "valid_gen = ImageDataGenerator(validation_split=0.5, rescale=1.0 / 255.0,)\n",
    "\n",
    "train_set = train_gen.flow_from_dataframe(\n",
    "    dataframe=response.iloc[train_index],\n",
    "    directory=DATADIR,\n",
    "    x_col=\"GreenID\",\n",
    "    target_size=(224, 224),\n",
    "    color_mode=\"rgb\",\n",
    "    subset=\"training\",\n",
    "    shuffle=False,\n",
    "    y_col=[0, 1, 2, 3,],\n",
    "    class_mode=\"raw\",\n",
    ")\n",
    "\n",
    "validation_set = valid_gen.flow_from_dataframe(\n",
    "    dataframe=response.iloc[test_index],\n",
    "    directory=DATADIR,\n",
    "    x_col=\"GreenID\",\n",
    "    target_size=(224, 224),\n",
    "    color_mode=\"rgb\",\n",
    "    subset=\"training\",\n",
    "    shuffle=True,\n",
    "    batch_size=64,\n",
    "    y_col=[0, 1, 2, 3,],\n",
    "    class_mode=\"raw\",\n",
    ")\n",
    "\n",
    "test_set = valid_gen.flow_from_dataframe(\n",
    "    dataframe=response.iloc[test_index],\n",
    "    directory=DATADIR,\n",
    "    x_col=\"GreenID\",\n",
    "    target_size=(224, 224),\n",
    "    color_mode=\"rgb\",\n",
    "    subset=\"validation\",\n",
    "    shuffle=True,\n",
    "    batch_size=64,\n",
    "    y_col=[0, 1, 2, 3,],\n",
    "    class_mode=\"raw\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model__ = fine_tune_model(model__,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 15 steps, validate for 1 steps\n",
      "Epoch 1/10\n",
      "15/15 [==============================] - 10s 647ms/step - loss: 0.8732 - soft_acc_multi_output: 0.1500 - val_loss: 0.4139 - val_soft_acc_multi_output: 0.3684\n",
      "Epoch 2/10\n",
      "15/15 [==============================] - 4s 288ms/step - loss: 0.7882 - soft_acc_multi_output: 0.1792 - val_loss: 0.4187 - val_soft_acc_multi_output: 0.3509\n",
      "Epoch 3/10\n",
      "15/15 [==============================] - 4s 287ms/step - loss: 0.6969 - soft_acc_multi_output: 0.1937 - val_loss: 0.4258 - val_soft_acc_multi_output: 0.4211\n",
      "Epoch 4/10\n",
      "15/15 [==============================] - 4s 291ms/step - loss: 0.6181 - soft_acc_multi_output: 0.2625 - val_loss: 0.4342 - val_soft_acc_multi_output: 0.4035\n",
      "Epoch 5/10\n",
      "15/15 [==============================] - 4s 288ms/step - loss: 0.5712 - soft_acc_multi_output: 0.2750 - val_loss: 0.4419 - val_soft_acc_multi_output: 0.4737\n",
      "Epoch 6/10\n",
      "15/15 [==============================] - 4s 288ms/step - loss: 0.5283 - soft_acc_multi_output: 0.3583 - val_loss: 0.4488 - val_soft_acc_multi_output: 0.4912\n",
      "Epoch 7/10\n",
      "15/15 [==============================] - 4s 287ms/step - loss: 0.5110 - soft_acc_multi_output: 0.3562 - val_loss: 0.4532 - val_soft_acc_multi_output: 0.4912\n",
      "Epoch 8/10\n",
      "15/15 [==============================] - 4s 287ms/step - loss: 0.4798 - soft_acc_multi_output: 0.4250 - val_loss: 0.4554 - val_soft_acc_multi_output: 0.4912\n",
      "Epoch 9/10\n",
      "15/15 [==============================] - 4s 286ms/step - loss: 0.4474 - soft_acc_multi_output: 0.4250 - val_loss: 0.4571 - val_soft_acc_multi_output: 0.4912\n",
      "Epoch 10/10\n",
      "15/15 [==============================] - 4s 288ms/step - loss: 0.4232 - soft_acc_multi_output: 0.4750 - val_loss: 0.4561 - val_soft_acc_multi_output: 0.4912\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x282fb52fa48>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch = next(test_set)\n",
    "# true_labels = batch[1]\n",
    "# predictions = model__.predict(batch[0])\n",
    "\n",
    "# print(model__.metrics_names)\n",
    "# print(\n",
    "#     model__.evaluate(train_set, verbose=0)\n",
    "# )  # working well with original unstratified model, including both trainning and testing sets?\n",
    "\n",
    "model__.fit(\n",
    "    x=train_set,\n",
    "    epochs=10,\n",
    "    validation_data=validation_set,\n",
    "    callbacks=[early_stopping, reduce_lr_plateau],\n",
    "    #         verbose=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_base_model():\n",
    "    conv_base = enet.EfficientNetB0(\n",
    "        include_top=False, input_shape=(224, 224, 3), pooling=\"avg\", weights=\"imagenet\",\n",
    "    )\n",
    "    conv_base.trainable = False\n",
    "\n",
    "    x = conv_base.output\n",
    "    x = Dropout(0.5)(x)\n",
    "    preds = Dense(5, activation=\"softmax\")(x)\n",
    "    model = Model(inputs=conv_base.input, outputs=preds)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Nadam(lr=0.0014),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[tf.keras.metrics.CategoricalAccuracy()],\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def fine_tune_model(model, fine_tune=None):\n",
    "    if fine_tune is None:\n",
    "        try:\n",
    "            fine_tune = [layer.name for layer in model.layers].index(r\"top_conv\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    model.trainable = True\n",
    "    for layer in model.layers[:fine_tune]:\n",
    "        layer.trainable = False\n",
    "    for layer in model.layers[fine_tune:]:\n",
    "        layer.trainable = True\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Nadam(lr=0.0003),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[tf.keras.metrics.CategoricalAccuracy()],\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def generate_train_val_test(train_index, val_index, test_index):\n",
    "    train_dataset = response.iloc[train_index]\n",
    "    val_dataset = response.iloc[val_index]\n",
    "    test_dataset = response.iloc[test_index]\n",
    "    train_gen = ImageDataGenerator(\n",
    "        rotation_range=5,\n",
    "        fill_mode=\"reflect\",\n",
    "        horizontal_flip=True,\n",
    "        rescale=1.0 / 255.0,\n",
    "        zoom_range=0.1,\n",
    "    )\n",
    "    valid_test_gen = ImageDataGenerator(rescale=1.0 / 255.0,)\n",
    "\n",
    "    train_set = train_gen.flow_from_dataframe(\n",
    "        dataframe=train_dataset,\n",
    "        directory=DATADIR,\n",
    "        x_col=\"GreenID\",\n",
    "        target_size=(224, 224),\n",
    "        color_mode=\"rgb\",\n",
    "        subset=\"training\",\n",
    "        shuffle=True,\n",
    "        y_col=\"Grade\",\n",
    "    )\n",
    "\n",
    "    validation_set = valid_test_gen.flow_from_dataframe(\n",
    "        dataframe=val_dataset,\n",
    "        directory=DATADIR,\n",
    "        x_col=\"GreenID\",\n",
    "        target_size=(224, 224),\n",
    "        color_mode=\"rgb\",\n",
    "        subset=\"training\",\n",
    "        shuffle=False,\n",
    "        batch_size=64,\n",
    "        y_col=\"Grade\",\n",
    "    )\n",
    "\n",
    "    test_set = valid_test_gen.flow_from_dataframe(\n",
    "        dataframe=test_dataset,\n",
    "        directory=DATADIR,\n",
    "        x_col=\"GreenID\",\n",
    "        target_size=(224, 224),\n",
    "        color_mode=\"rgb\",\n",
    "        subset=\"training\",\n",
    "        shuffle=False,\n",
    "        batch_size=64,\n",
    "        y_col=\"Grade\",\n",
    "    )\n",
    "    return train_set, validation_set, test_set\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "innerkf = StratifiedKFold(n_splits=2, shuffle=True, random_state=seed)\n",
    "response = response.sample(frac=1.0)\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=21, restore_best_weights=True,\n",
    ")\n",
    "reduce_lr_plateau = ReduceLROnPlateau(monitor=\"val_loss\", patience=7, factor=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "def stratified_cv(fine_tune_layer=None):\n",
    "    acc_coef_scores = []\n",
    "    raw_outputs = []\n",
    "    for train_index, val_test_index in kf.split(\n",
    "        np.zeros(len(response)), response[\"Grade\"]\n",
    "    ):\n",
    "        val_index, test_index = next(\n",
    "            innerkf.split(\n",
    "                np.zeros(len(val_test_index)), response[\"Grade\"].iloc[val_test_index]\n",
    "            )\n",
    "        )\n",
    "        val_index, test_index = val_test_index[val_index], val_test_index[test_index]\n",
    "        train_set, validation_set, test_set = generate_train_val_test(\n",
    "            train_index, val_index, test_index\n",
    "        )\n",
    "        model = generate_base_model()\n",
    "\n",
    "        _ = model.fit(\n",
    "            x=train_set,\n",
    "            epochs=15,\n",
    "            validation_data=validation_set,\n",
    "            callbacks=[early_stopping, reduce_lr_plateau],\n",
    "            verbose=0,\n",
    "        )\n",
    "\n",
    "        model = fine_tune_model(model, fine_tune=fine_tune_layer)\n",
    "\n",
    "        _ = model.fit(\n",
    "            x=train_set,\n",
    "            epochs=100,\n",
    "            validation_data=validation_set,\n",
    "            callbacks=[early_stopping, reduce_lr_plateau],\n",
    "            verbose=0,\n",
    "        )\n",
    "\n",
    "        batch = next(test_set)\n",
    "        true_labels = batch[1]\n",
    "        predictions = model.predict(batch[0])\n",
    "        acc = soft_acc_multi_output(predictions, true_labels).numpy()\n",
    "        corr = np.corrcoef(np.argmax(predictions, axis=1),np.argmax(true_labels, axis=1))[0][\n",
    "            1\n",
    "        ]\n",
    "        acc_coef_scores.append([acc, corr])\n",
    "        raw_outputs.append([np.array(response.iloc[test_index].index), true_labels, predictions])\n",
    "        del train_set, validation_set, test_set, _, model, batch, true_labels, predictions, acc, corr\n",
    "        tf.keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "    return acc_coef_scores, raw_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  np.where(np.array(['conv' in layer.name for layer in model.layers]) == True)[0][::-1]\n",
    "trainable_sequence = np.array([227, 225, 217, 214, 210, 202, 199, 195, 187, 184, 180, 172, 169,\n",
    "       167, 159, 156, 152, 144, 141, 137, 129, 126, 124, 116, 113, 109,\n",
    "       101,  98,  94,  86,  83,  81,  73,  70,  66,  58,  55,  53,  45,\n",
    "        42,  38,  30,  27,  25,  17,  14,  12,   4,   1])\n",
    "\n",
    "fine_tune_scores_acc_coef = []\n",
    "fine_tune_raw_outputs = []\n",
    "\n",
    "for fine_tune in trainable_sequence[20:30]:\n",
    "    acc_coef_scores, raw_outputs = stratified_cv(fine_tune)\n",
    "    fine_tune_scores_acc_coef.append(acc_coef_scores)\n",
    "    fine_tune_raw_outputs.append(raw_outputs)\n",
    "    np.save(r\"./stratified_cross_validation_results/effnets/onehot_acc_coef_20-30\", np.array(fine_tune_scores_acc_coef))\n",
    "    np.save(r\"./stratified_cross_validation_results/effnets/onehot_raw_outputs_20-30\", np.array(fine_tune_raw_outputs))\n",
    "    del acc_coef_scores, raw_outputs\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "[[32  9  2  0  1]\n",
      " [12 34  4  4  0]\n",
      " [13 20 26  9 10]\n",
      " [ 1 13  3 18 14]\n",
      " [ 2  3  5 18 29]]\n",
      "+++++++++++++++++++++++++++++++++\n",
      "+++++++++++++++++++++++++++++++++\n",
      "[[0.72727273 0.20454545 0.04545455 0.         0.02272727]\n",
      " [0.22222222 0.62962963 0.07407407 0.07407407 0.        ]\n",
      " [0.16666667 0.25641026 0.33333333 0.11538462 0.12820513]\n",
      " [0.02040816 0.26530612 0.06122449 0.36734694 0.28571429]\n",
      " [0.03508772 0.05263158 0.0877193  0.31578947 0.50877193]]\n",
      "+++++++++++++++++++++++++++++++++\n",
      "+++++++++++++++++++++++++++++++++\n",
      "49.29078014184397\n",
      "+++++++++++++++++++++++++++++++++\n",
      "+++++++++++++++++++++++++++++++++\n",
      "max accuracy with tuning from 98 layers, or tune 135 layers\n",
      "27 [72.72727272727273, 62.96296296296296, 33.33333333333333, 36.734693877551024, 50.877192982456144]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD7CAYAAABnoJM0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO29eXxcd33v/f7OaEajXaPdluzIi+zEcYIT20lICDiUQMKSQAMlIWW9NIWSJi29fUoXaBvK697SPuWWNuU28CSBlpCmCRQXEgK0NovJYjt2Fi+KbXmTZG22pBlJM5Jm5vv8MXNGY3k0OrPImhn93q+XXtacOefodzzS+ZzvLqqKwWAwGJYujsVegMFgMBgWFyMEBoPBsMQxQmAwGAxLHCMEBoPBsMQxQmAwGAxLHCMEBoPBsMSxJQQicouIdIrIURH5XJL3PyYigyKyP/b1yYT3PioiR2JfH03YvllEXo2d86siIrm5JIPBYDCkg8xXRyAiTuB14GagG9gN3KWqBxP2+RiwRVXvnXVsHbAH2AIosBfYrKrDIvIicD/wPPA08FVVfSZH12UwGAwGm5TY2Oca4KiqdgGIyOPA7cDBlEdFeQfwE1U9Fzv2J8AtIrITqFbV52LbvwW8F0gpBA0NDdre3m7jxxoMBoPBYu/evUOq2jjX+3aEoBU4nfC6G7g2yX53iMibiVoPv6+qp+c4tjX21Z1ke0ra29vZs2ePjSUbDAaDwUJETqZ6306MIJnvfrY/6T+BdlW9Evgp8M15jrVzzugJRO4RkT0ismdwcNDGcg0Gg8GQDnaEoBtYkfC6DehN3EFVz6rqZOzl14HN8xzbHft+znMmnPshVd2iqlsaG+e0bAwGg8GQIXaEYDfQISKrRMQN3AlsT9xBRJYlvLwNOBT7/lng7SLiFREv8HbgWVU9A/hF5LpYttBHgO9neS0Gg8FgyIB5YwSqGhKRe4ne1J3Aw6p6QEQeAPao6nbgPhG5DQgB54CPxY49JyJfJComAA9YgWPg08CjQBnRILHJGDIYDIZFYN700Xxiy5YtaoLFBoPBkB4isldVt8z1vqksNhgMhiWOEQKDwWBY4hghMBgMhhwQiSi/OjrEv+0+RSG53MFeQZnBYDAY5qBnJMCTe7r5972n6R4OxLYF+ezN6xZ5ZfYxQmAwGAxpMhkK85OD/fzb7tP88ugQqnDD2nr+8B3r+eWRIb76X0dY4S3jA1tWzH+yPMAIgcFgMNjkYK+PJ/ac5j/29zAyMc3yGg+/+9YOPrC5jRV15QC884plnBkN8sfffZXltWXcsLZhkVc9PyZ91GAoEPzBaXYdPcstG1sWeylLClXl3/d2863nTvBajw+308HbL2/mg1tXcP2aBpyOCzvm+ILTfOBrz9E7EuDJT1/P+paqi7/wBEz6qMFQJPz7nm4+9a97GfAFF3spSwZ/cJrf+fZL/D9PvkI4An/xng28+Ke/xj9+6Gpu7GhMKgIA1R4XD398K2VuJ594dHfef2ZGCAyGAuH08AQAQ2NTi7ySpcHhPh+3/eMufnywnz9952U8fd+b+NgNq6gtd9s6vrW2jIc/tpXhiSk+8c3djE+GFnjFmWOEwGAoEHpiGSkjE0YIFpon93bz3gd3MT4Z4ju/dR2/9ebVZDJEcWNrDf/4oas42Ovjvu/sIxzJT1e8EQIDAIGpMD96rW+xl2FIQe9oVAjOGSFYMILTYf74u6/wP//9ZTatqOUH972Ja1bVZXXOt17azF/edjn/dXiAv/zPA3lZY2CyhgwA/OOOIzy44xg//v03s655cQNbhuT0jkT9zMPjRggWgtPnJvj0t/fyWo+P39m2hs/evI4SZ26elT/8xnZODwd46OddrKwr55M3rs7JeXOFEQIDwekwj71wCoDX+/1GCPKQwFSYczEBGJ6YXuTVFB8/PdjPZ5/YD8A3PrKFt21ozvnP+Nwtl3L63ARfevoQbd4ybtm4bP6DLhLGNWTg+/t74jeXI/1ji7waQzJ6RgLx788ZiyBnhMIR/vpHh/nkt/awsr6cH/zujQsiAgAOh/CVD25i04pa7n98P/tODS/Iz8kEYxEscVSVR3ad4NKWKiamwhwdNEKQj/QmCIEJFueGkYkpPvWve3m+6xx3XbOCP3/P5XhczgX9mR6Xk298ZAvv+6df8clv7uH33taBY44U1Nl8cMuKnLmqZmOEYInzXNdZDvf5+es7ruAnB/s5aiyCvMQSgobKUs4Z11BOePRXJ3jh+Dn+9gNv4P2b2+Y/IEfUV5by6Me38hv//Byf//4B28fdcXUbJQukU0YIljiP7DqBt9zF7Zta6Roa52evDxIKRxbsycOQGb0jARwCly2rMsHiHLH35DDrm6suqghYrG6s5Jd/9FZ8AfuiXlqycH+TRgiWMKfOTvDTQ/38zrY1eFxOOpqqmA4rp85NsLqxcrGXZ0igeyRAS7WHxspSjg+NL/ZyCp5IRHn59AjvunLxArYel3PBXVF2MY99S5hvPncCpwgfvq4dgLVN0Zv/kQHjHso3ekcCLK8tw1vhXpIWgT84zQtdZ3N2vq6hcXzBEFet8ObsnIWMEYIlythkiCd2n+bWK5bRUuMBZoTgqBGCvKN3JBgVgnIX41NhJkPhxV7SRSM4HeYTj+7mzq8/z5nRwPwH2GD/6REANq2szcn5Ch1bQiAit4hIp4gcFZHPpdjv/SKiIrIl9vpuEdmf8BURkU2x93bGzmm915SbSzLY4am93fgnQ3z8hvb4tsrSEpbXeIwQ5BmRiHJmdMYiABhZIgHjSET5wydfYfeJYVRh94ncpFzuOzVMVWkJa40LFLAhBCLiBB4EbgU2AHeJyIYk+1UB9wEvWNtU9duquklVNwEfBk6o6v6Ew+623lfVgSyvxWCTSER59FcneMOKWq5eeb5pvKapkiMD/kVamSEZg2OTTIeVVm8Z3ljDs+ElkkL6tz/u5D9f7uUP37GeCreT3cfP5eS8+0+PcOWKGtupm8WOHYvgGuCoqnap6hTwOHB7kv2+CHwZmKvf6l3AdzJapSGn/Oz1QY4PjfOJBGvAoqOpimMD40TytDnWUsQqJmut9cSFYCkUlX3nxVP8085jfOjalfzOtjVcfYmX3SeyF4LAVJjDfX4TH0jAjhC0AqcTXnfHtsURkauAFar6gxTn+SAXCsEjMbfQ5yWT1n6GjHh413Gaqkq5NUmJe0dzJYHp8HmVrIbFxaohiLqGXAAMjxe3a+hnrw/yZ//xGm9Z18gDt12OiLC1vY7Ofj+jaaRcJuPVnlHCEWXTChMfsLAjBMlu0PHHRRFxAF8B/mDOE4hcC0yo6msJm+9W1SuAG2NfH57j2HtEZI+I7BkcHLSxXEMqjg74+cWRIT583SW4k+Qlm4Bx/mG1n26tLaNuCbiGDvb6+My3X2JdcxUP3n11vKZla3sdqvDSyeziBFZrBxMonsGOEHQDiROY24DehNdVwEZgp4icAK4DtlsB4xh3MssaUNWe2L9+4DGiLqgLUNWHVHWLqm5pbGy0sVxDKh7ZdQJ3iYMPXbsy6ftW8MwIQf7QOxKgylNClccVH4pSrCmkfaNBPvHobipLS3jkY1upLJ0pddq0ohaXU3gxS/fQ/tMjrKgro6GyNNvlFg12hGA30CEiq0TETfSmvt16U1VHVbVBVdtVtR14HrhNVfdA3GL4ANHYArFtJSLSEPveBbwbSLQWDAvA6MQ0332ph9vfsJz6Of4IvBVuGipLTcA4j+gZCdJaWwaAu8RBZWlJUXYgHZsM8YlHdzM2GeKRj2+NpzVblLmdbGytYU+WQrDv1IiJD8xiXiFQ1RBwL/AscAh4QlUPiMgDInKbjZ/xZqBbVbsStpUCz4rIK8B+oAf4etqrN6TF47tPEZgO8/EbVqXcb21ThSkqyyOsYjILb4Wr6FxDoXCEex97ic5+Pw/efTWXLatOut/W9jpePj1KcDqzOoozowH6fEETH5iFrRYTqvo08PSsbV+YY99ts17vJOouStw2DmxOY52GLAmFI3zruZNcu6qODcuT/5FZdDRV8R/7e1DVjMbzGXJLz0iAzZfMPMF6y91FlTWkqvz59gPs7Bzkf/36Fbxl3dwu4K3tdTz08y5e7Rlla3v6k8P2n4oWkl1l4gPnYSqLlwg/OdhPz0hgXmsAogFjfzDEgH/yIqzMkIqxyRCjgenzLYJyd1G1on7o5118+4VT/M62Ndx1TfLYlcWWmCC+mGE9wf7TI7idjnkfhpYaRgiWCI/sOkGbt4ybbQzd6DCZQ3nDmXjq6Iy/vK7CXTRzi3/4yhn+1zOHec8blvM/375+3v29FW46miozrifYd2qEDcurKV2ofs4FihGCJcBrPaO8eOIcH31jO04blZRrm2PN5/pNwHix6Y4JQZt3xiKoLXcxUiR1BH++/QCbVtTyN++/0naV75b2OvaeHCacZtFjKBzh1Z5REx9IghGCJcAju05Q7nbyG1tXzL8z0FhZSrWnxASM84DEYjKLunI3/skQU6HIYi0rJ0QiytnxSd6yrjGtdszXrPLiD4bo7EvvQaWz309gOmziA0kwQlDkDPon+c+Xe7nj6jZqyly2jhEROpqrjGsoD+gdCeB0CE1VM66h2njjucJ2D/knQ6hCtc3fS4stl0SDxHtOpuce2mcFik3q6AUYIShyHnvhFFPhCB9L0lcoFR1NlUYI8oDekSAt1Z7zXHoz1cWF7R6ypnNVe9Kbj9XmLWNZjSftgPH+0yPUV7hZUVc2/85LDCMERczRAT/f+GUX29Y3sibNdrtrmyo5Oz5VVGmKhUjPcIBW7/k3Lm959Am60D8bXzAqBFWe9CwCEWFLex27T5xD1X6cYN+pYTatqDUp0UkwQlCkDPon+dgjuyktcfLF2zemfbzpOZQf9IwE4lXFFt4icQ35AiEAqsvSn5h7TbuXft8k3cP2miOOBqY5Njhu4gNzYISgCAlMhfnkN3dzdmyKhz+2hRV15WmfY2ZspckcWizCEaXPFzwvdRSi6aNAwaeQWhZBdZoWAUQzhwDbaaQvWxPJTHwgKUYIioxwRLn/8X280jPKV++6iivbMnsCWl5TRrnbaSyCRWTAHyQcUVprzxfy2phrqNCnlFkxArtJDImsb66iylNiWwj2nx5BBK5cUZP2z1oKGCEoMr70w0P8+GA/f/7uDbaKx+bC4RDWmoDxomK1n55tEZSWOKlwOws+RuAPxlxDGVgEDoew5RKv7YDxvlPDrG2szOhnLQWMEBQRj+w6zsO7jvOJG1bxMRutJOZjbWMlR/qNECwWM5PJLsxyqS13F3wrass1VJlm1pDF1lV1HBsc5+xY6lYoqsr+0yMmPpACIwRFwo8P9PHADw7y9g3N/Om7LsvJOdc2V9LnC+IPFrYLolDpHYlOfV2eRAjqKtwF34HUFwhRVVpiq9o9Gde0W/UEqQfVnDw7wfDEtIkPpMAIwQKjqgtuwr98eoT7Ht/Hla01/P2dV2X8hzWbjqYqwGQOLRY9IxPUlruoKL3widlb4eZcoccIgtNUZWgNAFzRVoO7xDHvfIL9p03H0fkwQrDA7Ogc4Oov/oT3/MMv+ZfnTmQ9b3U2p89N8D++uYeGylK+8dGtlLlz10xrJnPICMFi0DsSZHlN8uInb7mrCNJHp9OuKk6ktMTJprZaXjyR2iLYd2qYcreTdc1VGf+sYscIwQKz9+QwTocQiiif//4BrvnST7n/8X386ugQkTSbZs1mNDDNxx/dzVQozKMf30pjVW5H763wluEucXDMCMGiMHsgTSLFMJPAF5zOOni7pd3LgZ5RJqZCc+6z//QIV7bV5MxSLkaMECwwnX1jrG6o4Jn7b+QHv/smPrh1BTsOD/Chb7zAm/9mB3//0yPxoGA6TIUifOpf9nLy7Dj//OEtrG3K/dNOidPB6gYzrWyx6BkJnNd1NBFvuRt/MMR0uHAbz/kCoYyKyRLZuqqOUETjA2dmE5wOc/CMz8QH5iG7T8EwL539vngu/8bWGja21vAn77yMZw/08cSe03zlp6/zf/7rdd60toHf2LKCN7TVYqcC/is/fZ3nus7ylQ++gTeuqV+w9a9tquTl7uR/ZIaFwxecxh8MXZA6alFXMVNLkGtL8GLhn5ym2pPdA8zVK72IwO4Tw1y/tuGC9w/0+pgOq4kPzIMRggVkfDLE6XMBPrD5/PbPHpeT2ze1cvumVk6fm+DJvd08ubeb3/3OvrTO/9mb1/G+q9pyueQL6Giq4oevniEwFc5p/MGQmmTtpxOx2kwMT0wVrBBELYLsXEM1ZS4ubames7Bs36lo/OAqM4MgJbaEQERuAf4ecALfUNX/Pcd+7wf+HdiqqntEpJ3owPvO2C7Pq+qnYvtuBh4FyojOQ75f0+kgVQC8Hhvssr5l7qeeFXXl/P7N67jv1zp4oessvaNBW+eur3Czbf3cs11zxdqmSlTh2OAYG1tNVebFYl4hsDqQFmicIBJR/MHptDuPJmNru5cn93YTCkcocZ7v7d5/eoTW2jKaqpNbVoYo834KIuIEHgRuBrqB3SKyXVUPztqvCrgPeGHWKY6p6qYkp/4acA/wPFEhuAV4Ju0ryGPiQmAjW8HpkKSm7WLTEZtWZoTg4tITqyFom08ICjRzaHwqRETT7zyajK3tdXzruZMcPOO7oKXKvlMjZiKZDewEi68Bjqpql6pOAY8DtyfZ74vAl4F5H2lFZBlQrarPxayAbwHvtb/swqCzbwyPy8HKDJq+5Qvt9RU4HWIqjC8yPcMBXE6hoTK528dbYbWiLsxaAl8w886js9kaKyyb3W5iwB+kZyRg4gM2sCMErcDphNfdsW1xROQqYIWq/iDJ8atEZJ+I/ExEbkw4Z3eqcxYDnf0+1jVX2Z7Fmo+4SxxcUl9uupBeZHpHAiyrKZvzd6fQLYKZoTTZWwQtNR5W1JWxZ1Y9gZVJZCyC+bEjBMl+E+O+fBFxAF8B/iDJfmeAlap6FfBZ4DERqZ7vnOf9cJF7RGSPiOwZHBy0sdz8obNvrCiKWMy0sotPtIZgbr+2x+WkzOUs2BhBXAiyDBZbbL3kwkE1+0+PUOIQ49K0gR0h6AYS017agN6E11XARmCniJwArgO2i8gWVZ1U1bMAqroXOAasi52zLcU546jqQ6q6RVW3NDYufHA0V5wdm2RobJJLUwSKC4WOpipOnJ0o+GHphUR0IE1ql2K031Bhuoay6TyajK2r6jg7PsXxofH4tn2nRrhsWTUel8l2mw87QrAb6BCRVSLiBu4EtltvquqoqjaoaruqthMN/t4WyxpqjAWbEZHVQAfQpapnAL+IXCfRuXEfAb6f20tbXDpjgeJisAjWNlUSjignzo7Pv7Mha6bDEfp9QVpTWAQQjRMUrGvIGkqTgxgBRDOHYGZQTTiivNJtOo7aZV4hUNUQcC/wLNFU0CdU9YCIPCAit81z+JuBV0TkZeBJ4FOqakV0Pg18AzhK1FIoroyhvqgQFINFYMZWXlz6fUEiOnfqqIW3vHA7kOYyRgCwprGSugo3u2NxgiMDfsanwiY+YBNbcqyqTxNN8Uzc9oU59t2W8P1TwFNz7LeHqEupKOns91Nb7irYYp9E1jRWIkI0c+iKxV5N8WO1n549tH423nI3p89NXIwl5RwrayjTWQSzEYkOqrEsAitQfNVK01rCDqbX0ALR2ednfXMVYqdfRJ5T5nbS5i3j6KCxCC4GPSPRm/v8FoGrYBvP+QLTlLuduJy5uwVtba/j5NkJBnxB9p0aobbcRXt94aZuX0yWhBAM+IIXtTmXqvJ6/1jKiuJCo6OpiiP9JoX0YhAfSDNHC2oLb4UbXzBEqAAbz+Wi8+hstq6yBtoPs/90tJCsGB7ELgZFLwTT4QgfefhFPvT15+n32WvfkC09IwHGJkNFESi2WNtUSdfQOOEsW2cb5qdnJEBdhXve3k51sX5DIzmecXExyEXn0dlcvryaMpeTHZ0DvD7gN/GBNCh6IXA5HXx62xpe6/Hxrq/+gl8dG1rwn2m1liiGQLHF2qZKpkKRgvVJFxK9I4Gkc4pnUxsrKivEATXRzqO5tQhcTgdXraxl+/5eVE18IB2KXggAbt/UyvZ7b6CmzMVvfuMFHtxxNOuhMKk4HMsY6igii6DDTCu7aPQMpy4ms6iLCUEhtpnIRefRZGxpr2Mq5irb1GYsArssCSGA6E35+/e+iXdesYy/ebaT3/rWHkYXqBjn9T4/y2s81CzAL/pisSYuBCZOsJCoasrJZInUllv9hgrPIvDlqPPobKyB9qsbK6gpL56/v4VmyQgBQGVpCf9w11X85W2X8/Mjg7zrH37Bq92jOf85nf1jrCsitxBE871bqj1FWUvwiyODbPubHYxPzj3uMF12HR3ipr/dmbbbxhcIMT4VtuUaiscICtA15AtM56Tz6GyuWlmL0yEmPpAmS0oIIJpv/NHr2/m3334jkYhyx9d+xWMvnDqvR0k2TIcjHBsYs9V6utDoaC7OnkNP7e3mxNmJnFZO7z05zPGhcX58sD+t47pjqaN2hMBqPHeuwIRAVfEFcx8sBqgoLeGff3Mzv/dr63J+7mJmyQmBxdUrvfzgvhu5dnUdf/K9V/mDJ14mMBXO+rwnz44zFY4UVeqoxZrGqBAU0/ygcET52evRZoYDvsmcndfKUPvRa31pHRdPHbUhBGVuJx6Xg5EC6zc0MRUmHNGcB4st3rahmZWmfiAtlqwQQNS0fvTj13D/r3Xwvf09vPfBXXRlWTRlBYqLKXXUoqO5kompsO0paoXAy90j8cZtfTlML7aE4BdHBuN9deww32Sy2dSVuwsuRjDTZ8j48POFJS0EEJ0M9vs3r+PRj1/DgD/I7f+4iwF/5jeE1/v8OGSmP08xsbYxFjAuosKynYcHcAiIQF8OBa7PF6Sh0s10WPnvQwO2j+sdCeAucdBQ6ba1f225u+BaUee686ghe5a8EFi8ZV0j//I/rsU/GeK/0vjDnU1nv5/2hoqibH1rpcMWU5xgR+cgmy/xUl9RmtUDwGz6fZO89dImmqtLeea1M7aP647VENitiI22oi4sIZiZRZD7GIEhM4wQJHD58mpaa8vYcTgLIYj1GCpG6irc1Fe4i0YIBvxBXu0ZZdv66A07VxbBdDjC0Ngky2rKeMflLezsHLSdkTTfQJrZ1Ja7Cm4mQdw1ZCyCvMEIQQIiwk2XNvLLo0NMhtIPHAemwpw8N1GUgWKLNU2VRVNU9rPOaJD4pvVNtFR76MtRsHjQP4kqNFd7uHXjMiZDEXZ22puuZ7eq2KIwLYKoKFYtQB2BITOMEMzipvVNTEyF2X18eP6dZ3FkwI8qRWsRwMzYymLIHNrZOUhzdSmXLauiqdrDQI6CxVaguKWmlGtW1VFf4bblHpoKRRjwT9oOFEM0hXQ0MF1QPaBMsDj/MEIwizeuqcdd4mBHZ/ruoc5YxlAxWwQdTZWMBqYZHMtdquViMB2O8PMjg9y0vgkRoaXaw9nxqYwswdlYQtBU5cHpEN5+eTM7Dg8QnE597r7RIGpjIE0i3nIXqjBaQI3nrBiBsQjyByMEsyh3l3Dd6vqMhOD1fj/uEgeX1FcswMryg7VNxREwfunkMP5giG3rm4Do0ztE3TrZ0h9zMbXURH39t2xcxvhUmF8cSd3wsCeWOpqOa8hbYfUbKhz3kC8YwuNyUFpSfAkVhYoRgiTctL6RrsFxTqZZaXq4z09HUyVOR/H2QO9oLo6xlTs6B3E5hRvW1gPQVB29aeeiVXmfL4jLKfGmcNevqaemzMUzr6Z2D/VmIgSxn1FIcQL/AswiMGSHEYIk3BR7SrQb4LN4vd9f1G4hgKaqUqo8JXE3WKGys3OAre118X43LTEh6BvNgUUwGqSpyoMj9kDgcjp422XN/ORQP1OhuYfIWBaBZUnYweo3lGktwe4T57jroefndVvlkoXqPGrIHCMESWhvqGBVQ0Va7qGRiSn6fZNFHSiGaGbV1Su9PNd1drGXkjG9IwEO9/njgg8zQpALi6DfH6S5+vxZ1bdubMEfDKWch9E7EqChsjStGhTLNZSpRfBfhwZ4rutsfNbvxWChOo8aMseWEIjILSLSKSJHReRzKfZ7v4ioiGyJvb5ZRPaKyKuxf9+asO/O2Dn3x76a5jrvYrBtfSPPHTtru//QUggUW2TqOssXLEtv2/rG+LbachfuEkduXEOjwQue6t/U0UCF25my91DPSGDegfWz8cZaLWdaS3As1lJl19GLJ+wL1XnUkDnzCoGIOIEHgVuBDcBdIrIhyX5VwH3ACwmbh4D3qOoVwEeBf5l12N2quin2lXkV1wJw0/omJkMRnrf55GtNJVsKQrAtQ9dZvrCjc4DW2rLz2oCISLSoLBcWgW+SpqrzhcDjcvJrlzXz7IG+OWcMR2sI7LuFAMpcTkpLHBm7hqzeWs9dhMl9FtHOo0YI8gk7FsE1wFFV7VLVKeBx4PYk+30R+DIQ/0tS1X2q2ht7eQDwiEhpkmPzjmtW1cXnn9rhcJ+fKk9J3MVQzLQ3VLA6TddZvjAZCkdnBVzaeEEbh5ZqT9YWwdhkiLHJUFI//60bWxiemObF4xe6YVSVnpHAvAPrZyMieDNsPBcKRzh1boLSEgev9oxetBRUX8C4hvINO0LQCpxOeN0d2xZHRK4CVqjqD1Kc5w5gn6omRuMeibmFPi9zNFcRkXtEZI+I7BkcvHhPoB6XkxvW1vPfhwdsFU+93u/n0pYq2z1iCp1t65vScp3lC7uPDzMxFT4vPmDRVO2Jp35mSryYLMkDwVvWN+JxOXgmiXtoeGKa4HQkrRoCC2+FOyPX0OnhANNh5bY3LCei2LZ+syE6i2DaWAR5hh0hSHZni98ZRcQBfAX4gzlPIHI58NfAbydsvjvmMrox9vXhZMeq6kOqukVVtzQ2NibbZcHYtr6J7uEAxwZT+8JVlcN9/qJsPT0XN13amJbrLF/Y0TmAu8TBG9fUX/BeS7UnVtSVeZVuf6xfUVP1hYZvubuEbeua+NGBvgtmZsdTR9OMEUA0TpBJsNhyC92xuQ2Py8Fzxxb+s5wMRZgOL9wsAkNm2BGCbmBFwus2oDfhdRWwEdgpIieA64DtCQHjNuB7wA6yhYUAACAASURBVEdU9Zh1kKr2xP71A48RdUHlFVYwcec8LpA+XxB/MMSlSyA+YJGu6yxf2NE5wHWr6yl3X+iaaKn2EJgO489iZGW/f26LAODWK1oY9E+y99T5LUwyKSaz8GbYb6gr9oBzaUsVW9vr2HV04eMEpvNofmJHCHYDHSKySkTcwJ3AdutNVR1V1QZVbVfVduB54DZV3SMitcAPgT9W1V3WMSJSIiINse9dwLuB13J2VTmizVvOuubKeW92nUU8jGYuSkuc3LC2wbbrbD7+8j8P8P39PTlY2dycPDtO1+A4N61PbllaT/H9WXQhteoQmucQgrde2oTb6eCZV893D/UMpzeQJpG6DGcSHBsco67CTW25mxvWNnBkYCynrbiTYTqP5ifzCoGqhoB7gWeBQ8ATqnpARB4QkdvmOfxeYC3w+VlpoqXAsyLyCrAf6AG+ns2FLBQ3rW/ixePnGEvxlLiUUkcTuenSRluus/noGhzjkV0n+N6+hRWCnQndRpMRLyrLImDc7wtSVVpCRWnyJ94qj4sbOxp49kDfeQLaOxKgzOWMp4Omg7fcxUgGjee6BsdZ3RBth3LDmgaABXcPjZrOo3mJrToCVX1aVdep6hpV/VJs2xdUdXuSfbep6p7Y93+lqhUJKaKbVHVAVcdVdbOqXqmql6vq/aqal1HHbeubmA5rSrO5s99Pc3UpteX2pkoVCzNppNm5h556qRuAE0MLW5ewo3OAVQ0VtDck7wXVHC8qyzxg3O8L0jxPZfAtG1voGQnwSvdofFvvaHQOQSbJBt4KN6ozbhe7dA2Nsbox+n+xYXk11Z6SBXcPmc6j+YmpLJ6HLe1eKktLUt7sOpdYoNiitbaM9c1VWcUJIhHley9FLYFoFsvcLRiyITAV5rljZ88rIpuNlfKZTQppn+/CquLZ3LyhmRKH8HRCa+qe4UBGbiGY6Td0Lo04wWhgmqGxKdbExo86HcIb19TzqwW2COIxAuMayiuMEMyDy+ngxo4GdhweTOoLD0eUIwNjSypQnMi2SxvndZ2l4rmus/SOBtm2vpFwROmO+cpzzfNdZ5kMReZ0C0E0ZbimzJWVEAz4JueMD1jUlrt545p6fvTajHuoZySYUaAYZtpMjKQhBFbG0OrGmaK669c00D0c4NTZiYzWYQefNa/YBIvzCiMENrhpfRN9viCHkzRaO3l2nKlQZElaBBD9v5nPdZaKp/Z2U+Up4Z4bVwML5x7a0TlAmcvJNavqUu6XzcjKSETp9wVtFRXeunEZJ89OcOiMn+B0mKGxyYyFwOpyem7cvmvIiutYriEg3ok1VT+kbPGbYHFeYoTABm+JuROSuUCWaqDYYvMlXqrmcZ3NxdhkiGde6+PdVy6L//8dXwAhUFX++/AAN6ytn7ehW3MW1cVnx6cIRXReiwDg7Zc34xD40Wtn4sKTqWuoNt5vKD2LoMQhrKwrj29b01hJU1UpuxbQPeQLhHCXONJqrGdYeIwQ2KC52sPly6vZefjCyubOfj8i0NG0NIXA5XRw47q5XWepeObVMwSmw9xxdRt1FW6qPCWcWIBGdscGx+keDsSD26loyaK62BIQO0LQUFnK1vY6nnmtL15DkKkQZNKKumtwnJV15bicM7cAEeH6NfU8d2xowUaR+swsgrzECIFNblrfxN5Tw4zOKuXv7PNzSV05Ze6l+4SzLYXrLBVPvdRNe305my/xIiKsaqhYEIvAslZSBYotmqs9DI5NZjQDeGZWsb1+U++8YhlHBsb4+evRB4y2DKqKAcrdTtxOR1rB4mjGUOUF269f28DQ2BSv9y/M4CFfYNrEB/IQIwQ2uenSaDDzF0fPtwo6l8AwmvnYtm5u19lcnD43wfNd57jj6rZ4ymR7fcWCWAQ7OgdY11xJm7d83n2bazyEI8pQBjOZ++IWgb2+iu+4vAWAx148hYg9SyIZIoK3wsWIzRhBOKKcODvBmsYL02ivj7XeWKg0Ul8wZFpQ5yFGCGyyaYWX2nIXOxLcQ8HpMCeGxot+GM18NFV72Nia3HU2F9+NpYy+7+qZ/oXtDRX0DAdSTvFKl7HJEC8eP5cyWyiRbAbU9PsmEYHGSntC0FLj4eqVtfiDIZqqSnGXZP7n6C1327YIrP/j1UmEoM1bziX15QuWRmo6j+YnRghs4nQIb+5o5GevD8Qbhh0dGCOisG6JWwQwt+ssGarKd/d188bV9ec9pa9qKCeicOpc7tIXdx0dYjqstuIDMPM0n0nmUP9okIbKUkqc9v+sbt24DMg8PmDhLXfbTh89liR1NJHr1zTwQtfZOecmZIPpPJqfGCFIg5subWRobIrXeqMVodYwmqVaQ5DItvVNSV1nydhzcpiTZye4Y3Pbedvb66NPqLlMId3ZOUBlaQlb2r229s/GIuizmTqayC0bo+6hTFNHLeoq7M8kiAvBHBXW16+pxz8Z4tWe0aTvZ4M/GDLB4jzECEEavLmjERHi7qHOPj9up4NL6pP/QS0lNq2ovcB1NhdP7e2m3O3k1thN0GJV7MaUqziBqrLj8CA3djSclx2TivrKUpwOyShzqN8XTNvPv6KunN9+y2reu6l1/p1TUFvusj2ToGtonJoyVzzbaDZWi+6FcA+ZYHF+YoQgDeorS3lDW208KNrZ72dNU6Xtm0wx43QIb1l3vussGcHpMD985Qy3blx2QWO22nI3teWunGUOHe7z0+cL2o4PQPQ6GiszG1nZb6O9RDL++NbLeNuG5rSPS6SuIuoaSvV/b9E1OMaaxoo5+xo1VJZyaUtVzgvLgtNhJkMRYxHkIeYOliY3rW/i5e4Rzo5N8nqfn/XNyf2sS5Gb1jed5zpLxrMH+vBPhrhjc/In4FxmDlmC/RYbaaOJNNekX1QWnA4zPDG9aKNKa8vdRHSmqVsqugbH54wPWFy/poE9J4YJTueuF6Q/3l7CCEG+YYQgTW66tBFV+MErZ+gdDZpAcQJvXne+6ywZT+7tprW2jOtWXTghDKLuoRNDuQkW7zw8yOXLq9N21zRXlaYtBIP+2BwCmzUEuaauwqouTi0E/uA0A/7JpBlDidywtp7JUISXZg3QyYaZWQTGNZRvGCFIk43La2iodPONX3YBJlCcSF2Fm00rauesJ+gbDbLr6BC/fnUrDkdyt0R7fQW9o4Gsn0SnwxH2d4/E8+LToaXGk3bWUF8aVcULQbwD6TwBY2sq2eqG1BbBNavqcDqEXx3NXZzAdB7NX4wQpInDIbxlXROnz0XbAizVZnNzkeg6m8339vUQUfj1q9uSHBmlvaEczUEKaddgtBnghuXVaR/bXO3BFwwRmLIvRqmG1l8MLCGYr81E11A0YyhZMVkiVR4XV7bV5DROYDqP5i9GCDLgpkujPufK0pKs0/6KjZvWN6EKPz9yvntIVXnqpW42X+KNZwclw3ov24DxoTM+ADYsq0n72OYMUkgtC2KxhCDeb2ieWoKuwXEcAivr56+yvn5NPS93j8Y7hmaL6TyavxghyIAb1zbidAjrmiszmihVzFy+vJqGytIL4gSvdI9ydGCM92+e2xoA4tPDsq0lOHjGh7vEMa8vPBmZjKzs9wUpLXEs2tOu3Q6kVrO50pL5e2PdsKaBcETZfeJcTtboC5hgcb5ihCADaspdfOKGdt6/ecViLyXvcDiEbesb+dnrg+c1bnvqpW5KSxy868plKY+v9rior3BnnTl06IyPdc2Zpfa21MSG2KclBJO01GQ2ajIXVJaW4HLKvMHiY4PJm80l4+pLvLhLHOzKUZzADK7PX2z9lYjILSLSKSJHReRzKfZ7v4ioiGxJ2PbHseM6ReQd6Z4zX/nTd23gQ9euXOxl5CU3rW9iNDDN/tPRjJPJUJjtL/fy9stbbN0E2rPsQqqqHOz1sWFZ+vEBiPZOgjRdQxkUk+USEcFb7k4ZI4hElOND43NWFM/G43Ky5RJvzhrQ+QLTuJyCx2WeP/ONeT8REXECDwK3AhuAu0RkQ5L9qoD7gBcStm0A7gQuB24B/klEnHbPaShM3tTRgNMhcffQfx8aYGRimjuutlc9216fXQrpgH+Ss+NTXJahEFSVllDudtI3ar+6OJOq4lzjLXendA31jASYDEVsWwQAN6xt4HCfP2nwP118wWmqPC7jTs1D7EjzNcBRVe1S1SngceD2JPt9EfgykPgYdTvwuKpOqupx4GjsfHbPaShAaspcbL7EG08jfeqlbpqqSrmxw15h16qGcvp8wbSydhI5GA8UZyYEIhIdUOO3ZxGoWiMq068qziXeChfDKVpRdw1dOJ5yPqx2E891Ze8e8gVCpoYgT7EjBK3A6YTX3bFtcUTkKmCFqv7A5rHzntNQ2Ny0vokDvT4O9vrY2TnI+65qxTlH7cBs2rPsOXSwNyoEl2YoBABN1aX026wl8AVCBKcjeWERpGpFbQ2sX5OGRXBlaw1VpSU5iROYzqP5ix0hSPbXG48CiogD+ArwB2kcm/Kc551A5B4R2SMiewYH7fe7NywuVortHz75MqGIXtBpNBXZdiE9dMZHm7eMmixuOi3VHttZQ4tdTGbhrUjdirprcJwqTwkNlcmbzSWjxOng2tV1PJeDeoLoLAIjBPmIHSHoBhLTY9qA3oTXVcBGYKeInACuA7bHAsZzHTvfOeOo6kOqukVVtzQ2ptczxrB4rG+uYlmNhwO9Pq5sq0mr8M6yCI5nahGcyTxQbNFc42HAN2lrdm+6IyoXCm+sA+lca7bGU6bro3/jmgZOnJ2Iz1bOFH8wZIrJ8hQ7QrAb6BCRVSLiJhr83W69qaqjqtqgqu2q2g48D9ymqnti+90pIqUisgroAF6c75yGwkdE4sNg7khRSZyMytISGqtKM7IIJqZCHB8azzhQbNFc5WEqHLHV2jluEVQtthC4CUc0XsE7m67BcdbYzBhK5Ia1sbbUWWYPmcH1+cu8QqCqIeBe4FngEPCEqh4QkQdE5LZ5jj0APAEcBH4EfEZVw3OdM7tLMeQb79/cxpVtNdy+aXnax67KMHOos8+PKhm1lkjEerq303PIiiU0LXKwOF5dnCSFdHwyxJnRYEYFduuaqqivcGc9n8AXCJkYQZ5iy05T1aeBp2dt+8Ic+26b9fpLwJfsnNNQXGy+xMv2e9+U0bHtDeXs6Ew/JpRtxpCFNVeg3x9kA6nP1e8P4i134XHNX627kMQbz01M0c75N/zj8Yyh9NumOxzCG9fUs+voEKqaUfrnVChCYDpMValxDeUjprLDkJe0N1Qw6J9kbDK5m2MuDp3xUVVaQps3ux5Q8X5DNiyCvtHJRQ8UQzRYDCQNGB/LIGMokRvWNjDgn+TYYGZxm3ifIWMR5CVGCAx5yaoMM4cO9vq4bFl11kVLTVVWdfH8hVQD/sUvJoNosBjgXJJagq7BcUTgEhvN5pJxfXx8ZWZxAtN5NL8xQmDISzKpJYhElMN9/qzjAwDuEgf1FW5bKaR9o+kPrV8IUlkEXUPjtHnLMnZfrawrp6GylFe6Mxtob2YR5DdGCAx5SSa1BCfPTTAxFc46PmDRXD3/yMpQOMLQ2GRGs4pzTVVpCSUOSTqc5tjA2LzDaFIhIqysK6NnOLMUUjOmMr8xQmDIS8rcTlqqPRxPI3PIqijONnXUosXG7OKhsSkiungjKhMREWqT9BuKN5vLIGMokTZvOd0jmfWAMp1H8xsjBIa8pb2hPC3X0KEzPpwOoaM58yffRJqr559dnC81BBZ1SfoN9fmCBKbDGQeKLdq8ZZwZCZ7XXtwucdeQiRHkJUYIDHlLdJC9fSE4eMbHmsaKnKVxNld7GBqbYioUmXOf+GSyPLAIAGqT9BuKzynO0iJo9ZYRimha7bktLIugylgEeYkRAkPe0l5fwdnxqfhNZD4O5aC1RCJWAHgwRQvmAX9+9BmyqCu/sN/QzJzibC2CaMZRdwZxAl8ghEOgwr24tRaG5BghMOQt6YytHB6f4sxoMGfxAZi5uaeqLu4bDVLiEOor7DdyW0i8Fa4L0ke7BsepcDtpqsouoG3VZvRkECewOo+aWQT5iRECQ96SziD7+LD6HKSOWtgZYt/nC9JUVYrDZovthcYbswgSG89Z4ymzvQm31kaFoPtcJhaB6TOUzxghMOQtK+vKEcFWzyGrtUQuLQLL759KCAZ8k3mRMWRRV+EmFFH8CRXZXYPZZwxBdHRlQ2VpRq4h03k0vzFCYMhbPC4ny2vKbGUOHez10VRVSkNl7vL5veUu3E5HyqKyPl8wbzKGIBoshpnGc4GpMD0jgazjAxZt3rKMUkhN59H8xgiBIa9pbyi35Ro6eMaXU7cQRPPy55tU1j8azJuMIYimjwLx9tnHMxhPmYo2b2ZFZdExlUYI8hUjBIa8pr2+Yl6LYDIU5ujAWE7dQhbR6uLkWUPjkyH8k6G8yRiCCy0CK2Mom6riRFq9ZfSMBIikWUsQHVxvXEP5ihECQ16zqqGCkYnplCMYjw6MEYpoTlNHLVpStJnoj4+oXPz2EhZ1lhDE/r+sGoJVGQykSUabt5zpsDLgn78ZXyK+gJlXnM8YITDkNVbPoVTuoVy3lkgkVb8hy1LIh4ZzFlbjOavf0LHBMVpryyjLUf5+JimkoXCE8amwcQ3lMUYIDHmNnS6kB8/48LgcOXvqTaS5upTxqXC8n34ilkA05ZEQVHtKcDrkPIsgV/EBgDYrhTSNOIHftKDOe4wQGPKalXXlOISUzecOnfFxaUs1zgXI5U+VQtqXJ0PrExGR84bYdw2O5SxjCKIxAshQCIxFkLcYITDkNe4SB63esjmri1U1PoxmIZgpKrvQJ97vC1JZWkJlno1frC13Mzw+xYB/kvGpcE4tgnJ3CfUV7rSEwGemk+U9toRARG4RkU4ROSoin0vy/qdE5FUR2S8ivxSRDbHtd8e2WV8REdkUe29n7JzWe025vTRDsZAqc6h3NIgvGMp56qhFqjYT/b7gog+sT0ZdrBW1NZ4yVxlDFm3eMrqH7ccIZobS5JdgGmaYVwhExAk8CNwKbADusm70CTymqleo6ibgy8DfAajqt1V1U2z7h4ETqro/4bi7rfdVdSAXF2QoPlY1VHB8aPy8tgkWVqB4w7KqBfnZViA4WVFZvkwmm4031oo6V11HZ9OaZi2B6Tya/9ixCK4Bjqpql6pOAY8DtyfuoKq+hJcVQLIk47uA72S6UMPSpb2+An8wlHTy1sFeHyKwvmVhLIIyt5NqTwkDSYSg3zeZn0IQa0V9bHCMMpcz52uMDqixX0vgC5hgcb5jRwhagdMJr7tj285DRD4jIseIWgT3JTnPB7lQCB6JuYU+L6YtoWEOVqXIHDp0xscldeUL6qdvrvZcYBFEIsqAP5hXGUMW3opo4zkrYyjXDfHavGVMhSIMjdurJTAxgvzHjhAk+y264FFAVR9U1TXAHwF/dt4JRK4FJlT1tYTNd6vqFcCNsa8PJ/3hIveIyB4R2TM4OGhjuYZioz3ehfRCv/RCtJaYTUuNh75ZweJzE1NMh5WWPIwReMtdTIeVV3tGWZ3DjCGL1jRTSH2BaUSg0m0sgnzFjhB0AysSXrcBvSn2fxx476xtdzLLGlDVnti/fuAxoi6oC1DVh1R1i6puaWxstLFcQ7HR5i3D6ZALMof8wWlOnZtYkIriRJqqPBe4hvrzMHXUwls+U1S2egFqK9IdUOMLhqgqLcmbVt2GC7EjBLuBDhFZJSJuojf17Yk7iEhHwst3AUcS3nMAHyAqENa2EhFpiH3vAt4NJFoLBkMcl9PBCm8Zx2e5hg73+YGFqShOpKWmlAH/5HmzevOxmMzCEgLIfaAYZmoJ7AaMraE0hvxlXltNVUMici/wLOAEHlbVAyLyALBHVbcD94rI24BpYBj4aMIp3gx0q2pXwrZS4NmYCDiBnwJfz8kVGYqS9iTzixdiGE0yWqo9hCPK2bHJ+I2/bzT/2ktYeBOmpeWymMyisrQEb7nLdgqp6Tya/9hy2qnq08DTs7Z9IeH7+1McuxO4bta2cWBzOgs1LG3a6yvYffwcqhqftHWw10dtuWvBb8ZNCUVlTQlTy0SgMcvxjwtBXYIQLETbDYhaBfZdQ6bzaL5jKosNBcGqhgrGp8LnDZI/GBtWv9AJZ8lqCfp9QeorSnE58+9PyFseffpeVuOhYoGyqdpqy9OwCIxrKN/Jv99igyEJM4PsozefUDhCZ59/weMDkLzfUL8vSEtN/lkDEO3p45CFiQ9YtMXmEiQr8puNP2hcQ/mOEQJDQbCq3hKCaJzg+NA4k6HIgmcMAdRXuHHI+ULQ55vMqxGViTgcQnt9BW9oq12wn9HqLSM4HeFskiK/2UQtAuMaymfMp2MoCJbXenA5JZ45tBDD6ueixOmgsar0vH5D/b4gV61cuBtttvzHvTfgKcnNDIJkJKaQppoTHYkoY1PGIsh3jEVgKAhKnA5W1JXHLYKDZ3y4nMLaptxnxSSjpdpDf2wq12QozLnxqbzMGLKo9rhwlyzcn3ebzRRS/2QIVVNVnO8YITAUDKvqK+KTyg72+uhoqlrQm10iTdWe+BD7gViVcT6NqLzYzMwlSB0wNp1HCwMjBIaCob2hgpNnJ1BVDp25OIFii5aEfkMzs4rz1yJYaKo9Lqo9JfOmkJrOo4WBEQJDwdDeUEFgOsyBXh9DY5MLXkiWSEuNh9HANMHp8Mys4jxsL3ExafPOn0JqOo8WBkYIDAWDlTn0w1fPAHDZAs0gSEZTrHCs3xeMWwb5mjV0sbBSSFMR7zxqLIK8xgiBoWBob4hmqjwTE4KLkTpqYT39940G6fcFcZc4qC1f2jc3q7o4VS2BFSOoMcHivMYIgaFgWF5ThrvEwYmzEyyv8VCb0FxtoYnPLvZPRovJqj0LXtGc77R5y5mYCjM8MT3nPmZwfWFghMBQMDgcwiV1UavgYsYHIEEIRoP0jQaXdMaQhZ0UUss1VGmyhvIaIwSGgsJqNXEx3UIQTX8scznp80VdQ0s5Y8hiZkDN3AFjXyA6i8BpZhHkNUYIDAWF1U3zYqaOAogIzdWlMSGYNEIArLAxoMZ0Hi0MjBAYCopLW6pwCGxsrbnoP7u52sOxgTEC0+G8riq+WFSXlVBVWpIyc8h0Hi0MjFQbCorbN7VyRWsNK2KxgotJS42H3SfOAdC8xGsIIGolRTOHUriGgtMmUFwAGIvAUFA4HUJH88WrH0ikudqDNa2yOQ8H0iwGbfMMqPEFQqaYrAAwQmAw2CQxLrDUq4ototXFc9cS+CeNRVAIGCEwGGySGBcwweIobd4yxiZD8VYSs4laBEYI8h0jBAaDTazagZoyFx7XwvX6LySsFNLTSeIEkYjiD06bzqMFgC0hEJFbRKRTRI6KyOeSvP8pEXlVRPaLyC9FZENse7uIBGLb94vI/004ZnPsmKMi8lVZ6mWahrzHsgJMxtAMbSlSSMenQkTUdB4tBOYVAhFxAg8CtwIbgLusG30Cj6nqFaq6Cfgy8HcJ7x1T1U2xr08lbP8acA/QEfu6JYvrMBgWnKaYRWAyhmaIVxcnSSH1BU3n0ULBjkVwDXBUVbtUdQp4HLg9cQdV9SW8rABSTrQWkWVAtao+p9Eo07eA96a1coPhIlNa4qSl2sOK2M3PALXlLirczqQppDNDaYxFkO/YkepW4HTC627g2tk7ichngM8CbuCtCW+tEpF9gA/4M1X9Reyc3bPO2Zre0g2Gi8+/fvIavBex2V2+M1NLkMQisITABIvzHjsWQTLf/QVP/Kr6oKquAf4I+LPY5jPASlW9iqhIPCYi1XbPCSAi94jIHhHZMzg4aGO5BsPCsbapivoUw9qXIlYK6WxM59HCwY4QdAMrEl63Ab0p9n+cmJtHVSdV9Wzs+73AMWBd7Jxtds6pqg+p6hZV3dLY2GhjuQaD4WLS5i2jJ5lryBpKY2IEeY8dIdgNdIjIKhFxA3cC2xN3EJGOhJfvAo7EtjfGgs2IyGqiQeEuVT0D+EXkuli20EeA72d9NQaD4aLTWluGLxhiNHD+XAITIygc5pVqVQ2JyL3As4ATeFhVD4jIA8AeVd0O3CsibwOmgWHgo7HD3ww8ICIhIAx8SlXPxd77NPAoUAY8E/syGAwFhpVC2jMcOG8SmZU1ZGYR5D+2PiFVfRp4eta2LyR8f/8cxz0FPDXHe3uAjbZXajAY8pLEFNLEgUG+wDTlbicup6lbzXfMJ2QwGLLCEoLZKaSm82jhYITAYDBkRV2FG4/LcUHmkOk8WjgYITAYDFkhIrEU0vMtAtN5tHAwQmAwGLKmzVt2QZsJ03m0cDBCYDAYsqa19sLqYp/pPFowGCEwGAxZ0+YtZ2RimrHJmbkEZl5x4WCEwGAwZE08hTRmFagqvmCIKmMRFARGCAwGQ9bMTiGdmAoTjqgJFhcIRggMBkPWtMaFIGoRzPQZMkJQCBghMBgMWdNYWUppiSOeOWQ6jxYWRggMBkPWzMwliLqGZmYRmBhBIWCEwGAw5ITEFNK4a8hYBAWBEQKDwZATEgfU+ALWvGIjBIWAEQKDwZAT2rxlnBufYmIqFLcITPpoYWCEwGAw5ITEWgIrRmCEoDAwQmAwGHJCW0IKqS8YwuNyUFriXORVGexghMBgMOQEa1JZ90gAv5lFUFAYITAYDDmhsbIUt9NB9/CE6TxaYBghMBgMOcHhEJbXemKuIdN5tJCwJQQicouIdIrIURH5XJL3PyUir4rIfhH5pYhsiG2/WUT2xt7bKyJvTThmZ+yc+2NfTbm7LIPBsBhYKaSm82hhMa8QiIgTeBC4FdgA3GXd6BN4TFWvUNVNwJeBv4ttHwLeo6pXAB8F/mXWcXer6qbY10A2F2IwGBafNm9ZNGsoGKLKxAgKBju22zXAUVXtAhCRx4HbgYPWDqrqS9i/AtDY9n0J2w8AHhEpVdXJbBduMBjyjzZvGUNjkwSnw8Y1VEDY+aRagdMJr7uBa2fvJCKfAT4L2EagiwAABelJREFUuIG3zn4fuAPYN0sEHhGRMPAU8FeqqnYXbjAY8g+rC+nYpAkWFxJ2YgSSZNsFN2xVfVBV1wB/BPzZeScQuRz4a+C3EzbfHXMZ3Rj7+nDSHy5yj4jsEZE9g4ODNpZrMBgWCyuFFEyfoULCjhB0AysSXrcBvSn2fxx4r/VCRNqA7wEfUdVj1nZV7Yn96wceI+qCugBVfUhVt6jqlsbGRhvLNRgMi4VVVAam82ghYUcIdgMdIrJKRNzAncD2xB1EpCPh5buAI7HttcAPgT9W1V0J+5eISEPsexfwbuC1bC7EYDAsPk1VHkocUSeCsQgKh3klW1VDInIv8CzgBB5W1QMi8gCwR1W3A/eKyNuAaWCYaIYQwL3AWuDzIvL52La3A+PAszERcAI/Bb6ew+syGAyLgNMhLK8t49S5CRMjKCBs2W6q+jTw9KxtX0j4/v45jvsr4K/mOO1mm2s0GAwFRJs3KgSm4VzhYCqLDQZDTrHiBMY1VDgYITAYDDmltTaaOWTqCAoH80kZDIac8r6rWlGUxqrSxV6KwSZGCAwGQ05ZWV/O771t3WIvw5AGxjVkMBgMSxwjBAaDwbDEMUJgMBgMSxwjBAaDwbDEMUJgMBgMSxwjBAaDwbDEMUJgMBgMSxwjBAaDwbDEkUIaCiYig8DJDA9vIDpDuVgotuuB4rumYrseKL5rKrbrgeTXdImqzjnQpaCEIBtEZI+qblnsdeSKYrseKL5rKrbrgeK7pmK7HsjsmoxryGAwGJY4RggMBoNhibOUhOChxV5Ajim264Hiu6Ziux4ovmsqtuuBDK5pycQIDAaDwZCcpWQRGAwGgyEJRS8EInKLiHSKyFER+dxirycXiMgJEXlVRPaLyJ7FXk8miMjDIjIgIq8lbKsTkZ+IyJHYv97FXGM6zHE9fyEiPbHPab+IvHMx15gOIrJCRHaIyCEROSAi98e2F/JnNNc1FeTnJCIeEXlRRF6OXc9fxravEpEXYp/Rv4mIe95zFbNrSEScwOvAzUA3sBu4S1UPLurCskRETgBbVLVg859F5M3AGPAtVd0Y2/Zl4Jyq/u+YaHtV9Y8Wc512meN6/gIYU9W/Xcy1ZYKILAOWqepLIlIF7AXeC3yMwv2M5rqm36AAPycREaBCVcdExAX8Ergf+CzwXVV9XET+L/Cyqn4t1bmK3SK4Bjiqql2qOgU8Dty+yGsyAKr6c+DcrM23A9+Mff9Non+kBcEc11OwqOoZVX0p9r0fOAS0Utif0VzXVJBolLHYS1fsS4G3Ak/Gttv6jIpdCFqB0wmvuyngDz4BBX4sIntF5J7FXkwOaVbVMxD9owWaFnk9ueBeEXkl5joqGDdKIiLSDlwFvECRfEazrgkK9HMSEaeI7AcGgJ8Ax4ARVQ3FdrF1zyt2IZAk24rBF3aDql4N3Ap8JuaWMOQfXwPWAJuAM8D/u7jLSR8RqQSeAn5PVX2LvZ5ckOSaCvZzUtWwqm4C2oh6QC5Lttt85yl2IegGViS8bgN6F2ktOUNVe2P/DgDfI/oLUAz0x/y4lj93YJHXkxWq2h/7Q40AX6fAPqeY3/kp4Nuq+t3Y5oL+jJJdU6F/TgCqOgLsBK4DakWkJPaWrXtesQvBbqAjFkV3A3cC2xd5TVkhIhWxQBciUgG8HXgt9VEFw3bgo7HvPwp8fxHXkjXWDTPG+yigzykWiPz/gEOq+ncJbxXsZzTXNRXq5yQijSJSG/u+DHgb0bjHDuD9sd1sfUZFnTUEEEsF+z+AE3hYVb+0yEvKChFZTdQKACgBHivEaxKR7wDbiHZK7Af+HPgP4AlgJXAK+ICqFkQAdo7r2UbU3aDACeC3Lf96viMibwJ+AbwKRGKb/4SoT71QP6O5rukuCvBzEpEriQaDnUQf6p9Q1Qdi94jHgTpgH/CbqjqZ8lzFLgQGg8FgSE2xu4YMBoPBMA9GCAwGg2GJY4TAYDAYljhGCAwGg2GJY4TAYDAYljhGCAwGg2GJY4TAYDAYljhGCAwGg2GJ8/8DpEky/j8S6qYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "acc_coef8 = np.load(r\"./stratified_cross_validation_results/effnets/onehot_acc_coef_0-9.npy\", allow_pickle=True)[:9]\n",
    "raw_outputs8 = np.load(r\"./stratified_cross_validation_results/effnets/onehot_raw_outputs_0-9.npy\", allow_pickle=True)[:9]\n",
    "acc_coef12 = np.load(r\"./stratified_cross_validation_results/effnets/onehot_acc_coef_9-20.npy\", allow_pickle=True)\n",
    "raw_outputs12 = np.load(r\"./stratified_cross_validation_results/effnets/onehot_raw_outputs_9-20.npy\", allow_pickle=True)\n",
    "acc_coef20 = np.load(r\"./stratified_cross_validation_results/effnets/onehot_acc_coef_20-30.npy\", allow_pickle=True)\n",
    "raw_outputs20 = np.load(r\"./stratified_cross_validation_results/effnets/onehot_raw_outputs_20-30.npy\", allow_pickle=True)\n",
    "acc_coef = np.vstack(np.array([acc_coef8,acc_coef12,acc_coef20]))\n",
    "raw_outputs = np.vstack(np.array([raw_outputs8,raw_outputs12,raw_outputs20]))\n",
    "print(len(acc_coef))\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def show_confusion_matrix(raw_outputs, fine_tune_layers):\n",
    "\n",
    "    y_true = np.argmax(np.vstack(raw_outputs[fine_tune_layers, :, 1]), axis=1)\n",
    "    y_pred = np.argmax(\n",
    "        np.rint(np.vstack(raw_outputs[fine_tune_layers, :, 2])), axis=1\n",
    "    ).astype(int)\n",
    "    return confusion_matrix(y_true, y_pred)\n",
    "\n",
    "def show_matrix_percentage(confusion_matrix):\n",
    "    return np.transpose(np.transpose(my_confusion_matrix) / np.sum(my_confusion_matrix, axis=1))\n",
    "\n",
    "# total accuracy\n",
    "def calculate_accuracy(my_confusion_matrix):\n",
    "    return np.trace(my_confusion_matrix)/np.sum(my_confusion_matrix)\n",
    "\n",
    "max_acc_layer = np.argmax([calculate_accuracy(show_confusion_matrix(raw_outputs, i))  for i in range(len(acc_coef))])\n",
    "\n",
    "my_confusion_matrix = show_confusion_matrix(raw_outputs,max_acc_layer)\n",
    "print(my_confusion_matrix)\n",
    "print(\"+++++++++++++++++++++++++++++++++\")\n",
    "print(\"+++++++++++++++++++++++++++++++++\")\n",
    "print(show_matrix_percentage(my_confusion_matrix))\n",
    "print(\"+++++++++++++++++++++++++++++++++\")\n",
    "print(\"+++++++++++++++++++++++++++++++++\")\n",
    "print(calculate_accuracy(my_confusion_matrix)*100)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot([i for i in range(len(acc_coef))],[calculate_accuracy(show_confusion_matrix(raw_outputs, i))  for i in range(len(acc_coef))])\n",
    "print(\"+++++++++++++++++++++++++++++++++\")\n",
    "print(\"+++++++++++++++++++++++++++++++++\")\n",
    "trainable_sequence = np.array([227, 225, 217, 214, 210, 202, 199, 195, 187, 184, 180, 172, 169,\n",
    "       167, 159, 156, 152, 144, 141, 137, 129, 126, 124, 116, 113, 109,\n",
    "       101,  98,  94,  86,  83,  81,  73,  70,  66,  58,  55,  53,  45,\n",
    "        42,  38,  30,  27,  25,  17,  14,  12,   4,   1])\n",
    "print(f\"max accuracy with tuning from {trainable_sequence[max_acc_layer]} layers, or tune {233-trainable_sequence[max_acc_layer]} layers\")\n",
    "print(max_acc_layer, [show_matrix_percentage(my_confusion_matrix)[i,i]*100 for i in range(5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56, 4)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(cvscores)[:, 1][:, 0][4].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.54385966, 0.7954881615019466],\n",
       "       [0.5964912, 0.851205445137053],\n",
       "       [0.4107143, 0.7907964869050351],\n",
       "       [0.54545456, 0.8426871818521864],\n",
       "       [0.66071427, 0.8581865352844125]], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(cvscores)[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5,  3,  1,  0,  0],\n",
       "       [ 0,  9,  3,  0,  0],\n",
       "       [ 1,  4, 11,  0,  0],\n",
       "       [ 0,  0,  3,  5,  3],\n",
       "       [ 0,  0,  1,  3,  5]], dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(\n",
    "    y_true=K.sum(K.cast(K.round(cvscores[1][1][0]), \"int32\"), axis=1).numpy(),\n",
    "    y_pred=K.sum(K.cast(K.round(cvscores[1][1][1]), \"int32\"), axis=1).numpy(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "mycsv = pd.DataFrame(\n",
    "    np.hstack(\n",
    "        np.array(\n",
    "            [\n",
    "                np.vstack(np.array(cvscores)[:, 1][:, 0]),\n",
    "                np.vstack(np.array(cvscores)[:, 1][:, 1]),\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(mycsv[range(4, 8)].to_numpy(), np.vstack(np.array(cvscores)[:, 1][:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "mycsv.to_csv(\n",
    "    \"./stratified_cross_validation_results/effnet_multinomial.csv\", index=False\n",
    ")\n",
    "# next time include which image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "mycsv = pd.read_csv(\"./stratified_cross_validation_results/effnet_multinomial.csv\")\n",
    "y_true = np.sum((mycsv[[str(i) for i in range(0, 4)]]).to_numpy(dtype=int), axis=1)\n",
    "y_pred = np.sum(\n",
    "    np.rint((mycsv[[str(i) for i in range(4, 8)]]).to_numpy()), axis=1\n",
    ").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[33, 10,  1,  0,  0],\n",
       "       [ 6, 36, 14,  1,  0],\n",
       "       [ 7, 22, 35,  5,  4],\n",
       "       [ 0,  2, 20, 17,  9],\n",
       "       [ 0,  0,  7, 17, 35]], dtype=int64)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "my_confusion_matrix = confusion_matrix(y_true, y_pred,)\n",
    "my_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.75      , 0.22727273, 0.02272727, 0.        , 0.        ],\n",
       "       [0.10526316, 0.63157895, 0.24561404, 0.01754386, 0.        ],\n",
       "       [0.09589041, 0.30136986, 0.47945205, 0.06849315, 0.05479452],\n",
       "       [0.        , 0.04166667, 0.41666667, 0.35416667, 0.1875    ],\n",
       "       [0.        , 0.        , 0.11864407, 0.28813559, 0.59322034]])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.transpose(np.transpose(my_confusion_matrix) / np.sum(my_confusion_matrix, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.82710261],\n",
       "       [0.82710261, 1.        ]])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# coef\n",
    "np.corrcoef(y_true, np.sum((mycsv[[str(i) for i in range(4, 8)]]).to_numpy(), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5551601423487544"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# acc\n",
    "sum(np.isclose(y_true, y_pred)) / len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.utils import plot_model\n",
    "\n",
    "# plot_model(model, to_file=\"effnet.png\", show_shapes=True)\n",
    "# from IPython.display import Image\n",
    "\n",
    "# Image(filename=\"effnet.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 281 validated image filenames.\n",
      "['loss', 'soft_acc_multi_output']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "[0.21299249678850174, 0.74404764]\n"
     ]
    }
   ],
   "source": [
    "response = response.sample(frac=1.0)\n",
    "\n",
    "test_set = valid_gen.flow_from_dataframe(\n",
    "    dataframe=response,\n",
    "    directory=DATADIR,\n",
    "    x_col=\"GreenID\",\n",
    "    target_size=(224, 224),\n",
    "    color_mode=\"rgb\",\n",
    "    subset=\"validation\",\n",
    "    shuffle=False,\n",
    "    batch_size=56,\n",
    "    y_col=[0, 1, 2, 3,],\n",
    "    class_mode=\"raw\",\n",
    "    #     seed = seed\n",
    ")\n",
    "\n",
    "batch = next(test_set)\n",
    "true_labels = batch[1]\n",
    "predictions = model.predict(batch[0])\n",
    "\n",
    "print(model.metrics_names)\n",
    "print(model.evaluate(test_set, verbose=0))  # loss/accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.85824516],\n",
       "       [0.85824516, 1.        ]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(np.sum(predictions, axis=1), np.sum(true_labels, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=80)\n",
    "batch = next(test_set)\n",
    "\n",
    "y_true = batch[1]\n",
    "y_pred = model.predict(batch[0])\n",
    "print(soft_acc_multi_output(y_true, y_pred))\n",
    "\n",
    "# print examples from the validation set\n",
    "for i in range(len(batch[1])):\n",
    "    img = batch[0][i]\n",
    "    label = batch[1][i]\n",
    "    assert (label == y_true[i]).all()\n",
    "    right = K.all(\n",
    "        K.equal(K.cast(K.round(label), \"int32\"), K.cast(K.round(y_pred[i]), \"int32\"),)\n",
    "    )\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    print(f\"true label: {label}; rounded pred: {y_pred[i]}; Correct: {right}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ds = tf.data.Dataset.list_files(\n",
    "    str(\"C:/Users/feroc/OneDrive - The University of Melbourne/Dataset/adult/*\")\n",
    ")\n",
    "\n",
    "\n",
    "def decode_img(img):\n",
    "    # convert the compressed string to a 3D uint8 tensor\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    # resize the image to the desired size.\n",
    "    return tf.image.resize(img, [IMG_WIDTH, IMG_HEIGHT])\n",
    "\n",
    "\n",
    "def get_label(file_path):\n",
    "    # convert the path to a list of path components\n",
    "    image_id = tf.strings.split(file_path, os.path.sep)[-1]\n",
    "    return response.loc[] \n",
    "\n",
    "list(list_ds.take(1).as_numpy_iterator())[0]\n",
    "tf.strings.split(list(list_ds.take(1).as_numpy_iterator())[0],os.path.sep)[-1].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
