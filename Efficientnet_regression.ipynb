{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "IMG_WIDTH, IMG_HEIGHT = 224, 224\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.applications.densenet import (\n",
    "    DenseNet121,\n",
    "    preprocess_input,\n",
    ")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import IPython.display as display\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense,\n",
    "    GlobalAveragePooling2D,\n",
    "    Conv2D,\n",
    "    Flatten,\n",
    "    GlobalMaxPooling2D,\n",
    "    Dropout,\n",
    ")\n",
    "\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam, Nadam\n",
    "from datetime import datetime\n",
    "from packaging import version\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import (\n",
    "    TensorBoard,\n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint,\n",
    "    ReduceLROnPlateau,\n",
    ")\n",
    "import efficientnet.tfkeras as enet\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def append_extension(fn):\n",
    "    return (fn + \".jpg\").zfill(7)\n",
    "\n",
    "\n",
    "def ordered_logit(class_number):\n",
    "    # zero portability\n",
    "    target = np.zeros(4, dtype=int)\n",
    "    target[: class_number - 2] = 1\n",
    "    return target\n",
    "\n",
    "\n",
    "DATADIR = r\"./adult\"\n",
    "CSV_PATH = r\"./adult/CastControls_ALP.xlsx\"\n",
    "response = pd.read_excel(CSV_PATH, sheet_name=0,)[[\"GreenID\", \"Grade\"]].dropna(\n",
    "    axis=0, subset=[\"Grade\"]\n",
    ")\n",
    "response.Grade = response.Grade.astype(\"int\")\n",
    "response.GreenID = response.GreenID.astype(\"str\").apply(append_extension)\n",
    "response = response[response.Grade != 99]\n",
    "response = pd.concat(\n",
    "    [response, pd.DataFrame.from_dict(dict(response.Grade.apply(ordered_logit))).T,],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "\n",
    "# shuffle dataset\n",
    "response = response.sample(frac=1)\n",
    "seed = np.random.randint(30027)\n",
    "\n",
    "\n",
    "def soft_acc(y_true, y_pred):\n",
    "    return K.mean(K.equal(K.round(y_true), K.round(y_pred)))\n",
    "\n",
    "\n",
    "def soft_acc_multi_output(y_true, y_pred):\n",
    "    return K.mean(\n",
    "        K.all(\n",
    "            K.equal(\n",
    "                K.cast(K.round(y_true), \"int32\"), K.cast(K.round(y_pred), \"int32\"),\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "# from tensorflow.keras import mixed_precision\n",
    "\n",
    "# policy = tf.keras.mixed_precision.experimental.Policy(\"mixed_float16\")\n",
    "# mixed_precision.experimental.set_policy(policy)\n",
    "\n",
    "# gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "# if gpus:\n",
    "#     try:\n",
    "#         # Currently, memory growth needs to be the same across GPUs\n",
    "#         for gpu in gpus:\n",
    "#             tf.config.experimental.set_memory_growth(gpu, True)\n",
    "#         logical_gpus = tf.config.experimental.list_logical_devices(\"GPU\")\n",
    "#         print(\n",
    "#             len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\",\n",
    "#         )\n",
    "#     except RuntimeError as e:\n",
    "#         # Memory growth must be set before GPUs have been initialized\n",
    "#         print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "train_gen = ImageDataGenerator(\n",
    "    rotation_range=5,\n",
    "    fill_mode=\"reflect\",\n",
    "    horizontal_flip=True,\n",
    "    #     vertical_flip=True,\n",
    "    validation_split=0.1,\n",
    "    # dude i wasnt cheating...\n",
    "    rescale=1.0 / 255.0,\n",
    "    #     preprocessing_function = preprocess_input\n",
    "    zoom_range=0.1,\n",
    ")\n",
    "\n",
    "valid_gen = ImageDataGenerator(validation_split=0.1, rescale=1.0 / 255.0,)\n",
    "\n",
    "train_set = train_gen.flow_from_dataframe(\n",
    "    dataframe=response,\n",
    "    directory=DATADIR,\n",
    "    x_col=\"GreenID\",\n",
    "    target_size=(224, 224),\n",
    "    color_mode=\"rgb\",\n",
    "    subset=\"training\",\n",
    "    shuffle=True,\n",
    "    #     class_mode = \"sparse\"\n",
    "    #     y_col=\"Grade\",\n",
    "    y_col=[0, 1, 2, 3,],\n",
    "    class_mode=\"raw\",\n",
    ")\n",
    "\n",
    "validation_set = valid_gen.flow_from_dataframe(\n",
    "    dataframe=response,\n",
    "    directory=DATADIR,\n",
    "    x_col=\"GreenID\",\n",
    "    target_size=(224, 224),\n",
    "    color_mode=\"rgb\",\n",
    "    subset=\"validation\",\n",
    "    shuffle=True,\n",
    "    batch_size=28,\n",
    "    #     class_mode = \"sparse\"\n",
    "    #     y_col=\"Grade\",\n",
    "    y_col=[0, 1, 2, 3,],\n",
    "    class_mode=\"raw\",\n",
    ")\n",
    "\n",
    "train_set.reset()\n",
    "validation_set.reset()\n",
    "# print(next(validation_set)[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-4e887602e298>:25: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 16 steps, validate for 2 steps\n",
      "Epoch 1/25\n",
      "16/16 [==============================] - 11s 669ms/step - loss: 0.6069 - soft_acc_multi_output: 0.2124 - val_loss: 0.4696 - val_soft_acc_multi_output: 0.3571\n",
      "Epoch 2/25\n",
      "16/16 [==============================] - 5s 305ms/step - loss: 0.4838 - soft_acc_multi_output: 0.3115 - val_loss: 0.3935 - val_soft_acc_multi_output: 0.4286\n",
      "Epoch 3/25\n",
      "16/16 [==============================] - 5s 307ms/step - loss: 0.4403 - soft_acc_multi_output: 0.3268 - val_loss: 0.3645 - val_soft_acc_multi_output: 0.4643\n",
      "Epoch 4/25\n",
      "16/16 [==============================] - 5s 305ms/step - loss: 0.4071 - soft_acc_multi_output: 0.3978 - val_loss: 0.3524 - val_soft_acc_multi_output: 0.4821\n",
      "Epoch 5/25\n",
      "16/16 [==============================] - 5s 304ms/step - loss: 0.4045 - soft_acc_multi_output: 0.3818 - val_loss: 0.3363 - val_soft_acc_multi_output: 0.4821\n",
      "Epoch 6/25\n",
      "16/16 [==============================] - 5s 315ms/step - loss: 0.3879 - soft_acc_multi_output: 0.3942 - val_loss: 0.3301 - val_soft_acc_multi_output: 0.5357\n",
      "Epoch 7/25\n",
      "16/16 [==============================] - 5s 313ms/step - loss: 0.3993 - soft_acc_multi_output: 0.4017 - val_loss: 0.3252 - val_soft_acc_multi_output: 0.5179\n",
      "Epoch 8/25\n",
      "16/16 [==============================] - 5s 309ms/step - loss: 0.3769 - soft_acc_multi_output: 0.4029 - val_loss: 0.3267 - val_soft_acc_multi_output: 0.5000\n",
      "Epoch 9/25\n",
      "16/16 [==============================] - 5s 315ms/step - loss: 0.3710 - soft_acc_multi_output: 0.4532 - val_loss: 0.3134 - val_soft_acc_multi_output: 0.5000\n",
      "Epoch 10/25\n",
      "16/16 [==============================] - 5s 308ms/step - loss: 0.3648 - soft_acc_multi_output: 0.4408 - val_loss: 0.3185 - val_soft_acc_multi_output: 0.5000\n",
      "Epoch 11/25\n",
      "16/16 [==============================] - 5s 316ms/step - loss: 0.3566 - soft_acc_multi_output: 0.4344 - val_loss: 0.3059 - val_soft_acc_multi_output: 0.5000\n",
      "Epoch 12/25\n",
      "16/16 [==============================] - 5s 308ms/step - loss: 0.3598 - soft_acc_multi_output: 0.4314 - val_loss: 0.3156 - val_soft_acc_multi_output: 0.5179\n",
      "Epoch 13/25\n",
      "16/16 [==============================] - 5s 307ms/step - loss: 0.3551 - soft_acc_multi_output: 0.4807 - val_loss: 0.2991 - val_soft_acc_multi_output: 0.4821\n",
      "Epoch 14/25\n",
      "16/16 [==============================] - 5s 308ms/step - loss: 0.3533 - soft_acc_multi_output: 0.4884 - val_loss: 0.3131 - val_soft_acc_multi_output: 0.5179\n",
      "Epoch 15/25\n",
      "16/16 [==============================] - 5s 315ms/step - loss: 0.3763 - soft_acc_multi_output: 0.4321 - val_loss: 0.2990 - val_soft_acc_multi_output: 0.5000\n",
      "Epoch 16/25\n",
      "16/16 [==============================] - 5s 311ms/step - loss: 0.3663 - soft_acc_multi_output: 0.4442 - val_loss: 0.3008 - val_soft_acc_multi_output: 0.5179\n",
      "Epoch 17/25\n",
      "16/16 [==============================] - 5s 308ms/step - loss: 0.3455 - soft_acc_multi_output: 0.4797 - val_loss: 0.2985 - val_soft_acc_multi_output: 0.5179\n",
      "Epoch 18/25\n",
      "16/16 [==============================] - 5s 309ms/step - loss: 0.3512 - soft_acc_multi_output: 0.4610 - val_loss: 0.2978 - val_soft_acc_multi_output: 0.5000\n",
      "Epoch 19/25\n",
      "16/16 [==============================] - 5s 306ms/step - loss: 0.3634 - soft_acc_multi_output: 0.4452 - val_loss: 0.2891 - val_soft_acc_multi_output: 0.5536\n",
      "Epoch 20/25\n",
      "16/16 [==============================] - 5s 305ms/step - loss: 0.3527 - soft_acc_multi_output: 0.4789 - val_loss: 0.3024 - val_soft_acc_multi_output: 0.5357\n",
      "Epoch 21/25\n",
      "16/16 [==============================] - 5s 302ms/step - loss: 0.3424 - soft_acc_multi_output: 0.4962 - val_loss: 0.2897 - val_soft_acc_multi_output: 0.5536\n",
      "Epoch 22/25\n",
      "16/16 [==============================] - 5s 305ms/step - loss: 0.3355 - soft_acc_multi_output: 0.5012 - val_loss: 0.3051 - val_soft_acc_multi_output: 0.5179\n",
      "Epoch 23/25\n",
      "16/16 [==============================] - 5s 305ms/step - loss: 0.3406 - soft_acc_multi_output: 0.5098 - val_loss: 0.2981 - val_soft_acc_multi_output: 0.5179\n",
      "Epoch 24/25\n",
      "16/16 [==============================] - 5s 306ms/step - loss: 0.3374 - soft_acc_multi_output: 0.4695 - val_loss: 0.2924 - val_soft_acc_multi_output: 0.5536\n",
      "Epoch 25/25\n",
      "16/16 [==============================] - 5s 307ms/step - loss: 0.3361 - soft_acc_multi_output: 0.4848 - val_loss: 0.2859 - val_soft_acc_multi_output: 0.5357\n"
     ]
    }
   ],
   "source": [
    "conv_base = enet.EfficientNetB0(\n",
    "    include_top=False, input_shape=(224, 224, 3), pooling=\"avg\", weights=\"imagenet\",\n",
    ")\n",
    "conv_base.trainable = False\n",
    "\n",
    "x = conv_base.output\n",
    "x = Dropout(0.5)(x)\n",
    "preds = Dense(4, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=conv_base.input, outputs=preds)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[soft_acc_multi_output],\n",
    ")\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=21, restore_best_weights=True,\n",
    ")\n",
    "reduce_lr_plateau = ReduceLROnPlateau(monitor=\"val_loss\", patience=7, factor=0.8)\n",
    "\n",
    "history_1 = model.fit_generator(\n",
    "    generator=train_set,\n",
    "    epochs=25,\n",
    "    validation_data=validation_set,\n",
    "    callbacks=[early_stopping, reduce_lr_plateau],\n",
    "    #     verbose=0,\n",
    ")\n",
    "\n",
    "# model.save(\n",
    "#     filepath=\"./saved_models/my_effnet/my_effnet_untuned_1_layer.h5\", save_format=\"h5\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from 227 of 233 layers; 5 trainables variables\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 16 steps, validate for 2 steps\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 11s 679ms/step - loss: 0.5760 - soft_acc_multi_output: 0.3372 - val_loss: 0.3130 - val_soft_acc_multi_output: 0.5536\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 5s 311ms/step - loss: 0.3600 - soft_acc_multi_output: 0.5082 - val_loss: 0.3111 - val_soft_acc_multi_output: 0.5714\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 5s 303ms/step - loss: 0.3116 - soft_acc_multi_output: 0.5399 - val_loss: 0.3271 - val_soft_acc_multi_output: 0.5536\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 5s 312ms/step - loss: 0.2968 - soft_acc_multi_output: 0.5624 - val_loss: 0.3075 - val_soft_acc_multi_output: 0.5714\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 5s 316ms/step - loss: 0.2799 - soft_acc_multi_output: 0.5781 - val_loss: 0.3050 - val_soft_acc_multi_output: 0.5536\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 5s 315ms/step - loss: 0.2575 - soft_acc_multi_output: 0.6148 - val_loss: 0.3134 - val_soft_acc_multi_output: 0.5714\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 5s 312ms/step - loss: 0.2519 - soft_acc_multi_output: 0.6398 - val_loss: 0.3286 - val_soft_acc_multi_output: 0.5536\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 5s 314ms/step - loss: 0.2520 - soft_acc_multi_output: 0.6171 - val_loss: 0.2865 - val_soft_acc_multi_output: 0.5357\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 5s 313ms/step - loss: 0.2293 - soft_acc_multi_output: 0.6464 - val_loss: 0.3166 - val_soft_acc_multi_output: 0.5357\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 5s 311ms/step - loss: 0.2342 - soft_acc_multi_output: 0.6667 - val_loss: 0.3107 - val_soft_acc_multi_output: 0.5357\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 5s 312ms/step - loss: 0.2185 - soft_acc_multi_output: 0.6832 - val_loss: 0.3253 - val_soft_acc_multi_output: 0.5000\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 5s 312ms/step - loss: 0.2164 - soft_acc_multi_output: 0.6725 - val_loss: 0.3073 - val_soft_acc_multi_output: 0.5714\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 5s 314ms/step - loss: 0.2137 - soft_acc_multi_output: 0.6510 - val_loss: 0.3030 - val_soft_acc_multi_output: 0.5714\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 5s 311ms/step - loss: 0.1913 - soft_acc_multi_output: 0.7256 - val_loss: 0.2914 - val_soft_acc_multi_output: 0.5893\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 5s 310ms/step - loss: 0.1933 - soft_acc_multi_output: 0.7194 - val_loss: 0.3099 - val_soft_acc_multi_output: 0.5179\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 5s 309ms/step - loss: 0.1885 - soft_acc_multi_output: 0.7373 - val_loss: 0.3176 - val_soft_acc_multi_output: 0.5179\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 5s 311ms/step - loss: 0.1763 - soft_acc_multi_output: 0.7526 - val_loss: 0.3385 - val_soft_acc_multi_output: 0.5536\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 5s 302ms/step - loss: 0.1832 - soft_acc_multi_output: 0.7404 - val_loss: 0.3142 - val_soft_acc_multi_output: 0.5179\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 5s 302ms/step - loss: 0.1711 - soft_acc_multi_output: 0.7548 - val_loss: 0.3038 - val_soft_acc_multi_output: 0.5714\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 5s 303ms/step - loss: 0.1769 - soft_acc_multi_output: 0.7576 - val_loss: 0.3032 - val_soft_acc_multi_output: 0.5714\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 5s 307ms/step - loss: 0.1689 - soft_acc_multi_output: 0.7666 - val_loss: 0.3283 - val_soft_acc_multi_output: 0.5000\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 5s 306ms/step - loss: 0.1587 - soft_acc_multi_output: 0.7915 - val_loss: 0.3187 - val_soft_acc_multi_output: 0.5179\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 5s 306ms/step - loss: 0.1620 - soft_acc_multi_output: 0.7771 - val_loss: 0.3148 - val_soft_acc_multi_output: 0.5536\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 5s 304ms/step - loss: 0.1691 - soft_acc_multi_output: 0.7670 - val_loss: 0.3413 - val_soft_acc_multi_output: 0.5357\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 5s 304ms/step - loss: 0.1517 - soft_acc_multi_output: 0.7951 - val_loss: 0.3551 - val_soft_acc_multi_output: 0.5357\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 5s 304ms/step - loss: 0.1471 - soft_acc_multi_output: 0.8075 - val_loss: 0.3255 - val_soft_acc_multi_output: 0.5179\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 5s 304ms/step - loss: 0.1593 - soft_acc_multi_output: 0.7873 - val_loss: 0.3340 - val_soft_acc_multi_output: 0.5536\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 5s 304ms/step - loss: 0.1514 - soft_acc_multi_output: 0.7908 - val_loss: 0.3602 - val_soft_acc_multi_output: 0.5536\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 5s 309ms/step - loss: 0.1411 - soft_acc_multi_output: 0.8166 - val_loss: 0.3396 - val_soft_acc_multi_output: 0.5893\n"
     ]
    }
   ],
   "source": [
    "#########################\n",
    "## fine tuning\n",
    "#########################\n",
    "\n",
    "\n",
    "fine_tune = [layer.name for layer in model.layers].index(r\"top_conv\")\n",
    "\n",
    "model.trainable = True\n",
    "for layer in model.layers[:fine_tune]:\n",
    "    layer.trainable = False\n",
    "for layer in model.layers[fine_tune:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "print(\n",
    "    f\"from {fine_tune} of {len(model.layers)} layers; {len(model.trainable_variables)} trainables variables\"\n",
    ")\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Nadam(),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[soft_acc_multi_output],\n",
    ")\n",
    "train_set.reset()\n",
    "validation_set.reset()\n",
    "\n",
    "\n",
    "# logdir_name = (\n",
    "#     r\".\\tfb\\logs\\effnet\\\\\"\n",
    "#     + \"effnet__1_layer\"\n",
    "#     + \"__\"\n",
    "#     + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# )\n",
    "# tensorboard_callback = TensorBoard(log_dir=logdir_name)\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=21, restore_best_weights=True,\n",
    ")\n",
    "reduce_lr_plateau = ReduceLROnPlateau(monitor=\"val_loss\", patience=7, factor=0.8)\n",
    "\n",
    "# initial epoch is useless for keras optimizers as they update internally independent of epoch number\n",
    "history_fine = model.fit_generator(\n",
    "    generator=train_set,\n",
    "    epochs=100,\n",
    "    validation_data=validation_set,\n",
    "    callbacks=[early_stopping, reduce_lr_plateau,],\n",
    ")\n",
    "\n",
    "# model.save(\n",
    "#     filepath=\"./saved_models/my_effnet/tuned_1_layer.h5\", save_format=\"h5\",\n",
    "# )\n",
    "# model.save_weights(\n",
    "#     \"./saved_models/my_effnet/tuned_1_layer_weights_only.h5\", save_format=\"h5\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "cvscores = []\n",
    "\n",
    "# model__ = tf.keras.models.load_model(\n",
    "#     \"./saved_models/my_effnet/tuned_1_layer.h5\",\n",
    "#     custom_objects={\"soft_acc_multi_output\": soft_acc_multi_output},\n",
    "# )\n",
    "\n",
    "# model__.trainable = False\n",
    "# len(model__.trainable_variables)\n",
    "# model__.compile(\n",
    "#         optimizer=keras.optimizers.Nadam(learning_rate=0.0001),\n",
    "#         loss=\"binary_crossentropy\",\n",
    "#         metrics=[soft_acc_multi_output],\n",
    "#     )\n",
    "\n",
    "model__ = generate_base_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 450 validated image filenames.\n",
      "Found 57 validated image filenames.\n",
      "Found 56 validated image filenames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\feroc\\anaconda3\\envs\\tf\\lib\\site-packages\\keras_preprocessing\\image\\dataframe_iterator.py:273: UserWarning: Found 5 invalid image filename(s) in x_col=\"GreenID\". These filename(s) will be ignored.\n",
      "  .format(n_invalid, x_col)\n",
      "C:\\Users\\feroc\\anaconda3\\envs\\tf\\lib\\site-packages\\keras_preprocessing\\image\\dataframe_iterator.py:273: UserWarning: Found 1 invalid image filename(s) in x_col=\"GreenID\". These filename(s) will be ignored.\n",
      "  .format(n_invalid, x_col)\n"
     ]
    }
   ],
   "source": [
    "traintest = list(kf.split(np.zeros(len(response)), response[\"Grade\"]))\n",
    "train_index, test_index = np.stack(traintest)[:, 0][0], np.stack(traintest)[:, 1][0]\n",
    "\n",
    "\n",
    "train_gen = ImageDataGenerator(\n",
    "    rotation_range=5,\n",
    "    fill_mode=\"reflect\",\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0,\n",
    "    rescale=1.0 / 255.0,\n",
    "    zoom_range=0.1,\n",
    ")\n",
    "valid_gen = ImageDataGenerator(validation_split=0.5, rescale=1.0 / 255.0,)\n",
    "\n",
    "train_set = train_gen.flow_from_dataframe(\n",
    "    dataframe=response.iloc[train_index],\n",
    "    directory=DATADIR,\n",
    "    x_col=\"GreenID\",\n",
    "    target_size=(224, 224),\n",
    "    color_mode=\"rgb\",\n",
    "    subset=\"training\",\n",
    "    shuffle=False,\n",
    "    y_col=[0, 1, 2, 3,],\n",
    "    class_mode=\"raw\",\n",
    ")\n",
    "\n",
    "validation_set = valid_gen.flow_from_dataframe(\n",
    "    dataframe=response.iloc[test_index],\n",
    "    directory=DATADIR,\n",
    "    x_col=\"GreenID\",\n",
    "    target_size=(224, 224),\n",
    "    color_mode=\"rgb\",\n",
    "    subset=\"training\",\n",
    "    shuffle=True,\n",
    "    batch_size=64,\n",
    "    y_col=[0, 1, 2, 3,],\n",
    "    class_mode=\"raw\",\n",
    ")\n",
    "\n",
    "test_set = valid_gen.flow_from_dataframe(\n",
    "    dataframe=response.iloc[test_index],\n",
    "    directory=DATADIR,\n",
    "    x_col=\"GreenID\",\n",
    "    target_size=(224, 224),\n",
    "    color_mode=\"rgb\",\n",
    "    subset=\"validation\",\n",
    "    shuffle=True,\n",
    "    batch_size=64,\n",
    "    y_col=[0, 1, 2, 3,],\n",
    "    class_mode=\"raw\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model__ = fine_tune_model(model__,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 15 steps, validate for 1 steps\n",
      "Epoch 1/10\n",
      "15/15 [==============================] - 10s 647ms/step - loss: 0.8732 - soft_acc_multi_output: 0.1500 - val_loss: 0.4139 - val_soft_acc_multi_output: 0.3684\n",
      "Epoch 2/10\n",
      "15/15 [==============================] - 4s 288ms/step - loss: 0.7882 - soft_acc_multi_output: 0.1792 - val_loss: 0.4187 - val_soft_acc_multi_output: 0.3509\n",
      "Epoch 3/10\n",
      "15/15 [==============================] - 4s 287ms/step - loss: 0.6969 - soft_acc_multi_output: 0.1937 - val_loss: 0.4258 - val_soft_acc_multi_output: 0.4211\n",
      "Epoch 4/10\n",
      "15/15 [==============================] - 4s 291ms/step - loss: 0.6181 - soft_acc_multi_output: 0.2625 - val_loss: 0.4342 - val_soft_acc_multi_output: 0.4035\n",
      "Epoch 5/10\n",
      "15/15 [==============================] - 4s 288ms/step - loss: 0.5712 - soft_acc_multi_output: 0.2750 - val_loss: 0.4419 - val_soft_acc_multi_output: 0.4737\n",
      "Epoch 6/10\n",
      "15/15 [==============================] - 4s 288ms/step - loss: 0.5283 - soft_acc_multi_output: 0.3583 - val_loss: 0.4488 - val_soft_acc_multi_output: 0.4912\n",
      "Epoch 7/10\n",
      "15/15 [==============================] - 4s 287ms/step - loss: 0.5110 - soft_acc_multi_output: 0.3562 - val_loss: 0.4532 - val_soft_acc_multi_output: 0.4912\n",
      "Epoch 8/10\n",
      "15/15 [==============================] - 4s 287ms/step - loss: 0.4798 - soft_acc_multi_output: 0.4250 - val_loss: 0.4554 - val_soft_acc_multi_output: 0.4912\n",
      "Epoch 9/10\n",
      "15/15 [==============================] - 4s 286ms/step - loss: 0.4474 - soft_acc_multi_output: 0.4250 - val_loss: 0.4571 - val_soft_acc_multi_output: 0.4912\n",
      "Epoch 10/10\n",
      "15/15 [==============================] - 4s 288ms/step - loss: 0.4232 - soft_acc_multi_output: 0.4750 - val_loss: 0.4561 - val_soft_acc_multi_output: 0.4912\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x282fb52fa48>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch = next(test_set)\n",
    "# true_labels = batch[1]\n",
    "# predictions = model__.predict(batch[0])\n",
    "\n",
    "# print(model__.metrics_names)\n",
    "# print(\n",
    "#     model__.evaluate(train_set, verbose=0)\n",
    "# )  # working well with original unstratified model, including both trainning and testing sets?\n",
    "\n",
    "model__.fit(\n",
    "    x=train_set,\n",
    "    epochs=10,\n",
    "    validation_data=validation_set,\n",
    "    callbacks=[early_stopping, reduce_lr_plateau],\n",
    "    #         verbose=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_base_model():\n",
    "    conv_base = enet.EfficientNetB0(\n",
    "        include_top=False, input_shape=(224, 224, 3), pooling=\"avg\", weights=\"imagenet\",\n",
    "    )\n",
    "    conv_base.trainable = False\n",
    "\n",
    "    x = conv_base.output\n",
    "    x = Dropout(0.5)(x)\n",
    "    preds = Dense(\n",
    "        1,\n",
    "        activation=\"linear\",\n",
    "        bias_initializer=tf.keras.initializers.Constant(value=4),\n",
    "        bias_constraint=tf.keras.constraints.MinMaxNorm(\n",
    "            min_value=2, max_value=6, rate=1.0, axis=0\n",
    "        ),\n",
    "    )(x)\n",
    "    model = Model(inputs=conv_base.input, outputs=preds)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Nadam(lr=0.0014), loss=\"mse\", metrics=[soft_acc]\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def fine_tune_model(model, fine_tune=None):\n",
    "    if fine_tune is None:\n",
    "        try:\n",
    "            fine_tune = [layer.name for layer in model.layers].index(r\"top_conv\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    model.trainable = True\n",
    "    for layer in model.layers[:fine_tune]:\n",
    "        layer.trainable = False\n",
    "    for layer in model.layers[fine_tune:]:\n",
    "        layer.trainable = True\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Nadam(lr=0.0003), loss=\"mse\", metrics=[soft_acc],\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def generate_train_val_test(train_index, val_index, test_index):\n",
    "    train_dataset = response.iloc[train_index]\n",
    "    val_dataset = response.iloc[val_index]\n",
    "    test_dataset = response.iloc[test_index]\n",
    "    train_gen = ImageDataGenerator(\n",
    "        rotation_range=5,\n",
    "        fill_mode=\"reflect\",\n",
    "        horizontal_flip=True,\n",
    "        rescale=1.0 / 255.0,\n",
    "        zoom_range=0.1,\n",
    "    )\n",
    "    valid_test_gen = ImageDataGenerator(rescale=1.0 / 255.0,)\n",
    "\n",
    "    train_set = train_gen.flow_from_dataframe(\n",
    "        dataframe=train_dataset,\n",
    "        directory=DATADIR,\n",
    "        x_col=\"GreenID\",\n",
    "        target_size=(224, 224),\n",
    "        color_mode=\"rgb\",\n",
    "        subset=\"training\",\n",
    "        shuffle=True,\n",
    "        y_col=\"Grade\",\n",
    "        class_mode=\"raw\",\n",
    "    )\n",
    "\n",
    "    validation_set = valid_test_gen.flow_from_dataframe(\n",
    "        dataframe=val_dataset,\n",
    "        directory=DATADIR,\n",
    "        x_col=\"GreenID\",\n",
    "        target_size=(224, 224),\n",
    "        color_mode=\"rgb\",\n",
    "        subset=\"training\",\n",
    "        shuffle=False,\n",
    "        batch_size=64,\n",
    "        y_col=\"Grade\",\n",
    "        class_mode=\"raw\",\n",
    "    )\n",
    "\n",
    "    test_set = valid_test_gen.flow_from_dataframe(\n",
    "        dataframe=test_dataset,\n",
    "        directory=DATADIR,\n",
    "        x_col=\"GreenID\",\n",
    "        target_size=(224, 224),\n",
    "        color_mode=\"rgb\",\n",
    "        subset=\"training\",\n",
    "        shuffle=False,\n",
    "        batch_size=64,\n",
    "        y_col=\"Grade\",\n",
    "        class_mode=\"raw\",\n",
    "    )\n",
    "    return train_set, validation_set, test_set\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "innerkf = StratifiedKFold(n_splits=2, shuffle=True, random_state=seed)\n",
    "response = response.sample(frac=1.0)\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=21, restore_best_weights=True,\n",
    ")\n",
    "reduce_lr_plateau = ReduceLROnPlateau(monitor=\"val_loss\", patience=7, factor=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "def stratified_cv(fine_tune_layer=None):\n",
    "    acc_coef_scores = []\n",
    "    raw_outputs = []\n",
    "    for train_index, val_test_index in kf.split(\n",
    "        np.zeros(len(response)), response[\"Grade\"]\n",
    "    ):\n",
    "        val_index, test_index = next(\n",
    "            innerkf.split(\n",
    "                np.zeros(len(val_test_index)), response[\"Grade\"].iloc[val_test_index]\n",
    "            )\n",
    "        )\n",
    "        val_index, test_index = val_test_index[val_index], val_test_index[test_index]\n",
    "        train_set, validation_set, test_set = generate_train_val_test(\n",
    "            train_index, val_index, test_index\n",
    "        )\n",
    "        model = generate_base_model()\n",
    "\n",
    "        _ = model.fit(\n",
    "            x=train_set,\n",
    "            epochs=15,\n",
    "            validation_data=validation_set,\n",
    "            callbacks=[early_stopping, reduce_lr_plateau],\n",
    "                    verbose=0,\n",
    "        )\n",
    "\n",
    "        model = fine_tune_model(model, fine_tune=fine_tune_layer)\n",
    "\n",
    "        _ = model.fit(\n",
    "            x=train_set,\n",
    "            epochs=100,\n",
    "            validation_data=validation_set,\n",
    "            callbacks=[early_stopping, reduce_lr_plateau],\n",
    "                    verbose=0,\n",
    "        )\n",
    "\n",
    "        batch = next(test_set)\n",
    "        true_labels = batch[1]\n",
    "        predictions = model.predict(batch[0])\n",
    "        acc = soft_acc_multi_output(predictions, true_labels).numpy()\n",
    "        corr = np.corrcoef(predictions.reshape(-1), true_labels)[0][1]\n",
    "        acc_coef_scores.append([acc, corr])\n",
    "        raw_outputs.append([np.array(response.iloc[test_index].index), true_labels, predictions.reshape(-1)])\n",
    "        del train_set, validation_set, test_set, _, model, batch, true_labels, predictions, acc, corr\n",
    "        tf.keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "    return acc_coef_scores, raw_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/10 [00:00<?, ?it/s]C:\\Users\\ferocs\\anaconda3\\envs\\tf\\lib\\site-packages\\keras_preprocessing\\image\\dataframe_iterator.py:273: UserWarning: Found 4 invalid image filename(s) in x_col=\"GreenID\". These filename(s) will be ignored.\n",
      "  .format(n_invalid, x_col)\n",
      "C:\\Users\\ferocs\\anaconda3\\envs\\tf\\lib\\site-packages\\keras_preprocessing\\image\\dataframe_iterator.py:273: UserWarning: Found 1 invalid image filename(s) in x_col=\"GreenID\". These filename(s) will be ignored.\n",
      "  .format(n_invalid, x_col)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 451 validated image filenames.\n",
      "Found 56 validated image filenames.\n",
      "Found 56 validated image filenames.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: \n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: lr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/10 [02:51<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-fbe96d1097be>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mfine_tune\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainable_sequence\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0macc_coef_scores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstratified_cv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfine_tune\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mfine_tune_scores_acc_coef\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macc_coef_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mfine_tune_raw_outputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_outputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-8b490bf7ccb5>\u001b[0m in \u001b[0;36mstratified_cv\u001b[1;34m(fine_tune_layer)\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_set\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce_lr_plateau\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m                     \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m         )\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    597\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 599\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    600\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2361\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2365\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1613\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#  np.where(np.array(['conv' in layer.name for layer in model.layers]) == True)[0][::-1]\n",
    "from tqdm import tqdm\n",
    "\n",
    "trainable_sequence = np.array([227, 225, 217, 214, 210, 202, 199, 195, 187, 184, 180, 172, 169,\n",
    "       167, 159, 156, 152, 144, 141, 137, 129, 126, 124, 116, 113, 109,\n",
    "       101,  98,  94,  86,  83,  81,  73,  70,  66,  58,  55,  53,  45,\n",
    "        42,  38,  30,  27,  25,  17,  14,  12,   4,   1])\n",
    "\n",
    "fine_tune_scores_acc_coef = []\n",
    "fine_tune_raw_outputs = []\n",
    "\n",
    "\n",
    "for i in tqdm(range(20,30)):\n",
    "    fine_tune = trainable_sequence[i]\n",
    "    acc_coef_scores, raw_outputs = stratified_cv(fine_tune)\n",
    "    fine_tune_scores_acc_coef.append(acc_coef_scores)\n",
    "    fine_tune_raw_outputs.append(raw_outputs)\n",
    "    np.save(r\"./stratified_cross_validation_results/effnets/regression_acc_coef_20-30\", np.array(fine_tune_scores_acc_coef))\n",
    "    np.save(r\"./stratified_cross_validation_results/effnets/regression_raw_outputs_20-30\", np.array(fine_tune_raw_outputs))\n",
    "    del acc_coef_scores, raw_outputs\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "[[28 16  0  0  0]\n",
      " [ 4 32 15  3  0]\n",
      " [ 3 27 33 16  1]\n",
      " [ 0  5 16 23  4]\n",
      " [ 0  0 10 19 25]]\n",
      "+++++++++++++++++++++++++++++++++\n",
      "+++++++++++++++++++++++++++++++++\n",
      "[[0.63636364 0.36363636 0.         0.         0.        ]\n",
      " [0.07407407 0.59259259 0.27777778 0.05555556 0.        ]\n",
      " [0.0375     0.3375     0.4125     0.2        0.0125    ]\n",
      " [0.         0.10416667 0.33333333 0.47916667 0.08333333]\n",
      " [0.         0.         0.18518519 0.35185185 0.46296296]]\n",
      "+++++++++++++++++++++++++++++++++\n",
      "+++++++++++++++++++++++++++++++++\n",
      "0.5035714285714286\n",
      "+++++++++++++++++++++++++++++++++\n",
      "+++++++++++++++++++++++++++++++++\n",
      "max accuracy with tuning from 199 layers, or tune 34 layers\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "acc_coef = np.load(\n",
    "    r\"./stratified_cross_validation_results/effnets/regression_acc_coef_0-20.npy\",\n",
    "    allow_pickle=True,\n",
    ")\n",
    "raw_outputs = np.load(\n",
    "    r\"./stratified_cross_validation_results/effnets/regression_raw_outputs_0-20.npy\",\n",
    "    allow_pickle=True,\n",
    ")\n",
    "print(len(acc_coef))\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def show_confusion_matrix(raw_outputs, fine_tune_layers):\n",
    "\n",
    "    y_true = np.hstack(raw_outputs[fine_tune_layers, :, 1])\n",
    "    y_pred = np.rint(np.hstack(raw_outputs[fine_tune_layers, :, 2])).astype(\"int\")\n",
    "    y_pred[y_pred > 6] = 6\n",
    "    y_pred[y_pred < 2] = 2\n",
    "    return confusion_matrix(y_true, y_pred)\n",
    "\n",
    "\n",
    "def show_matrix_percentage(confusion_matrix):\n",
    "    return np.transpose(\n",
    "        np.transpose(my_confusion_matrix) / np.sum(my_confusion_matrix, axis=1)\n",
    "    )\n",
    "\n",
    "\n",
    "# total accuracy\n",
    "def calculate_accuracy(my_confusion_matrix):\n",
    "    return np.trace(my_confusion_matrix) / np.sum(my_confusion_matrix)\n",
    "\n",
    "\n",
    "max_acc_layer = np.argmax([calculate_accuracy(show_confusion_matrix(raw_outputs, i))  for i in range(len(acc_coef))])\n",
    "\n",
    "my_confusion_matrix = show_confusion_matrix(raw_outputs, max_acc_layer)\n",
    "print(my_confusion_matrix)\n",
    "print(\"+++++++++++++++++++++++++++++++++\")\n",
    "print(\"+++++++++++++++++++++++++++++++++\")\n",
    "print(show_matrix_percentage(my_confusion_matrix))\n",
    "print(\"+++++++++++++++++++++++++++++++++\")\n",
    "print(\"+++++++++++++++++++++++++++++++++\")\n",
    "print(calculate_accuracy(my_confusion_matrix))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(\n",
    "    [i for i in range(len(acc_coef))],\n",
    "    [\n",
    "        calculate_accuracy(show_confusion_matrix(raw_outputs, i))\n",
    "        for i in range(len(acc_coef))\n",
    "    ],\n",
    ")\n",
    "print(\"+++++++++++++++++++++++++++++++++\")\n",
    "print(\"+++++++++++++++++++++++++++++++++\")\n",
    "trainable_sequence = np.array([227, 225, 217, 214, 210, 202, 199, 195, 187, 184, 180, 172, 169,\n",
    "       167, 159, 156, 152, 144, 141, 137, 129, 126, 124, 116, 113, 109,\n",
    "       101,  98,  94,  86,  83,  81,  73,  70,  66,  58,  55,  53,  45,\n",
    "        42,  38,  30,  27,  25,  17,  14,  12,   4,   1])\n",
    "print(f\"max accuracy with tuning from {trainable_sequence[max_acc_layer]} layers, or tune {233-trainable_sequence[max_acc_layer]} layers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56, 4)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(cvscores)[:, 1][:, 0][4].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.54385966, 0.7954881615019466],\n",
       "       [0.5964912, 0.851205445137053],\n",
       "       [0.4107143, 0.7907964869050351],\n",
       "       [0.54545456, 0.8426871818521864],\n",
       "       [0.66071427, 0.8581865352844125]], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(cvscores)[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5,  3,  1,  0,  0],\n",
       "       [ 0,  9,  3,  0,  0],\n",
       "       [ 1,  4, 11,  0,  0],\n",
       "       [ 0,  0,  3,  5,  3],\n",
       "       [ 0,  0,  1,  3,  5]], dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(\n",
    "    y_true=K.sum(K.cast(K.round(cvscores[1][1][0]), \"int32\"), axis=1).numpy(),\n",
    "    y_pred=K.sum(K.cast(K.round(cvscores[1][1][1]), \"int32\"), axis=1).numpy(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "mycsv = pd.DataFrame(\n",
    "    np.hstack(\n",
    "        np.array(\n",
    "            [\n",
    "                np.vstack(np.array(cvscores)[:, 1][:, 0]),\n",
    "                np.vstack(np.array(cvscores)[:, 1][:, 1]),\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(mycsv[range(4, 8)].to_numpy(), np.vstack(np.array(cvscores)[:, 1][:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "mycsv.to_csv(\n",
    "    \"./stratified_cross_validation_results/effnet_multinomial.csv\", index=False\n",
    ")\n",
    "# next time include which image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "mycsv = pd.read_csv(\"./stratified_cross_validation_results/effnet_multinomial.csv\")\n",
    "y_true = np.sum((mycsv[[str(i) for i in range(0, 4)]]).to_numpy(dtype=int), axis=1)\n",
    "y_pred = np.sum(\n",
    "    np.rint((mycsv[[str(i) for i in range(4, 8)]]).to_numpy()), axis=1\n",
    ").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[33, 10,  1,  0,  0],\n",
       "       [ 6, 36, 14,  1,  0],\n",
       "       [ 7, 22, 35,  5,  4],\n",
       "       [ 0,  2, 20, 17,  9],\n",
       "       [ 0,  0,  7, 17, 35]], dtype=int64)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "my_confusion_matrix = confusion_matrix(y_true, y_pred,)\n",
    "my_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.75      , 0.22727273, 0.02272727, 0.        , 0.        ],\n",
       "       [0.10526316, 0.63157895, 0.24561404, 0.01754386, 0.        ],\n",
       "       [0.09589041, 0.30136986, 0.47945205, 0.06849315, 0.05479452],\n",
       "       [0.        , 0.04166667, 0.41666667, 0.35416667, 0.1875    ],\n",
       "       [0.        , 0.        , 0.11864407, 0.28813559, 0.59322034]])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.transpose(np.transpose(my_confusion_matrix) / np.sum(my_confusion_matrix, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.82710261],\n",
       "       [0.82710261, 1.        ]])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# coef\n",
    "np.corrcoef(y_true, np.sum((mycsv[[str(i) for i in range(4, 8)]]).to_numpy(), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5551601423487544"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# acc\n",
    "sum(np.isclose(y_true, y_pred)) / len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.utils import plot_model\n",
    "\n",
    "# plot_model(model, to_file=\"effnet.png\", show_shapes=True)\n",
    "# from IPython.display import Image\n",
    "\n",
    "# Image(filename=\"effnet.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 281 validated image filenames.\n",
      "['loss', 'soft_acc_multi_output']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "[0.21299249678850174, 0.74404764]\n"
     ]
    }
   ],
   "source": [
    "response = response.sample(frac=1.0)\n",
    "\n",
    "test_set = valid_gen.flow_from_dataframe(\n",
    "    dataframe=response,\n",
    "    directory=DATADIR,\n",
    "    x_col=\"GreenID\",\n",
    "    target_size=(224, 224),\n",
    "    color_mode=\"rgb\",\n",
    "    subset=\"validation\",\n",
    "    shuffle=False,\n",
    "    batch_size=56,\n",
    "    y_col=[0, 1, 2, 3,],\n",
    "    class_mode=\"raw\",\n",
    "    #     seed = seed\n",
    ")\n",
    "\n",
    "batch = next(test_set)\n",
    "true_labels = batch[1]\n",
    "predictions = model.predict(batch[0])\n",
    "\n",
    "print(model.metrics_names)\n",
    "print(model.evaluate(test_set, verbose=0))  # loss/accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.85824516],\n",
       "       [0.85824516, 1.        ]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(np.sum(predictions, axis=1), np.sum(true_labels, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=80)\n",
    "batch = next(test_set)\n",
    "\n",
    "y_true = batch[1]\n",
    "y_pred = model.predict(batch[0])\n",
    "print(soft_acc_multi_output(y_true, y_pred))\n",
    "\n",
    "# print examples from the validation set\n",
    "for i in range(len(batch[1])):\n",
    "    img = batch[0][i]\n",
    "    label = batch[1][i]\n",
    "    assert (label == y_true[i]).all()\n",
    "    right = K.all(\n",
    "        K.equal(K.cast(K.round(label), \"int32\"), K.cast(K.round(y_pred[i]), \"int32\"),)\n",
    "    )\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    print(f\"true label: {label}; rounded pred: {y_pred[i]}; Correct: {right}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ds = tf.data.Dataset.list_files(\n",
    "    str(\"C:/Users/feroc/OneDrive - The University of Melbourne/Dataset/adult/*\")\n",
    ")\n",
    "\n",
    "\n",
    "def decode_img(img):\n",
    "    # convert the compressed string to a 3D uint8 tensor\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    # resize the image to the desired size.\n",
    "    return tf.image.resize(img, [IMG_WIDTH, IMG_HEIGHT])\n",
    "\n",
    "\n",
    "def get_label(file_path):\n",
    "    # convert the path to a list of path components\n",
    "    image_id = tf.strings.split(file_path, os.path.sep)[-1]\n",
    "    return response.loc[] \n",
    "\n",
    "list(list_ds.take(1).as_numpy_iterator())[0]\n",
    "tf.strings.split(list(list_ds.take(1).as_numpy_iterator())[0],os.path.sep)[-1].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
